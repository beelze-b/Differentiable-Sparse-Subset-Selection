{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scipy.io.loadmat(\"../data/zeisel/zeisel_data.mat\")\n",
    "data= a['zeisel_data'].T\n",
    "N,d=data.shape\n",
    "\n",
    "#load labels (first level of the hierarchy) from file\n",
    "a = scipy.io.loadmat(\"../data/zeisel/zeisel_labels1.mat\")\n",
    "l_aux = a['zeisel_labels1']\n",
    "l_0=[l_aux[i][0] for i in range(l_aux.shape[0])]\n",
    "#load labels (second level of the hierarchy) from file\n",
    "a = scipy.io.loadmat(\"../data/zeisel/zeisel_labels2.mat\")\n",
    "l_aux = a['zeisel_labels2']\n",
    "l_1=[l_aux[i][0] for i in range(l_aux.shape[0])]\n",
    "#construct an array with hierarchy labels\n",
    "labels=np.array([l_0, l_1])\n",
    "\n",
    "# load names from file \n",
    "a = scipy.io.loadmat(\"../data/zeisel/zeisel_names.mat\")\n",
    "names0=np.array([a['zeisel_names'][i][0][0] for i in range(N)])\n",
    "names1=[a['zeisel_names'][i][1][0] for i in range(N)]\n",
    "\n",
    "np.random.seed(100)\n",
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))\n",
    "\n",
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_labels = names0[slices[:upto]]\n",
    "test_labels = names0[slices[upto:]]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "z_size = 100\n",
    "\n",
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 600\n",
    "batch_size = 64\n",
    "lr = 0.000005\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "global_t = 4\n",
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=lr, \n",
    "                                 betas = (b1,b2))\n",
    "        \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train(train_data, \n",
    "              model, \n",
    "              optimizer, \n",
    "              epoch, \n",
    "              batch_size)\n",
    "        model.t = max(0.001, model.t * 0.99)\n",
    "\n",
    "        \n",
    "    return model\n",
    "\n",
    "def save_model(base_path, model):\n",
    "    # make directory\n",
    "    if not os.path.exists(os.path.dirname(base_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(base_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise Exception(\"COULD NOT MAKE PATH\")\n",
    "    with open(base_path, 'wb') as PATH:\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, a = 0.01)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_logits_gumbel_globalgate_vae(data, model):\n",
    "    assert isinstance(model, VAE_Gumbel_GlobalGate)\n",
    "    with torch.no_grad():\n",
    "        w = model.logit_enc.clone().view(-1)\n",
    "        top_k_logits = torch.topk(w, k = model.k, sorted = True)[1]\n",
    "        enc_top_logits = torch.nn.functional.one_hot(top_k_logits, num_classes = data.shape[1]).sum(dim = 0)\n",
    "        \n",
    "        #subsets = sample_subset(w, model.k,model.t,True)\n",
    "        subsets = sample_subset(w, model.k,model.t)\n",
    "        #max_idx = torch.argmax(subsets, 1, keepdim=True)\n",
    "        #one_hot = Tensor(subsets.shape)\n",
    "        #one_hot.zero_()\n",
    "        #one_hot.scatter_(1, max_idx, 1)\n",
    "\n",
    "        \n",
    "    return enc_top_logits, subsets\n",
    "\n",
    "def top_logits_gumbel_runningstate_vae(data, model):\n",
    "    assert isinstance(model, VAE_Gumbel_RunningState)\n",
    "    with torch.no_grad():\n",
    "        w = model.logit_enc.clone().view(-1)\n",
    "        top_k_logits = torch.topk(w, k = model.k, sorted = True)[1]\n",
    "        enc_top_logits = torch.nn.functional.one_hot(top_k_logits, num_classes = data.shape[1]).sum(dim = 0)\n",
    "        \n",
    "        #subsets = sample_subset(w, model.k,model.t,True)\n",
    "        subsets = sample_subset(w, model.k,model.t)\n",
    "        #max_idx = torch.argmax(subsets, 1, keepdim=True)\n",
    "        #one_hot = Tensor(subsets.shape)\n",
    "        #one_hot.zero_()\n",
    "        #one_hot.scatter_(1, max_idx, 1)\n",
    "\n",
    "        \n",
    "    return enc_top_logits, subsets\n",
    "\n",
    "def top_logits_gumbel_concrete_vae_nsml(data, model):\n",
    "    assert isinstance(model, ConcreteVAE_NMSL)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        w = gumbel_keys(model.logit_enc, EPSILON = torch.finfo(torch.float32).eps)\n",
    "        w = torch.softmax(w/model.t, dim = -1)\n",
    "        subset_indices = w.clone().detach()\n",
    "\n",
    "        #max_idx = torch.argmax(subset_indices, 1, keepdim=True)\n",
    "        #one_hot = Tensor(subset_indices.shape)\n",
    "        #one_hot.zero_()\n",
    "        #one_hot.scatter_(1, max_idx, 1)\n",
    "\n",
    "        all_subsets = subset_indices.sum(dim = 0)\n",
    "\n",
    "        inds = torch.argsort(subset_indices.sum(dim = 0), descending = True)[:model.k]\n",
    "        all_logits = torch.nn.functional.one_hot(inds, num_classes = data.shape[1]).sum(dim = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return all_logits, all_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_Gumbel_RunningState(train_data.shape[1], 200, 50, k = k, t = global_t, alpha = 0.9, bias = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE_Gumbel_RunningState(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=4000, out_features=200, bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (enc_mean): Linear(in_features=200, out_features=50, bias=False)\n",
       "  (enc_logvar): Linear(in_features=200, out_features=50, bias=False)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=200, bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=4000, bias=False)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (weight_creator): Sequential(\n",
       "    (0): Linear(in_features=4000, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=4000, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2404 (0%)]\tLoss: 3601.575439\n",
      "Train Epoch: 1 [1280/2404 (53%)]\tLoss: 3587.273682\n",
      "====> Epoch: 1 Average loss: 3584.1322\n",
      "Train Epoch: 2 [0/2404 (0%)]\tLoss: 3544.147217\n",
      "Train Epoch: 2 [1280/2404 (53%)]\tLoss: 3558.588135\n",
      "====> Epoch: 2 Average loss: 3557.5481\n",
      "Train Epoch: 3 [0/2404 (0%)]\tLoss: 3493.164307\n",
      "Train Epoch: 3 [1280/2404 (53%)]\tLoss: 3550.557861\n",
      "====> Epoch: 3 Average loss: 3532.8485\n",
      "Train Epoch: 4 [0/2404 (0%)]\tLoss: 3493.610596\n",
      "Train Epoch: 4 [1280/2404 (53%)]\tLoss: 3518.282715\n",
      "====> Epoch: 4 Average loss: 3511.2554\n",
      "Train Epoch: 5 [0/2404 (0%)]\tLoss: 3517.474609\n",
      "Train Epoch: 5 [1280/2404 (53%)]\tLoss: 3495.145996\n",
      "====> Epoch: 5 Average loss: 3490.6198\n",
      "Train Epoch: 6 [0/2404 (0%)]\tLoss: 3430.353027\n",
      "Train Epoch: 6 [1280/2404 (53%)]\tLoss: 3490.442383\n",
      "====> Epoch: 6 Average loss: 3461.2573\n",
      "Train Epoch: 7 [0/2404 (0%)]\tLoss: 3440.864258\n",
      "Train Epoch: 7 [1280/2404 (53%)]\tLoss: 3439.857666\n",
      "====> Epoch: 7 Average loss: 3448.9637\n",
      "Train Epoch: 8 [0/2404 (0%)]\tLoss: 3438.154297\n",
      "Train Epoch: 8 [1280/2404 (53%)]\tLoss: 3398.206055\n",
      "====> Epoch: 8 Average loss: 3421.1222\n",
      "Train Epoch: 9 [0/2404 (0%)]\tLoss: 3434.461426\n",
      "Train Epoch: 9 [1280/2404 (53%)]\tLoss: 3391.113525\n",
      "====> Epoch: 9 Average loss: 3398.8003\n",
      "Train Epoch: 10 [0/2404 (0%)]\tLoss: 3379.768799\n",
      "Train Epoch: 10 [1280/2404 (53%)]\tLoss: 3416.727539\n",
      "====> Epoch: 10 Average loss: 3376.7966\n",
      "Train Epoch: 11 [0/2404 (0%)]\tLoss: 3345.195557\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.apply(weights_init)\n",
    "train_model(train_data, model)\n",
    "model.set_burned_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(test_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_logits_running_state = top_logits_gumbel_runningstate_vae(test_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argsort(top_logits_running_state[0], descending = True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_running_state = torch.argsort(top_logits_running_state[1], descending = True)[:50].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels[0])\n",
    "print(\"HOW TO GET NAME OF FEATURES?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(\"../data/models/final_run_zeisel/runningstate_vae/k_50/model.pt\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Global Gate too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_Gumbel_GlobalGate(train_data.shape[1], 200, 50, k = k, t = global_t, bias = False)\n",
    "model.to(device)\n",
    "model.apply(weights_init)\n",
    "train_model(train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_logits_global_gate = top_logits_gumbel_globalgate_vae(test_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_global_gate = torch.argsort(top_logits_global_gate[1], descending = True)[:50].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(\"../data/models/final_run_zeisel/global_gate/k_50/model.pt\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConcreteVAE_NMSL(train_data.shape[1], 200, 50, k = k, t = global_t, bias = False)\n",
    "model.to(device)\n",
    "model.apply(weights_init)\n",
    "train_model(train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_logits_concrete = top_logits_gumbel_concrete_vae_nsml(test_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argsort(top_logits_concrete[0], descending = True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_concrete_vae = torch.argsort(top_logits_concrete[1], descending = True)[:50].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(\"../data/models/final_run_zeisel/concrete_vae/k_50/model.pt\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare models and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_running_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_concrete_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.intersect1d(inds_running_state, inds_concrete_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding for actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? plt.colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_zeisel(data, labels, model_name, zeisel_encoder):\n",
    "    # seems good too\n",
    "    #embedding = umap.UMAP(n_neighbors=5).fit_transform(data)\n",
    "    \n",
    "    # seems the best\n",
    "    embedding = umap.UMAP(n_neighbors=10, min_dist= 0.05).fit_transform(data)\n",
    "    \n",
    "    # seems good\n",
    "    #embedding = TSNE(n_components=2).fit_transform(data)\n",
    "    \n",
    "    # weird\n",
    "    #embedding = TSNE(n_components=2, perplexity=35).fit_transform(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8.5))\n",
    "    \n",
    "    plt.scatter(*embedding.T, c = zeisel_encoder.transform(labels))\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    \n",
    "    cbar = plt.colorbar(ticks=np.arange(8), boundaries = np.arange(8) - 0.5)\n",
    "    cbar.ax.set_yticklabels(zeisel_encoder.classes_)\n",
    "    \n",
    "    plt.title(f\"Clustering Zeisel Data When Using {model_name} Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeisel_encoder = LabelEncoder()\n",
    "zeisel_encoder.fit(y = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_zeisel(test_data.cpu().numpy(), test_labels, \"No\", zeisel_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_zeisel(test_data[:, inds_global_gate].cpu().numpy(), test_labels, \"Global Gate VAE\", zeisel_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_zeisel(test_data[:, inds_running_state].cpu().numpy(), test_labels, \"Running State VAE\", zeisel_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_zeisel(test_data[:, inds_concrete_vae].cpu().numpy(), test_labels, \"Concrete VAE\", zeisel_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
