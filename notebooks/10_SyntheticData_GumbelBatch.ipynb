{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we have the gumbel trick generate one set of samples for the whole data set? Let's try to do it per batch.\n",
    "\n",
    "In addition, how does work when all the features are real, vs when half the features are real (not pure noise). \n",
    "For this, test when loss is calculated over half or all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 5000\n",
    "z_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified version of our train function that can calculation the reconstruction loss over a limited version of the features and also calculate the gradients to the features of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_truncated_with_gradients(df, model, optimizer, epoch, batch_size, Dim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    gradients = torch.zeros(df.shape[1]).to(device)\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone().to(device)\n",
    "        \n",
    "        \n",
    "        # need to do this twice because deriative with respect to input not implemented in BCE\n",
    "        # so need to switch them up\n",
    "        optimizer.zero_grad()\n",
    "        batch_data.requires_grad_(True)\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        # why clone detach here?\n",
    "        # still want gradient with respect to input, but BCE gradient with respect to target is not defined\n",
    "        # plus we only want to see how input affects mu_x, not the target\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim].clone().detach(), mu_x[:, :Dim], \n",
    "                                             mu_latent, logvar_latent) \n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gradients += torch.sqrt(batch_data.grad ** 2).sum(dim = 0)\n",
    "        # no step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # do not calculate with respect to \n",
    "        batch_data.requires_grad_(False)\n",
    "        mu_x.requires_grad_(True)\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim], mu_x[:, :Dim], mu_latent, logvar_latent) \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this first on a feature set where all the features are non-noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(18, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)\n",
    "\n",
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)\n",
    "\n",
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)\n",
    "\n",
    "actual_data = data_mapper(latent_data)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))\n",
    "    \n",
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)\n",
    "\n",
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_t = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate loss on only the first half of features. Here all the features in the data were non-noisy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel_NInsta(D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 10.297486\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 9.840182\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 9.418898\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 8.914977\n",
      "====> Epoch: 1 Average loss: 9.6234\n",
      "====> Test set loss: 19.3873\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 8.979761\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 8.583446\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 8.018302\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 7.674096\n",
      "====> Epoch: 2 Average loss: 8.3250\n",
      "====> Test set loss: 18.1919\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 7.719872\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 7.328157\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 7.006524\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 6.956886\n",
      "====> Epoch: 3 Average loss: 7.2780\n",
      "====> Test set loss: 17.6566\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 7.096731\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 7.008787\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 6.874085\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 7.034038\n",
      "====> Epoch: 4 Average loss: 6.8751\n",
      "====> Test set loss: 17.4200\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 6.877703\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 6.704479\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 6.813527\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 6.624295\n",
      "====> Epoch: 5 Average loss: 6.6757\n",
      "====> Test set loss: 17.3038\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 6.851572\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 6.614273\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 6.523686\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 6.671239\n",
      "====> Epoch: 6 Average loss: 6.5725\n",
      "====> Test set loss: 17.1789\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 6.698739\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 6.248917\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 6.258627\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 6.645110\n",
      "====> Epoch: 7 Average loss: 6.4732\n",
      "====> Test set loss: 17.0736\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 6.480068\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 6.530424\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 6.716979\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 6.555336\n",
      "====> Epoch: 8 Average loss: 6.4170\n",
      "====> Test set loss: 17.0281\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 6.483744\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 6.107954\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 6.506589\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 6.284665\n",
      "====> Epoch: 9 Average loss: 6.3687\n",
      "====> Test set loss: 17.0120\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 6.265841\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 6.416995\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 6.627335\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 6.455487\n",
      "====> Epoch: 10 Average loss: 6.3264\n",
      "====> Test set loss: 16.9998\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 6.347841\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 6.427380\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 6.091283\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 6.375041\n",
      "====> Epoch: 11 Average loss: 6.2891\n",
      "====> Test set loss: 16.9127\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 6.381680\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 6.169864\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 6.277218\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 6.270780\n",
      "====> Epoch: 12 Average loss: 6.2753\n",
      "====> Test set loss: 16.8558\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 6.127457\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 6.213811\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 6.266612\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 6.226641\n",
      "====> Epoch: 13 Average loss: 6.2369\n",
      "====> Test set loss: 16.8177\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 6.017147\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 6.104245\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 6.312294\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 6.155954\n",
      "====> Epoch: 14 Average loss: 6.2167\n",
      "====> Test set loss: 16.8007\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 6.296331\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 6.429549\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 6.106196\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 5.638452\n",
      "====> Epoch: 15 Average loss: 6.1821\n",
      "====> Test set loss: 16.7798\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 5.784521\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 5.886128\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 5.970484\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 5.834663\n",
      "====> Epoch: 16 Average loss: 6.1496\n",
      "====> Test set loss: 16.7310\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 6.493556\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 6.538465\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 6.174248\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 5.943320\n",
      "====> Epoch: 17 Average loss: 6.1209\n",
      "====> Test set loss: 16.7487\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 6.071372\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 6.356735\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 6.115577\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 6.066781\n",
      "====> Epoch: 18 Average loss: 6.0806\n",
      "====> Test set loss: 16.6893\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 5.695485\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 5.847662\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 5.881423\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 6.140705\n",
      "====> Epoch: 19 Average loss: 6.0403\n",
      "====> Test set loss: 16.6058\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 5.857124\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 5.800861\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 6.444478\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 5.824520\n",
      "====> Epoch: 20 Average loss: 6.0043\n",
      "====> Test set loss: 16.5566\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 6.139254\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 5.994491\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 6.142058\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 5.908755\n",
      "====> Epoch: 21 Average loss: 5.9743\n",
      "====> Test set loss: 16.5421\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 6.043991\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 5.945944\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 5.941624\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 5.866823\n",
      "====> Epoch: 22 Average loss: 5.9325\n",
      "====> Test set loss: 16.5312\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 5.770072\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 5.801189\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 5.860751\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 5.829428\n",
      "====> Epoch: 23 Average loss: 5.9157\n",
      "====> Test set loss: 16.4681\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 5.836315\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 6.012181\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 5.705913\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 5.895569\n",
      "====> Epoch: 24 Average loss: 5.9071\n",
      "====> Test set loss: 16.4680\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 5.629681\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 5.753871\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 5.792142\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 5.959679\n",
      "====> Epoch: 25 Average loss: 5.8739\n",
      "====> Test set loss: 16.4624\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 5.965430\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 5.645813\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 5.809639\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 6.193795\n",
      "====> Epoch: 26 Average loss: 5.8907\n",
      "====> Test set loss: 16.4433\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 5.782059\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 5.962022\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 5.580315\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 6.114537\n",
      "====> Epoch: 27 Average loss: 5.8671\n",
      "====> Test set loss: 16.4265\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 5.676579\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 5.945484\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 5.765113\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 5.908872\n",
      "====> Epoch: 28 Average loss: 5.8550\n",
      "====> Test set loss: 16.4082\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 5.586582\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 5.692275\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 5.623438\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 5.939415\n",
      "====> Epoch: 29 Average loss: 5.8375\n",
      "====> Test set loss: 16.3758\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 5.693781\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 5.804242\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 6.251395\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 5.942711\n",
      "====> Epoch: 30 Average loss: 5.8325\n",
      "====> Test set loss: 16.3764\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 5.768879\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 5.860924\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 5.943866\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 6.002486\n",
      "====> Epoch: 31 Average loss: 5.8058\n",
      "====> Test set loss: 16.4143\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 5.960031\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 6.009021\n",
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 5.858872\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 5.982724\n",
      "====> Epoch: 32 Average loss: 5.8211\n",
      "====> Test set loss: 16.3386\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 5.570322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 5.994228\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 6.030520\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 5.593287\n",
      "====> Epoch: 33 Average loss: 5.8083\n",
      "====> Test set loss: 16.4088\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 5.978364\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 5.982869\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 5.597217\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 5.899381\n",
      "====> Epoch: 34 Average loss: 5.7836\n",
      "====> Test set loss: 16.3929\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 5.783190\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 5.598122\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 5.911641\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 5.798098\n",
      "====> Epoch: 35 Average loss: 5.7979\n",
      "====> Test set loss: 16.3779\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 5.746149\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 5.673071\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 5.799855\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 6.031126\n",
      "====> Epoch: 36 Average loss: 5.8011\n",
      "====> Test set loss: 16.3575\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 5.947552\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 5.969201\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 5.426183\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 5.659766\n",
      "====> Epoch: 37 Average loss: 5.7856\n",
      "====> Test set loss: 16.3367\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 5.907727\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 5.859684\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 6.215934\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 5.786956\n",
      "====> Epoch: 38 Average loss: 5.7835\n",
      "====> Test set loss: 16.3610\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 5.691721\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 5.997086\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 5.978383\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 5.758012\n",
      "====> Epoch: 39 Average loss: 5.7816\n",
      "====> Test set loss: 16.3283\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 5.608979\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 5.646009\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 5.722291\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 5.828291\n",
      "====> Epoch: 40 Average loss: 5.7662\n",
      "====> Test set loss: 16.3087\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 6.036457\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 5.640835\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 5.629264\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 5.872087\n",
      "====> Epoch: 41 Average loss: 5.7371\n",
      "====> Test set loss: 16.2773\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 5.710727\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 5.728333\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 5.523361\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 5.808943\n",
      "====> Epoch: 42 Average loss: 5.7233\n",
      "====> Test set loss: 16.2373\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 5.613236\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 5.664737\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 5.906525\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 5.766557\n",
      "====> Epoch: 43 Average loss: 5.7109\n",
      "====> Test set loss: 16.2692\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 5.667623\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 6.007842\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 5.727509\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 5.684147\n",
      "====> Epoch: 44 Average loss: 5.7226\n",
      "====> Test set loss: 16.2923\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 5.564205\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 5.893180\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 5.699396\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 5.826172\n",
      "====> Epoch: 45 Average loss: 5.7038\n",
      "====> Test set loss: 16.1816\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 5.634628\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 5.799866\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 5.918568\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 5.529622\n",
      "====> Epoch: 46 Average loss: 5.6708\n",
      "====> Test set loss: 16.2260\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 5.653765\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 5.283326\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 5.661169\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 5.710719\n",
      "====> Epoch: 47 Average loss: 5.6786\n",
      "====> Test set loss: 16.1230\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 5.738130\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 5.282795\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 5.537126\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 5.394859\n",
      "====> Epoch: 48 Average loss: 5.6663\n",
      "====> Test set loss: 16.1449\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 5.663391\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 5.638316\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 5.868351\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 5.274017\n",
      "====> Epoch: 49 Average loss: 5.6335\n",
      "====> Test set loss: 16.1255\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 5.337204\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 5.790874\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 5.530893\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 5.327760\n",
      "====> Epoch: 50 Average loss: 5.6221\n",
      "====> Test set loss: 16.1241\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = int(D/2))\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f13792d2590>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1370190890>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the values to see if our predictions are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0723, 0.2725, 0.1108, 0.2433, 0.2462, 0.2292, 0.0998, 0.2706, 0.0307,\n",
       "        0.2064, 0.0224, 0.0537, 0.0900, 0.2398, 0.2477, 0.5886, 0.5253, 0.4511,\n",
       "        0.5892, 0.4499, 0.4474, 0.4963, 0.4496, 0.4943, 0.4809, 0.5092, 0.4741,\n",
       "        0.5025, 0.4630, 0.5358], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0725, 0.1197, 0.1243, 0.0906, 0.1169, 0.0687, 0.1216, 0.0977, 0.0424,\n",
       "        0.1939, 0.0163, 0.0497, 0.1313, 0.1116, 0.1648, 0.0631, 0.0534, 0.0668,\n",
       "        0.0727, 0.0661, 0.0594, 0.0592, 0.0667, 0.0611, 0.0626, 0.0836, 0.0849,\n",
       "        0.0601, 0.0737, 0.0540], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same when the loss is calculated over all the features. We keep all the features non-noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel_NInsta(D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 20.870155\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 19.960140\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.194572\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 18.370110\n",
      "====> Epoch: 1 Average loss: 19.5208\n",
      "====> Test set loss: 18.1819\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 18.204750\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 17.318251\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 16.435675\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 15.259007\n",
      "====> Epoch: 2 Average loss: 16.7961\n",
      "====> Test set loss: 15.0355\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 15.067239\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 14.500895\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 13.919978\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 13.610239\n",
      "====> Epoch: 3 Average loss: 14.2072\n",
      "====> Test set loss: 13.5598\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 13.512154\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 13.049765\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 13.209951\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 13.473091\n",
      "====> Epoch: 4 Average loss: 13.2288\n",
      "====> Test set loss: 12.9940\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 13.231510\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 12.680857\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 12.607175\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 12.534602\n",
      "====> Epoch: 5 Average loss: 12.8280\n",
      "====> Test set loss: 12.6692\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 12.708134\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 12.984596\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.442845\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 12.295327\n",
      "====> Epoch: 6 Average loss: 12.5858\n",
      "====> Test set loss: 12.4760\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 12.727048\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.456384\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.331480\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 11.947690\n",
      "====> Epoch: 7 Average loss: 12.4237\n",
      "====> Test set loss: 12.3610\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.304271\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.253939\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.458853\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.236322\n",
      "====> Epoch: 8 Average loss: 12.2924\n",
      "====> Test set loss: 12.2119\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.299989\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.573852\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 12.503789\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 12.453973\n",
      "====> Epoch: 9 Average loss: 12.2144\n",
      "====> Test set loss: 12.1665\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.238624\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 12.388555\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 12.553123\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 11.819516\n",
      "====> Epoch: 10 Average loss: 12.1361\n",
      "====> Test set loss: 12.0943\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 12.154721\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 11.977881\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 12.186426\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 11.953444\n",
      "====> Epoch: 11 Average loss: 12.0838\n",
      "====> Test set loss: 12.0397\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 12.171670\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 12.036248\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 11.819390\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 12.263078\n",
      "====> Epoch: 12 Average loss: 12.0599\n",
      "====> Test set loss: 12.0372\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 11.782436\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 12.023368\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 12.215362\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 12.390973\n",
      "====> Epoch: 13 Average loss: 12.0264\n",
      "====> Test set loss: 11.9796\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 11.808447\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 12.549583\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 12.089308\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 11.889318\n",
      "====> Epoch: 14 Average loss: 11.9624\n",
      "====> Test set loss: 11.9499\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 12.062263\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 11.988182\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 12.373759\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 11.629896\n",
      "====> Epoch: 15 Average loss: 11.9419\n",
      "====> Test set loss: 11.8872\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 12.036036\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 11.735325\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 12.124424\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 11.656000\n",
      "====> Epoch: 16 Average loss: 11.8960\n",
      "====> Test set loss: 11.8058\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 12.057304\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 11.970687\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 11.308255\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 11.388789\n",
      "====> Epoch: 17 Average loss: 11.8199\n",
      "====> Test set loss: 11.7553\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 11.780087\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 11.557651\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 11.512178\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 11.617936\n",
      "====> Epoch: 18 Average loss: 11.7254\n",
      "====> Test set loss: 11.6342\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 11.889426\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 11.627471\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 11.523501\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 11.662474\n",
      "====> Epoch: 19 Average loss: 11.6175\n",
      "====> Test set loss: 11.5558\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 12.136449\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 11.476753\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 11.525683\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 11.720364\n",
      "====> Epoch: 20 Average loss: 11.5277\n",
      "====> Test set loss: 11.4225\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 11.633521\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 11.396595\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 11.612123\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 11.213796\n",
      "====> Epoch: 21 Average loss: 11.4201\n",
      "====> Test set loss: 11.3120\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 11.134829\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 11.209304\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 10.795856\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 11.330512\n",
      "====> Epoch: 22 Average loss: 11.3489\n",
      "====> Test set loss: 11.2841\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 11.587430\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 10.928200\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 10.996296\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 11.223490\n",
      "====> Epoch: 23 Average loss: 11.2716\n",
      "====> Test set loss: 11.2740\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 11.186312\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 11.329554\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 11.491168\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 11.020967\n",
      "====> Epoch: 24 Average loss: 11.2389\n",
      "====> Test set loss: 11.1327\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 11.491358\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 10.822961\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 11.221913\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 11.011796\n",
      "====> Epoch: 25 Average loss: 11.1736\n",
      "====> Test set loss: 11.1187\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 11.294405\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 11.077533\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 11.038929\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 11.061684\n",
      "====> Epoch: 26 Average loss: 11.1449\n",
      "====> Test set loss: 11.0271\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 11.371720\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 11.066154\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 11.824573\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 11.279622\n",
      "====> Epoch: 27 Average loss: 11.1064\n",
      "====> Test set loss: 11.0629\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 11.038867\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 11.254529\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 11.549279\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 10.970607\n",
      "====> Epoch: 28 Average loss: 11.0685\n",
      "====> Test set loss: 11.0259\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 11.463100\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 11.172876\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 11.071051\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 11.024437\n",
      "====> Epoch: 29 Average loss: 11.0459\n",
      "====> Test set loss: 11.0274\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 10.899508\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 11.318254\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 10.344438\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 11.124943\n",
      "====> Epoch: 30 Average loss: 11.0219\n",
      "====> Test set loss: 10.9797\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 11.624239\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 11.482255\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 10.808919\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 10.810449\n",
      "====> Epoch: 31 Average loss: 10.9887\n",
      "====> Test set loss: 10.9486\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 10.969516\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 10.950982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 10.778258\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 11.354824\n",
      "====> Epoch: 32 Average loss: 10.9596\n",
      "====> Test set loss: 10.8758\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 10.881223\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 10.923767\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 10.985347\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 10.736973\n",
      "====> Epoch: 33 Average loss: 10.8941\n",
      "====> Test set loss: 10.8235\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 10.937256\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 11.183161\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 10.785748\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 10.614501\n",
      "====> Epoch: 34 Average loss: 10.9001\n",
      "====> Test set loss: 10.8384\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 10.642886\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 10.946718\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 10.899540\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 10.947985\n",
      "====> Epoch: 35 Average loss: 10.8263\n",
      "====> Test set loss: 10.7475\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 10.709097\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 10.847151\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 11.274919\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 10.497417\n",
      "====> Epoch: 36 Average loss: 10.7999\n",
      "====> Test set loss: 10.8068\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 10.544708\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 10.483947\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 10.806014\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 10.675930\n",
      "====> Epoch: 37 Average loss: 10.7564\n",
      "====> Test set loss: 10.7165\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 10.704087\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 10.425450\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 10.849393\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 10.631173\n",
      "====> Epoch: 38 Average loss: 10.7519\n",
      "====> Test set loss: 10.7315\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 10.614741\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 10.756127\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 10.437092\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 10.409025\n",
      "====> Epoch: 39 Average loss: 10.6743\n",
      "====> Test set loss: 10.6559\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 10.655912\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 10.734360\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 10.871384\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 11.022768\n",
      "====> Epoch: 40 Average loss: 10.6562\n",
      "====> Test set loss: 10.6020\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 10.816217\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 10.544459\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 10.367709\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 10.344109\n",
      "====> Epoch: 41 Average loss: 10.6329\n",
      "====> Test set loss: 10.5954\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 10.536823\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 11.029982\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 10.557474\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 10.475102\n",
      "====> Epoch: 42 Average loss: 10.5963\n",
      "====> Test set loss: 10.5407\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 10.598855\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 10.631427\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 10.466350\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 10.545608\n",
      "====> Epoch: 43 Average loss: 10.6011\n",
      "====> Test set loss: 10.5869\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 10.721181\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 10.425979\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 10.678950\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 10.290699\n",
      "====> Epoch: 44 Average loss: 10.5818\n",
      "====> Test set loss: 10.5108\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 10.585664\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 10.458451\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 10.169127\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 10.323301\n",
      "====> Epoch: 45 Average loss: 10.5440\n",
      "====> Test set loss: 10.5278\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 10.409440\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 10.695813\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 10.703066\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 10.894125\n",
      "====> Epoch: 46 Average loss: 10.5415\n",
      "====> Test set loss: 10.5064\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 10.249372\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 10.505456\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 10.354534\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 10.532875\n",
      "====> Epoch: 47 Average loss: 10.5311\n",
      "====> Test set loss: 10.5150\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 10.377612\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 10.789891\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 10.422205\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 10.689496\n",
      "====> Epoch: 48 Average loss: 10.5154\n",
      "====> Test set loss: 10.4745\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 10.938197\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 10.666235\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 10.639854\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 10.299567\n",
      "====> Epoch: 49 Average loss: 10.4813\n",
      "====> Test set loss: 10.4675\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 10.507107\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 10.433463\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 10.380956\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 10.532099\n",
      "====> Epoch: 50 Average loss: 10.4605\n",
      "====> Test set loss: 10.4625\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f137008ad10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f135379ecd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0944, 0.3144, 0.1389, 0.2100, 0.2027, 0.2134, 0.1227, 0.2354, 0.0400,\n",
       "        0.2087, 0.0254, 0.0560, 0.1008, 0.2734, 0.2338, 0.3331, 0.2775, 0.0097,\n",
       "        0.0353, 0.1003, 0.0863, 0.2600, 0.0072, 0.0267, 0.0039, 0.1069, 0.4745,\n",
       "        0.3264, 0.2125, 0.2919], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it on data where half of the features are just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49, device='cuda:0')\n",
      "tensor(42, device='cuda:0')\n",
      "tensor(48, device='cuda:0')\n",
      "tensor(44, device='cuda:0')\n",
      "tensor(44, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)\n",
    "\n",
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)\n",
    "\n",
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)\n",
    "\n",
    "actual_data = data_mapper(latent_data)\n",
    "\n",
    "\n",
    "noise_features = torch.empty(N * D).normal_(mean=0,std=0.01).reshape(N, D).to(device)\n",
    "noise_features.requires_grad_(False)\n",
    "\n",
    "actual_data = torch.cat([actual_data, noise_features], dim = 1)\n",
    "\n",
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))\n",
    "    \n",
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)\n",
    "\n",
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing how it does when calculating loss over only the non-noisy features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel_NInsta(2*D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 20.898645\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 19.929802\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.052610\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 18.303110\n",
      "====> Epoch: 1 Average loss: 19.4577\n",
      "====> Test set loss: 39.1835\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 18.139664\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 17.451143\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 16.215115\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 15.406269\n",
      "====> Epoch: 2 Average loss: 16.7949\n",
      "====> Test set loss: 36.4903\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 15.273976\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 14.205248\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 13.512135\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 13.540610\n",
      "====> Epoch: 3 Average loss: 14.0477\n",
      "====> Test set loss: 35.0437\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 13.251816\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 12.921093\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 13.159525\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 12.858677\n",
      "====> Epoch: 4 Average loss: 13.1107\n",
      "====> Test set loss: 34.5161\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 13.096476\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 12.703549\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 13.023785\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 12.456649\n",
      "====> Epoch: 5 Average loss: 12.7638\n",
      "====> Test set loss: 34.2885\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 12.676977\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 12.213898\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.817050\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 12.306253\n",
      "====> Epoch: 6 Average loss: 12.5566\n",
      "====> Test set loss: 34.0682\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 12.728925\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.858442\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.082851\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 12.378213\n",
      "====> Epoch: 7 Average loss: 12.4535\n",
      "====> Test set loss: 33.9863\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.501989\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.632045\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.139208\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.343729\n",
      "====> Epoch: 8 Average loss: 12.3713\n",
      "====> Test set loss: 33.9147\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.521918\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.336174\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 11.999560\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 12.654683\n",
      "====> Epoch: 9 Average loss: 12.2792\n",
      "====> Test set loss: 33.8102\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.319931\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 12.189857\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 11.686921\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 12.253585\n",
      "====> Epoch: 10 Average loss: 12.2216\n",
      "====> Test set loss: 33.7457\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 12.001496\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 11.861188\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 11.977320\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 12.419578\n",
      "====> Epoch: 11 Average loss: 12.1460\n",
      "====> Test set loss: 33.6651\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 12.131129\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 12.354206\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 12.135280\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 11.839419\n",
      "====> Epoch: 12 Average loss: 12.1051\n",
      "====> Test set loss: 33.6215\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 12.123150\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 12.157158\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 12.021650\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 12.191448\n",
      "====> Epoch: 13 Average loss: 12.0539\n",
      "====> Test set loss: 33.5635\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 12.256674\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 11.796141\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 11.810780\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 12.078734\n",
      "====> Epoch: 14 Average loss: 11.9859\n",
      "====> Test set loss: 33.4624\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 12.068169\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 12.273953\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 11.474892\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 12.357166\n",
      "====> Epoch: 15 Average loss: 11.9432\n",
      "====> Test set loss: 33.4283\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 11.735337\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 12.462869\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 11.564680\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 11.875435\n",
      "====> Epoch: 16 Average loss: 11.8896\n",
      "====> Test set loss: 33.3246\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 11.994597\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 11.837506\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 11.893895\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 11.422355\n",
      "====> Epoch: 17 Average loss: 11.7686\n",
      "====> Test set loss: 33.2395\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 11.510633\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 11.946126\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 11.889265\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 11.875578\n",
      "====> Epoch: 18 Average loss: 11.6911\n",
      "====> Test set loss: 33.1019\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 11.360607\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 11.215530\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 11.601254\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 11.574265\n",
      "====> Epoch: 19 Average loss: 11.5891\n",
      "====> Test set loss: 33.0293\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 11.726285\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 10.926597\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 11.476710\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 11.529606\n",
      "====> Epoch: 20 Average loss: 11.4870\n",
      "====> Test set loss: 32.9535\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 11.214949\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 11.429251\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 11.149454\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 11.608332\n",
      "====> Epoch: 21 Average loss: 11.3857\n",
      "====> Test set loss: 32.8538\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 11.544928\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 11.461097\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 11.055024\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 11.362005\n",
      "====> Epoch: 22 Average loss: 11.3553\n",
      "====> Test set loss: 32.7431\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 11.404895\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 11.303160\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 11.165370\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 11.578817\n",
      "====> Epoch: 23 Average loss: 11.2975\n",
      "====> Test set loss: 32.7211\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 11.114488\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 11.139367\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 11.144479\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 11.210698\n",
      "====> Epoch: 24 Average loss: 11.2419\n",
      "====> Test set loss: 32.5861\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 11.028425\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 11.221003\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 11.319410\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 11.248523\n",
      "====> Epoch: 25 Average loss: 11.1920\n",
      "====> Test set loss: 32.5778\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 11.336606\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 11.259481\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 11.012713\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 11.248603\n",
      "====> Epoch: 26 Average loss: 11.1816\n",
      "====> Test set loss: 32.5261\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 11.015708\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 11.174129\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 11.068004\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 11.359070\n",
      "====> Epoch: 27 Average loss: 11.1352\n",
      "====> Test set loss: 32.5161\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 11.263726\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 10.866052\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 10.890785\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 10.914146\n",
      "====> Epoch: 28 Average loss: 11.0965\n",
      "====> Test set loss: 32.4527\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 10.851835\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 11.416049\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 10.892348\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 11.437597\n",
      "====> Epoch: 29 Average loss: 11.1014\n",
      "====> Test set loss: 32.4648\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 11.054575\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 11.364286\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 10.904508\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 10.775708\n",
      "====> Epoch: 30 Average loss: 11.0760\n",
      "====> Test set loss: 32.4115\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 10.495272\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 10.863899\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 10.875204\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 11.382383\n",
      "====> Epoch: 31 Average loss: 11.0433\n",
      "====> Test set loss: 32.4260\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 10.977687\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 11.293878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 11.085241\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 11.064404\n",
      "====> Epoch: 32 Average loss: 11.0010\n",
      "====> Test set loss: 32.4116\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 11.369688\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 11.212049\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 11.255682\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 10.907515\n",
      "====> Epoch: 33 Average loss: 11.0002\n",
      "====> Test set loss: 32.3624\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 11.076082\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 11.622751\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 10.859943\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 11.019300\n",
      "====> Epoch: 34 Average loss: 10.9824\n",
      "====> Test set loss: 32.3141\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 10.731936\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 10.881388\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 10.641421\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 11.242130\n",
      "====> Epoch: 35 Average loss: 10.9390\n",
      "====> Test set loss: 32.3230\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 11.241872\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 10.953383\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 10.636180\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 11.004389\n",
      "====> Epoch: 36 Average loss: 10.9249\n",
      "====> Test set loss: 32.2543\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 10.417111\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 10.904677\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 10.598989\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 10.868629\n",
      "====> Epoch: 37 Average loss: 10.8764\n",
      "====> Test set loss: 32.3165\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 10.638142\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 11.089766\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 10.547627\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 11.132683\n",
      "====> Epoch: 38 Average loss: 10.8410\n",
      "====> Test set loss: 32.2575\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 10.864138\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 10.146031\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 11.069434\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 10.832662\n",
      "====> Epoch: 39 Average loss: 10.8408\n",
      "====> Test set loss: 32.1879\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 10.804028\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 11.268338\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 10.993727\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 11.164903\n",
      "====> Epoch: 40 Average loss: 10.7941\n",
      "====> Test set loss: 32.2226\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 10.527624\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 10.634424\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 10.431192\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 10.718707\n",
      "====> Epoch: 41 Average loss: 10.7421\n",
      "====> Test set loss: 32.1330\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 10.618585\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 10.742419\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 10.605135\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 10.486070\n",
      "====> Epoch: 42 Average loss: 10.7443\n",
      "====> Test set loss: 32.0822\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 10.676762\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 10.510604\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 10.897457\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 10.648016\n",
      "====> Epoch: 43 Average loss: 10.6683\n",
      "====> Test set loss: 32.1697\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 10.780971\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 10.430047\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 10.615076\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 10.704823\n",
      "====> Epoch: 44 Average loss: 10.6740\n",
      "====> Test set loss: 32.0469\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 11.018789\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 10.978848\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 10.362288\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 10.753587\n",
      "====> Epoch: 45 Average loss: 10.6435\n",
      "====> Test set loss: 32.0636\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 10.687198\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 10.374470\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 10.428069\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 10.404938\n",
      "====> Epoch: 46 Average loss: 10.6123\n",
      "====> Test set loss: 32.0298\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 10.577153\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 10.567597\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 10.938880\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 10.243278\n",
      "====> Epoch: 47 Average loss: 10.5833\n",
      "====> Test set loss: 32.0315\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 11.093748\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 10.696848\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 10.546124\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 10.575742\n",
      "====> Epoch: 48 Average loss: 10.6227\n",
      "====> Test set loss: 32.0218\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 10.553415\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 10.365568\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 10.654428\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 10.423650\n",
      "====> Epoch: 49 Average loss: 10.5589\n",
      "====> Test set loss: 31.9058\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 10.637044\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 10.509233\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 10.432006\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 10.551024\n",
      "====> Epoch: 50 Average loss: 10.5185\n",
      "====> Test set loss: 31.9691\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f137003eb10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD7CAYAAAB+B7/XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAfeElEQVR4nO3da7ScVZ3n8e8vJxciEK6CMckIQmREWuMQI90sWy4KWYwj3pDQraTHLKMsaNFppyVqi4q6REWatkeWUWiCjVwapKERDGkUWa4Jl6ARwk0yEOSQCEIAQZRwzvnPi2dXqJxU1XmqnqfqHJ76fbKedap2PXs/O+fFrn325b8VEZiZWXVMGu8KmJlZudywm5lVjBt2M7OKccNuZlYxbtjNzCrGDbuZWcUUatglLZR0n6T1kk4rq1JmZtY5dbqOXdIA8Gvg7cAgcBtwQkTcXV71zMysXZML5F0ArI+IBwAkXQIcCzRt2Hd62b7eDWVmuTz73IMqWsYLjz+Qq82ZsuerCz9rIikyFDMLeLju/WBKMzOzcVSkx97oG267b0dJS4GlAFOn7MGUyTsXeKSZWRtGhse7BuOiSMM+CMypez8b2Dj6pohYDiwHmLbDnHhhZKjAI83M2jDcn+1NkYb9NmCupH2BR4BFwF+VUiszsxJEjIx3FcZFxw17RAxJOgVYCQwA50fEXaXVzMysqBE37G2LiGuBa0uqi5lZudxj774pk3r6ODPrd548NTOrGPfY2yNpB+AmYFoq5/KIOL2sipmZFRVeFdO254EjIuJZSVOAn0u6LiJuLqluZmbFePK0PZEFmXk2vZ2Srpbbd4f79M8iMxsnfdrmFI3uOCBpLfAYsCoibimnWmZmJRgZzndVTKGGPSKGI2Ie2a7TBZIOGn2PpKWS1khaMzT07PaFmJl1S4zkuyqmlFUxEfGUpBuBhcC6UZ9tDSkwddrsGK7gt6OZTVB9OnnacY9d0ssl7ZpeTwfeBtxbVsXMzAobGcl3VUyRHvtMYEU6cGMScFlEXFNOtczMiovozxGCIqti7gDeWGJdzMzKVcHx8zx6uvP0FTvu1svHmVm/q+AwSx6FVsWYmU1oJa2KkTRH0k8l3SPpLkmnpvTPS3pE0tp0HVOXZ5mk9ZLuk3R0XfrBku5Mn/2TJKX0aZIuTem3SNqnLs9iSfena/FY9S3UY0+Tp98DDiLbnPShiFhdpEwzs9IMv1BWSUPA30XELyTtDNwuaVX67OyI+Eb9zZIOJDuj4nXAK4H/lPSayAb9zyU7Ve5msui4C4HrgCXAkxGxv6RFwJnA8ZJ2B04H5pO1s7dLujoinmxW2aI99nOAH0fEfwXeANxTsDwzs/KUtComIjZFxC/S62fI2rpWZzwfC1wSEc9HxIPAerK9PjOBGRGxOu3evxB4V12eFen15cCRqTd/NNkG0M2pMV9F9mXQVJEgYDOAvwT+BiAitgBbWuW5eud9O32cmVn7ujB5moZI3gjcAhwKnCLpRGANWa/+SbJGvz5u1mBKeyG9Hp1O+vkwbD3I6Glgj/r0BnkaKtJjfzXwO+BfJP1S0vck7VigPDOzcuXssdfvkE/X0kbFSdoJuAL4eET8nmxYZT9gHrAJOKt2a4Ps0SK90zwNFWnYJwP/DTg3It4I/AE4bfRN9b+wK559qMDjzMzalLNhj4jlETG/7lo+uqgUxfYK4KKI+CFARDyaQquMAN8FFqTbB4E5ddlnAxtT+uwG6dvkkTQZ2AXY3KKspopMng4Cg3WBvy6nQcNeH1Jg8T7vjbMLPNDM+seKsW8ZU5Q0eZrGus8D7omIb9alz4yITentu3kxpMrVwA8kfZNs8nQucGtEDEt6RtIhZEM5JwLfqsuzGFgNvA/4SUSEpJXAVyTV1osfBSxrVd8iG5R+K+lhSQdExH3AkcDdnZZnZla68sbYDwU+CNyZItoCfBo4QdI8sqGRDcBHACLiLkmXkbWJQ8DJ8eI22JOAC4DpZKthrkvp5wHfl7SerKe+KJW1WdIZwG3pvi9GxOZWlVU2MduZ9B/6HjAVeAD4n62W4Cze572dP8zM+sqKDVc0Gltuyx9vWJ6rzZl+5NLCz5pICq1jj4i1ZGsrzcwmHocU6L5zDm3amTczK1+fhhToacNuZtZTfdpjL3o03qmS1qXYCR8vq1JmZqUYGsp3VUyRnacHAR8mW7e5BfixpB9FxP3N8ux1+a87fZyZ9ZktF5VQiHvsbXstcHNEPBcRQ8DPyNZxmplNDH16glKRhn0d8JeS9pD0MuAYtt0dZWY2vnyYdXsi4h5JZ5JFGnsW+BXZQvxtpJgLSwEGBnZl0oDDyZhZj1SwN55H0XXs55HtlkLSV9g2alntnq0hBWbs+GpvUDKz3qlgbzyPogdt7BURj0n6L8B7gD8vp1pmZiWo4IqXPIquY79C0h5kMYZPbhVOwMys5wqETHkpKzoU85ayKmJmVjqPsXff4Xsc2MvHmVm/c8NuZlYxfTp5OuY6dknnS3pM0rq6tK9LulfSHZKulLRrd6tpZtaB4eF8V8Xk6bFfAPwz2WnaNauAZenA1TPJTvP41FgF/efj68a6xcysPH06FDNmjz0ibiI7zaM+7foURgCyk7hnb5fRzGy89WlIgTLG2D8EXFpCOWZm5erTMfaiG5Q+QxZGoGkctvqQAlOn7M7kyTsXeaSZWW4x4nXsbZG0GHgHcGS0ODi1PqTAtB3mxNBI9SYqzGyCquAwSx4dNeySFpJNlr41Ip4rt0pmZiWp4IqXPMZs2CVdDBwG7ClpEDidbBXMNGCVJMjisn+0i/U0M2ufe+yNRcQJDZLP60JdzMzK5Ya9+767x1t7+Tgz63cOAmZmVjF92mPvNKTA5yU9Imltuo7pbjXNzDowEvmuiuk0pADA2RHxjXYetvSJm9q53cz62IllFOJVMY1FxE2S9ul+VczMyhUeimnbKSm64/mSdiutRmZmZenToZhOG/Zzgf2AecAm4KxmN0paKmmNpDXDw892+Dgzsw7ESL6rYjpaFRMRj9ZeS/oucE2Le7eGFJgydVYMO6SAmfVKSb1xSXPI5hlfAYwAyyPiHEm7kwVB3AfYALy/dvazpGXAEmAY+FhErEzpB5PNXU4HrgVOjYiQNC0942DgCeD4iNiQ8iwGPpuq86WIWNGqvh312CXNrHv7bsCB1s1s4hkaznflKAn4u4h4LXAIcLKkA4HTgBsiYi5wQ3pP+mwR8DpgIfBtSQOprHPJAiPOTdfClL4EeDIi9gfOBs5MZe1OtuP/zcAC4PSxhr/zLHe8GFgNHCBpUNIS4GuS7pR0B3A48Ikxfy1mZr1W0lBMRGyKiF+k188A9wCzgGOBWu95BfCu9PpY4JKIeD4iHgTWAwtSp3hGRKxOwRMvHJWnVtblwJHKYrYcDayKiM3pr4FVvPhl0FBPQwq8bOoOnWQzM+tMFyZG0yrBNwK3AHtHxCbIGn9Je6XbZpEdQlQzmNJeSK9Hp9fyPJzKGpL0NLBHfXqDPA1556mZVVbe5Y7150Yky9P84Oj7dgKuAD4eEb9PQRAbFtmoOi3SO83TUKc7T+dJujntOl0jacFY5ZiZ9VzO5Y4RsTwi5tddjRr1KWSN+kUR8cOU/GhtzjH9fCylDwJz6rLPBjam9NkN0rfJI2kysAvZsaTNymoqz+TpBWw/nvM14AsRMQ/4XHpvZjaxlLSOPY11nwfcExHfrPvoamBxer0YuKoufZGkaZL2JZskvTUN2zwj6ZBU5omj8tTKeh/wkzQOvxI4StJuadL0qJTWVKc7TwOYkV7vwhjfHjVTBzzyY2Y9VF5IgUOBDwJ3Slqb0j4NfBW4LC0q+Q1wHEBE3CXpMuBushU1J0dErTIn8eJyx+vSBdkXx/clrSfrqS9KZW2WdAZwW7rvixGxuVVl1eJUuxdvyhr2ayLioPT+tWTfGCLr9f9FRDw0Vjkv3+WA6m3xMrOu+N3T9zUdwM7rmY//j1xtzs7/+B+FnzWRdLrz9CTgExExh2ypY9NVMvU7T/+05akOH2dm1gGHFGjLYqA2efBvZIvmG6qflNhh6q4dPs7MrAMjI/muiul00Hsj8FbgRuAI4P48mU7b5U0dPs7MrAMV7I3n0elh1h8GzklLcv7Etus/zcwmBjfsjTXZeQpZoBozswkrhqs3zJJHT9cf7relP3/JZjZO3GM3M6uW6NOGPU9IgTmSfirpHkl3STo1pR+X3o9Imt/9qpqZtalPlzvm6bHX4hD/QtLOwO2SVpHFYH8P8J1uVtDMrGN9OvqbZ/J0E9nxd0TEM5LuAWZFxCqAFtHNtvPXv/+/HVbTzPrNH0ooI4b6s2Vva4x9VBxiM7OJrT/b9fwN++g4xG3k2xrneOqU3Zk8eee2K2lm1glPnrbQJA5xLvUhBdyom1lPjeS8KibPztNmcYjbNlzBmAxmNnH1a489z1BMszjE04BvAS8HfiRpbUQc3Z1qmpl1oE/7knlWxfycxmfuAVxZbnXMzMoTQ+Ndg/HR052n0ydP7eXjzKzPhXvsZmYV06cNe8chBeo+/6SkkLRn96ppZta+GMl3VU3HIQUi4m5Jc4C3kx3iamY2oVSx0c6j45ACZKdvnw38PXBVnoc9N/R85zU1M2tTDFfqjOrcOg4pIOmdwCMR8at24sWYmfWKe+xjqA8pQDY88xngqBz5toYUGJi8KwMDO3VWUzOzNsVIf3Y6czXso0MKSPozYF+g1lufDfxC0oKI+G193ohYDiwHeOHxB/pzG5iZjQv32JtoFFIgIu4E9qq7ZwMwPyIe71I9zczaFtGfPfY8QcBqIQWOkLQ2Xcd0uV5mZoV5uWMTY4QUqN2zT1kVMjMry4hXxXTfy175ll4+zsxewl7Y8kjhMjx5amZWMW7Ym0i7Sy8EXkEWeWF5RJwj6VLggHTbrsBTETGvazU1M2tT9Ok6vCIhBY6v3SDpLODpblXSzKwT7rE3MUZIgdpyyPcDR4xV1qRJuU7iMzMrRZnLHSWdD7wDeCwiDkppnwc+DPwu3fbpiLg2fbYMWAIMAx+LiJUp/WDgAmA6cC1wakSEpGlkoyMHA08Ax0fEhpRnMfDZ9IwvRcSKVnVtq6WtDylQl/wW4NGIuL+dsszMum14WLmunC4AFjZIPzsi5qWr1qgfCCwCXpfyfFvSQLr/XLLd+HPTVStzCfBkROxPFofrzFTW7sDpwJuBBcDpknZrVdHcDXt9SIGI+H3dRycAF7fIt1TSGklrhoefzfs4M7PCIpTryldW3ARszvnoY4FLIuL5iHgQWA8skDQTmBERqyMiyHro76rLU+uJXw4cmUZEjgZWRcTmiHgSWEXjL5itOgopUJc+GXgP2Z8ODdWHFJgydVaM+EBrM+uRHo2xnyLpRGAN2Xzkk2TD1TfX3TOY0l5Ir0enk34+DBARQ5KeBvaoT2+Qp6E8B21sF1KgztuAeyNicPucZmbjKyLfVT+ykK6lOR9xLrAfMI9sLvKslN7oGyVapHeap6E8PfZaSIE7Ja1NabUJgkW0GIYxMxtPeXvs9SMLbZUf8WjttaTvAtekt4PAnLpbZwMbU/rsBun1eQbTaMguZEM/g8Bho/Lc2KpehUIKRMTfjJXfzGy8DI90dyWepJlp5SDAu4F16fXVwA8kfRN4Jdkk6a0RMSzpGUmHkC1CORH4Vl2excBq4H3AT9JqmZXAV+omTI8ClrWqV093nh6+95/18nFm1ufK3KAk6WKynvOekgbJVqocJmke2dDIBuAj2XPjLkmXkS0LHwJOjojhVNRJvLjc8bp0QTbk/X1J68l66otSWZslnQHclu77YkS0nMRV9HBr1lFzFvbpPjAza9f1D/+48Mzn2le9M1ebM++hqyu1kynP5OkOkm6V9CtJd0n6QkrfXdIqSfenny3XVZqZ9VqZyx1fSvIMQD0PHBERbyCb+V2YxodOA26IiLnADem9mdmEkXdVTNXkmTwNoLazaEq6gmwx/WEpfQXZLO2nWpX100fv7LCaZmbtG6lgbzyPvBuUBoDbgf2B/xMRt0jauzYbHBGbJO3VshAzsx7r9qqYiSrX/zoihlNI3tlk22IPyvuA+oX/IyN/6LSeZmZti5xX1bS13DEinpJ0I1mcgkdrazhT/IPHmuTZuvB/2g5zqvg7NLMJql+HYvKsinm5pF3T6+mkMAK8uJie9POqblXSzKwT/boqJk+PfSawIo2zTwIui4hrJK0GLpO0BPgNcFwX62lm1rZ+DTmYZ1XMHWQx2EenPwEc2Y1KmZmVIRpHQ6m8noYUcMheM+uloQoOs+TR04bdzKyX+rXHXiSkwBmS7pC0VtL1kl7Z/eqameU3kvOqmiIhBb4eEa9P69uvAT7XxXqambUtUK6rajoOKTDq3NMdqeY6fzN7CatibzyPjkMKpPQvkwWKfxo4vFuVNDPrxHAFe+N5FAopEBGfiYg5wEXAKY3yOqSAmY2XEeW7qqZISIF1dR/9APgR2Ykio/NsDSkweeqsKkbINLMJasQ99saahRSQNLfutneShRkwM5swHASsuWYhBa6QdADZ/MRDwEe7WE8zs7Z58rSJFiEF3tuVGpmZlWRE/TkU09Odp5MnDfTycWbW54bHuwLjxCEFzKyyqrjiJY+OQwqkz/5W0n0p/WvdraqZWXtGUK6ravL02GshBZ6VNAX4uaTrgOlkB1q/PiKez3Pm6dBIv/5hZGbjoYorXvLoOKQAcBLw1Yh4Pt3X8Gg8M7Px4qGYFiQNSFpLdq7pqhRS4DXAWyTdIulnkt7UzYqambWrX6M75po8jYhhYF7aqHRlCikwGdgNOAR4E9kxea9OPfytJC0FlgJoYBcmTdqxzPqbmTU17B772CLiKeBGspACg8API3Mr2Rffng3yLI+I+REx3426mfVSv/bYOw4pAPw7cERKfw0wFXi8e1U1M2tPvzbsRUIKTAXOl7QO2AIsHj0MY2Y2nvr0yNNCIQW2AB/oRqXMzMpQxd54Hj3deTowqa0hfTOzQsrcOSPpfOAdwGMRcVBK2x24FNgH2AC8PyKeTJ8tA5akanwsIlam9IOBC8j2Al0LnBoRIWkacCFwMPAEcHxEbEh5FgOfTVX5UkSsaFVXt7RmVlklH7RxAdnCkXqnATdExFzghvQeSQcCi4DXpTzfTsPZAOeSrRScm65amUuAJyNif+Bs4MxU1u5kZ128GVgAnC5pt1YV7TikgKQ3SFot6U5J/yFpxlhlmZn1UpmTpxFxE7B5VPKxQK33vAJ4V136JRHxfEQ8CKwnO31uJjAjIlanOckLR+WplXU5cKQkAUeT7R/anP4aWMX2XzDbKBJS4FvAJyPiZ5I+BPxv4B9aFeS5VTPrpbyNdv1+m2R5Ov1tLHtHxCaAiNhUF1plFnBz3X2DKe2F9Hp0ei3Pw6msIUlPA3vUpzfI01CRkAIHADel9FXASsZo2M3MeilvV7L+CM+SNBrgiRbpneZpqEhIgXVkR+IBHAfMyVOWmVmv9OAw60fT8ArpZy1m1iDbtomzgY0pfXaD9G3ySJoM7EI29NOsrKZyNewRMRwR81KBC1JIgQ8BJ0u6HdiZbC37diQtlbRG0pqR4T/keZyZWSmGc14FXA0sTq8XA1fVpS+SNE3SvmSTpLemYZtnJB2Sxs9PHJWnVtb7gJ+kEZOVwFGSdkuTpkeltKbaWu4YEU9JuhFYGBHfSA+o7Tz9703ybP0TZ/LUWTHicXYz65GREgP3SroYOAzYU9Ig2UqVr5LFyVoC/IZs9IKIuEvSZcDdwBBwcoq5BVlk3AvIljtely6A84DvS1pP1lNflMraLOkM4LZ03xcjYvQk7rZ1HWtCU9LLgRdSoz4duJ5sGc6tEfGYpEmpkjdGxPmtypo8dZZbdTPLZWjLI4X3jZ7xqr/O1eb8w0MXVWqPap6hmJnATyXdQfaNsSoirgFOkPRrsrgxG4F/6V41zczaFzmvqikSUuAc4Jx2HjapT08MN7Px4ZACZmYVM6Qq9sfHljukQFry+EtJ16T3X5d0r6Q7JF1ZC+1rZjZR9OtQTDuxYk4F7ql7vwo4KCJeD/waWFZmxczMinI89hYkzSZbzvhl4H8BRMT1dbfcTLbusiWHFDCzXipzueNLSd4e+z8Cf0/zL7cP8eJaTDOzCcFDMU1IqsUfvr3J558hW4B/UZPPX9x5OuKdp2bWOx6Kae5Q4J2SjgF2AGZI+teI+EAK/v4O4Mhmx+LV7zyd4g1KZtZDw5Xsj48tzzr2ZaSJUUmHkYXq/YCkhcCngLdGxHN5HjbJJyiZWQ9VsTeeR5F17P8MTANWZbFsuDkiPlpKrczMShDusY8tIm4Ebkyv9+9CfczMSuMeew+MjPTrr9nMxkO/Lnd0SAEzq6z+bNaLhRT4vKRHJK1N1zHdq6aZWfuGiFxX1bTTY6+FFJhRl3Z2OnDDzGzC6dfJ07xnntZCCnyvu9UxMytPv25QKhpS4JQU3fH8dBafmdmEETn/VU2RkALnAvsB84BNwFlN8jukgJmNi37tsRcKKVC7QdJ3gWsaZXZIATMbL8N9GlF2zB57RCyLiNkRsQ/Zqdk/SSEFZtbd9m5gXZfqaGbWkREi11U1Rdaxf03SPLKlohuAj5RSIzOzklRx/DyPIiEFPtiF+piZlaaK4+d5eOepmVVWFYdZ8nDDbmaV1a9DMUVCCsyTdHMKJ7BG0oLuVdPMrH3DEbmuqmnn5ItaSIGarwFfiIh5wOfSezOzCaNfV8UUCSkQvBg3ZhdgY7lVMzMrxhuUWquFFNi5Lu3jwEpJ3yD7gviLkutmZlaIx9ibaBFS4CTgExExB/gEcF6T/A4pYGbjwkMxzdVCCmwALgGOkPSvwGLgh+mefwMaTp5GxPKImB8R8ydN2rGEKpuZ5RMRua48JG2QdGdtwUhK213SKkn3p5+71d2/TNJ6SfdJOrou/eBUznpJ/6R0aLSkaZIuTem3SNqn0/93xyEFyMbU35puOwK4v9NKmJl1wzCR62rD4RExLyLmp/enATdExFzghvQeSQeStZevAxYC35Y0kPKcCywF5qZrYUpfAjyZzpM+Gziz0/93O6tiRvswcJakXwFfSRU1M5swejAUcyywIr1eAbyrLv2SiHg+Ih4E1gMLUoytGRGxOrI/FS4cladW1uXAkbXefLuKhBT4OXBwJw81M+uFvMMseYsDrpcUwHdS5Nq9I2JTetYmSXule2cBN9flHUxpL6TXo9NreR5OZQ1JehrYA3i83Yp656mZVVbe3rikpWw76rA8Ndz1Do2IjanxXiXp3lZFNkiLFumt8rTNDbuZVVbe5Y7150a0uGdj+vmYpCvJFow8Kmlm6q3PBB5Ltw8Cc+qyzyablxxMr0en1+cZlDSZbH/Q5lz/gVHyblBqNBt8nKS7JI1Imj9WGWZmvVZWSAFJO0raufYaOIrsDIqryVYIkn5elV5fDSxKK132JZskvTUN2zwj6ZA0fn7iqDy1st5HtlCl6z32wyOifqxnHfAe4DudPNjMrNtKXKO+N3BlmsucDPwgIn4s6TbgMklLgN8AxwFExF2SLgPuBoaAkyNiOJV1EnABMB24Ll2Q7QX6vqT1ZD31RZ1WtuOhmIi4B6CdSdsOJ3jNzDpSVsMeEQ8Ab2iQ/gRwZJM8Xwa+3CB9DXBQg/Q/kb4Yisq73LE2G3x7mmQwM5vwytyg9FKSt8e+3WxwRNyUJ2P9bPPAwK5MGvDuUzPrjSqGC8gjV4+9fjYYqM0G57JNSAE36mbWQ5HzX9XkCQLWbDbYzGxCG46RXFfV5Omx7w38PIUOuBX4UZoNfrekQeDPgR9JWtnNipqZtctj7E20mA2+kmxYxsxsQurXMfae7jyt4jejmU1cVRw/z8MhBcysskb6tDPZcUiBus8+KSkk7dmdKpqZdaZfV8UUCSmApDnA28m20pqZTShVXPGSR9GhmLPJDrm+aqwbASZNKnKuh5lZezwU09p2IQUkvRN4JCJ+1bXamZkV4KGY1hoFmP8M2WallrYJKTB5VwYGduq4smZm7ejXHrvaXYIo6fPAMPC3wHMpuRYsfkFE/LZZ3qnTZvfnb9nM2rbl+cHC4WBfvecbc7U5Dzz+y0qFnh2zx57CCEyKiGfqQgp8MSL2qrtnAzB/9OSqmdl4Gt4aAr2/5BmKaRhgvqu1MjMrQb9uiuw4pMCoe/Ypq0JmZmVxSAEzs4pxj93MrGL6dVVMroY9TY4+Q7YaZigi5ku6FDgg3bIr8FREzOtKLc3MOlDFNep5dBxSICKOr72WdBbwdJkVMzMryiEFOqRsucz7gSOKV8fMrDz9OsbecUiBOm8BHo2I+8utmplZMSMRua6q6TikQETclD47Abi4WcZtQgoM7IoPtDazXunXHnunIQWejYhvSJoMPAIcHBGDY+V1SAEzy6uMkAK77LRfrjbn6Wf/X6VCCow5FCNpR0k7116ThRRYlz5+G3BvnkbdzKzXfJh1c61CCiyixTCMmdl46tdVMW0PxRThoRgzy6uMoZjp01+Vq8354x8fqtRQTE93nlZx9tnMJq4qDrPk4ZACZlZZ3nlqZlYx7rGbmVVMvw7/9nTy1KwRSUsjYvl418OsKvKGFDDrptFhKsysADfsZmYV44bdzKxi3LDbRODxdbMSefLUzKxi3GM3M6sYN+w2biQtlHSfpPWSThvv+phVhYdibFxIGgB+DbwdGARuA06IiLvHtWJmFeAeu42XBcD6iHggIrYAlwDHjnOdzCrBDbuNl1nAw3XvB1OamRXkht3GS6P41x4XNCuBG3YbL4PAnLr3s4GN41QXs0pxw27j5TZgrqR9JU0lO2bx6nGuk1klOGyvjYuIGJJ0CrASGADOj4i7xrlaZpXg5Y5mZhXjoRgzs4pxw25mVjFu2M3MKsYNu5lZxbhhNzOrGDfsZmYV44bdzKxi3LCbmVXM/wcm8Uz5zAN0kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f13536104d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0835, 0.2764, 0.1282, 0.2090, 0.2489, 0.2180, 0.0739, 0.2250, 0.0259,\n",
       "        0.2526, 0.0233, 0.0441, 0.1089, 0.2853, 0.1896, 0.3074, 0.2966, 0.0093,\n",
       "        0.0312, 0.0836, 0.0766, 0.2295, 0.0061, 0.0239, 0.0037, 0.0596, 0.4819,\n",
       "        0.3167, 0.2430, 0.3332], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1053, 0.0932, 0.1384, 0.0915, 0.0989, 0.0440, 0.1527, 0.1035, 0.0792,\n",
       "        0.1206, 0.0272, 0.0562, 0.0662, 0.1167, 0.1647, 0.1512, 0.0543, 0.0120,\n",
       "        0.0584, 0.1384, 0.0704, 0.1009, 0.0105, 0.0369, 0.0054, 0.1276, 0.0424,\n",
       "        0.1622, 0.0848, 0.1467], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when you calculate the loss over all the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel_NInsta(2*D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 42.446293\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 41.328918\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 40.605656\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 39.730019\n",
      "====> Epoch: 1 Average loss: 40.9412\n",
      "====> Test set loss: 39.5725\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 39.534889\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.611050\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 37.899136\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 36.834385\n",
      "====> Epoch: 2 Average loss: 38.3583\n",
      "====> Test set loss: 36.8626\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 36.771805\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 35.668232\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 35.159439\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.249161\n",
      "====> Epoch: 3 Average loss: 35.3995\n",
      "====> Test set loss: 34.4142\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.375629\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 33.946270\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 34.212067\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.694859\n",
      "====> Epoch: 4 Average loss: 34.1176\n",
      "====> Test set loss: 33.8168\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 33.906837\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.438404\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.543762\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.563469\n",
      "====> Epoch: 5 Average loss: 33.6732\n",
      "====> Test set loss: 33.5090\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 32.991928\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.126549\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.546253\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 33.282097\n",
      "====> Epoch: 6 Average loss: 33.4492\n",
      "====> Test set loss: 33.3315\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 32.962669\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 32.936424\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 33.164551\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 33.485981\n",
      "====> Epoch: 7 Average loss: 33.3043\n",
      "====> Test set loss: 33.2045\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 33.039600\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 33.080521\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 33.348656\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 33.160374\n",
      "====> Epoch: 8 Average loss: 33.1989\n",
      "====> Test set loss: 33.1354\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 32.664421\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 33.239861\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 32.728313\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 32.967358\n",
      "====> Epoch: 9 Average loss: 33.1136\n",
      "====> Test set loss: 33.0853\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 33.109112\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 33.735287\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 33.041481\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 32.764820\n",
      "====> Epoch: 10 Average loss: 33.0457\n",
      "====> Test set loss: 33.0044\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 33.105267\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 32.864410\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 33.026413\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 32.680058\n",
      "====> Epoch: 11 Average loss: 32.9846\n",
      "====> Test set loss: 32.9955\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 32.606941\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 32.681747\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 32.785168\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 32.619656\n",
      "====> Epoch: 12 Average loss: 32.9432\n",
      "====> Test set loss: 32.9133\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 33.004787\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 32.863918\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 33.058399\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 32.877701\n",
      "====> Epoch: 13 Average loss: 32.9231\n",
      "====> Test set loss: 32.8555\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 32.984726\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 33.065968\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 32.894993\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 32.935406\n",
      "====> Epoch: 14 Average loss: 32.8541\n",
      "====> Test set loss: 32.7786\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 33.122131\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 32.841957\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 32.856430\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 33.220970\n",
      "====> Epoch: 15 Average loss: 32.8188\n",
      "====> Test set loss: 32.7219\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 32.749348\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 33.052036\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 33.171280\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 32.651783\n",
      "====> Epoch: 16 Average loss: 32.7770\n",
      "====> Test set loss: 32.6934\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 32.838543\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 32.761833\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 32.503162\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 32.831299\n",
      "====> Epoch: 17 Average loss: 32.7217\n",
      "====> Test set loss: 32.6788\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 32.780327\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 32.487713\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 32.380394\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 32.383320\n",
      "====> Epoch: 18 Average loss: 32.6682\n",
      "====> Test set loss: 32.5944\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 32.884365\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 32.600651\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 32.098583\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 32.328938\n",
      "====> Epoch: 19 Average loss: 32.5874\n",
      "====> Test set loss: 32.5174\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 32.407879\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 32.705219\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 32.583576\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 32.232094\n",
      "====> Epoch: 20 Average loss: 32.4984\n",
      "====> Test set loss: 32.3799\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 32.319225\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 32.246006\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 32.341473\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 32.314320\n",
      "====> Epoch: 21 Average loss: 32.4044\n",
      "====> Test set loss: 32.2808\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 31.832569\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 32.164204\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 32.505959\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 32.232235\n",
      "====> Epoch: 22 Average loss: 32.2969\n",
      "====> Test set loss: 32.2510\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 32.002258\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 32.476452\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 32.230785\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 32.355484\n",
      "====> Epoch: 23 Average loss: 32.2121\n",
      "====> Test set loss: 32.1616\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 32.342594\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 32.171940\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 32.402031\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 32.318935\n",
      "====> Epoch: 24 Average loss: 32.1399\n",
      "====> Test set loss: 32.0354\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 32.017647\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 31.981970\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 32.264278\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 32.354725\n",
      "====> Epoch: 25 Average loss: 32.0928\n",
      "====> Test set loss: 31.9933\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 32.377022\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 32.267014\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 31.967045\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 32.021240\n",
      "====> Epoch: 26 Average loss: 32.0152\n",
      "====> Test set loss: 31.9491\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 31.760992\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 32.094826\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 31.931671\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 32.019150\n",
      "====> Epoch: 27 Average loss: 31.9710\n",
      "====> Test set loss: 31.9540\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 31.930660\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 32.065456\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 32.076736\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 31.900370\n",
      "====> Epoch: 28 Average loss: 31.9327\n",
      "====> Test set loss: 31.9266\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 32.014698\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 32.096966\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 31.921310\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 31.940002\n",
      "====> Epoch: 29 Average loss: 31.9086\n",
      "====> Test set loss: 31.8814\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 32.246155\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 31.702139\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 32.311649\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 31.731857\n",
      "====> Epoch: 30 Average loss: 31.9024\n",
      "====> Test set loss: 31.8764\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 31.898808\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 31.850677\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 31.500322\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 31.617294\n",
      "====> Epoch: 31 Average loss: 31.8526\n",
      "====> Test set loss: 31.7784\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 31.994930\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 31.772135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 31.885035\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 31.701113\n",
      "====> Epoch: 32 Average loss: 31.8344\n",
      "====> Test set loss: 31.7791\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 31.740072\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 31.768677\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 31.612665\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 31.807512\n",
      "====> Epoch: 33 Average loss: 31.8000\n",
      "====> Test set loss: 31.7884\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 31.718269\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 31.849508\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 31.296680\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 31.802605\n",
      "====> Epoch: 34 Average loss: 31.7971\n",
      "====> Test set loss: 31.7195\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 31.652014\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 31.663940\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 31.916801\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 31.255962\n",
      "====> Epoch: 35 Average loss: 31.7636\n",
      "====> Test set loss: 31.7048\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 32.048397\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 31.410902\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 32.263374\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 31.603758\n",
      "====> Epoch: 36 Average loss: 31.7586\n",
      "====> Test set loss: 31.7060\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 31.987267\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 31.732126\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 32.160622\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 31.529741\n",
      "====> Epoch: 37 Average loss: 31.7503\n",
      "====> Test set loss: 31.6400\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 32.156082\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 31.835756\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 31.468067\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 31.675859\n",
      "====> Epoch: 38 Average loss: 31.7346\n",
      "====> Test set loss: 31.6709\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 31.593239\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 31.681396\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 31.562864\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 31.665997\n",
      "====> Epoch: 39 Average loss: 31.6767\n",
      "====> Test set loss: 31.6317\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 31.634600\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 31.800032\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 31.823067\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 31.787752\n",
      "====> Epoch: 40 Average loss: 31.6649\n",
      "====> Test set loss: 31.6599\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 31.871138\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 31.514061\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 31.688164\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 31.821833\n",
      "====> Epoch: 41 Average loss: 31.6559\n",
      "====> Test set loss: 31.6078\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 31.405378\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 32.326866\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 31.377939\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 31.839144\n",
      "====> Epoch: 42 Average loss: 31.6419\n",
      "====> Test set loss: 31.5855\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 31.878559\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 31.608099\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 31.891603\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 31.556215\n",
      "====> Epoch: 43 Average loss: 31.6227\n",
      "====> Test set loss: 31.5769\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 31.589916\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 31.737070\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 31.626396\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 31.151709\n",
      "====> Epoch: 44 Average loss: 31.5852\n",
      "====> Test set loss: 31.4995\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 31.702906\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 31.448261\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 31.666382\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 31.613562\n",
      "====> Epoch: 45 Average loss: 31.5430\n",
      "====> Test set loss: 31.5017\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 31.780155\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 31.373156\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 31.703520\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 31.541447\n",
      "====> Epoch: 46 Average loss: 31.5375\n",
      "====> Test set loss: 31.4937\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 31.595526\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 31.640348\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 31.490448\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 31.727037\n",
      "====> Epoch: 47 Average loss: 31.5009\n",
      "====> Test set loss: 31.4293\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 31.455952\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 31.570715\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 31.636564\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 31.259434\n",
      "====> Epoch: 48 Average loss: 31.4499\n",
      "====> Test set loss: 31.4343\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 31.515604\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 31.668749\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 31.654142\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 31.363535\n",
      "====> Epoch: 49 Average loss: 31.4378\n",
      "====> Test set loss: 31.3975\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 31.213816\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 31.444927\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 31.135721\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 30.818291\n",
      "====> Epoch: 50 Average loss: 31.3939\n",
      "====> Test set loss: 31.3299\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = 2*D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f13536a4710>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1353462a90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1120, 0.3104, 0.1491, 0.1965, 0.2159, 0.2077, 0.0917, 0.2180, 0.0381,\n",
       "        0.2108, 0.0271, 0.0433, 0.0943, 0.2873, 0.1982, 0.3068, 0.2757, 0.0091,\n",
       "        0.0451, 0.1307, 0.0751, 0.2373, 0.0070, 0.0318, 0.0044, 0.0664, 0.4658,\n",
       "        0.3474, 0.2159, 0.3015], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1178, 0.0878, 0.1442, 0.1132, 0.1142, 0.0472, 0.1130, 0.1274, 0.0512,\n",
       "        0.1322, 0.0262, 0.0540, 0.0739, 0.1397, 0.1900, 0.1893, 0.0439, 0.0100,\n",
       "        0.0546, 0.1632, 0.1115, 0.1045, 0.0093, 0.0342, 0.0057, 0.1189, 0.0575,\n",
       "        0.1954, 0.0952, 0.1700], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that doing the Gumbel trick per batch does make it learn better which features are good. Still less useful than the gradient choosing though. Had to drop the the temperature (but not too much) to get stable, clean-cut heatmaps. Increasing the number of epochs helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu]",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
