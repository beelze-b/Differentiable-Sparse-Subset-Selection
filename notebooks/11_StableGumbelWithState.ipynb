{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know from Notebook 10 that trying to make it learn one set of logits per batch is good and gets features right.\n",
    "\n",
    "How can we make the logits consistent over batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will collect the gradients and gumbel selected values after 5 epochs in each mode. The two modes will be the behavior before burn-in and the behavior after mode-in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will follow Notebook 10 with slight modifications (described right before). Remember in Notebook 10, we explored behaviors when all the features were real vs when half the features were noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 600\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 10000\n",
    "z_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14, device='cuda:0')\n",
      "tensor(18, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor(17, device='cuda:0')\n",
      "tensor(16, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)\n",
    "\n",
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)\n",
    "\n",
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)\n",
    "\n",
    "actual_data = data_mapper(latent_data)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add noiise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0013,  0.0135,  0.0054,  ..., -0.0047,  0.0033, -0.0097],\n",
       "        [ 0.0080, -0.0057,  0.0010,  ...,  0.0009, -0.0134,  0.0105],\n",
       "        [-0.0103, -0.0029,  0.0185,  ..., -0.0133, -0.0037,  0.0134],\n",
       "        ...,\n",
       "        [ 0.0073, -0.0149, -0.0108,  ..., -0.0047, -0.0137,  0.0070],\n",
       "        [ 0.0006, -0.0141, -0.0124,  ..., -0.0085,  0.0069, -0.0110],\n",
       "        [-0.0159,  0.0177, -0.0087,  ..., -0.0076, -0.0009,  0.0078]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_features = torch.empty(N * D).normal_(mean=0,std=0.01).reshape(N, D).to(device)\n",
    "noise_features.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = torch.cat([actual_data, noise_features], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 60])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)\n",
    "\n",
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_t = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of what worked before\n",
    "Vanilla Gumbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_truncated = VAE_Gumbel(2*D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradients_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "gradient_post_burn_in = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_post_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                                      vae_gumbel_trunc_optimizer, \n",
    "                                                      epoch, \n",
    "                                                      batch_size, \n",
    "                                                      Dim = 2*D)\n",
    "    \n",
    "    vae_gumbel_truncated.t = max(0.001, vae_gumbel_truncated.t * 0.99)\n",
    "    if epoch <=(n_epochs//5*4):\n",
    "        gradients_before_burnin += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_before_burnin += vae_gumbel_truncated.subset_indices.sum(dim = 0)\n",
    "    if epoch > (n_epochs//5*4):\n",
    "        gradient_post_burn_in += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_post_burnin += vae_gumbel_truncated.subset_indices.sum(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradients_before_burnin[:D].sum())\n",
    "print(gradients_before_burnin[D:].sum())\n",
    "sns.heatmap(gradients_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradient_post_burn_in[:D].sum())\n",
    "print(gradient_post_burn_in[D:].sum())\n",
    "sns.heatmap(gradient_post_burn_in.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_before_burnin[:D].sum())\n",
    "print(subset_indices_before_burnin[D:].sum())\n",
    "sns.heatmap(subset_indices_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_post_burnin[:D].sum())\n",
    "print(subset_indices_post_burnin[(D):].sum())\n",
    "sns.heatmap(subset_indices_post_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices_post_burnin[:(D)].sum() - subset_indices_post_burnin[(D):].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE_Gumbel_NInsta test here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_truncated = VAE_Gumbel_NInsta(2*D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "gradient_post_burn_in = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_post_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                                      vae_gumbel_trunc_optimizer, \n",
    "                                                      epoch, \n",
    "                                                      batch_size, \n",
    "                                                      Dim = 2*D)\n",
    "    \n",
    "    vae_gumbel_truncated.t = max(0.001, vae_gumbel_truncated.t * 0.99)\n",
    "    if epoch <=(n_epochs//5*4):\n",
    "        gradients_before_burnin += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_before_burnin += vae_gumbel_truncated.subset_indices.sum(dim = 0)\n",
    "    if epoch > (n_epochs//5*4):\n",
    "        gradient_post_burn_in += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_post_burnin += vae_gumbel_truncated.subset_indices.sum(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradients_before_burnin[:D].sum())\n",
    "print(gradients_before_burnin[D:].sum())\n",
    "sns.heatmap(gradients_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradient_post_burn_in[:D].sum())\n",
    "print(gradient_post_burn_in[D:].sum())\n",
    "sns.heatmap(gradient_post_burn_in.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_before_burnin[:D].sum())\n",
    "print(subset_indices_before_burnin[D:].sum())\n",
    "sns.heatmap(subset_indices_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_post_burnin[:D].sum())\n",
    "print(subset_indices_post_burnin[(D):].sum())\n",
    "sns.heatmap(subset_indices_post_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices_post_burnin[:(D)].sum() - subset_indices_post_burnin[(D):].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The new model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_changed_loss_lambda = 0\n",
    "vae_gumbel_truncated = VAE_Gumbel_NInstaState(2*D, 100, 20, k = 3*z_size, t = global_t, alpha = 0.90)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                              lr=lr, \n",
    "                                              betas = (b1,b2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 42.005894\n",
      "Train Epoch: 1 [1280/8000 (16%)]\tLoss: 41.403408\n",
      "Train Epoch: 1 [2560/8000 (32%)]\tLoss: 40.911655\n",
      "Train Epoch: 1 [3840/8000 (48%)]\tLoss: 40.281277\n",
      "Loss tensor(2554.9790, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(2.2118, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 1 [5120/8000 (64%)]\tLoss: 39.795250\n",
      "Train Epoch: 1 [6400/8000 (80%)]\tLoss: 39.336693\n",
      "Train Epoch: 1 [7680/8000 (96%)]\tLoss: 38.659637\n",
      "====> Epoch: 1 Average loss: 40.2669\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 38.460083\n",
      "Train Epoch: 2 [1280/8000 (16%)]\tLoss: 38.149048\n",
      "Train Epoch: 2 [2560/8000 (32%)]\tLoss: 37.289993\n",
      "Loss tensor(2367.4109, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(14.2797, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 2 [3840/8000 (48%)]\tLoss: 36.707020\n",
      "Loss tensor(2318.8369, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(22.5613, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(2307.7639, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(23.1726, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 2 [5120/8000 (64%)]\tLoss: 36.152802\n",
      "Train Epoch: 2 [6400/8000 (80%)]\tLoss: 35.182575\n",
      "Train Epoch: 2 [7680/8000 (96%)]\tLoss: 34.626995\n",
      "====> Epoch: 2 Average loss: 36.5422\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 34.656143\n",
      "Train Epoch: 3 [1280/8000 (16%)]\tLoss: 34.363712\n",
      "Train Epoch: 3 [2560/8000 (32%)]\tLoss: 33.971676\n",
      "Train Epoch: 3 [3840/8000 (48%)]\tLoss: 34.052986\n",
      "Train Epoch: 3 [5120/8000 (64%)]\tLoss: 34.050243\n",
      "Train Epoch: 3 [6400/8000 (80%)]\tLoss: 33.250702\n",
      "Train Epoch: 3 [7680/8000 (96%)]\tLoss: 33.246017\n",
      "====> Epoch: 3 Average loss: 33.8702\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 33.419979\n",
      "Train Epoch: 4 [1280/8000 (16%)]\tLoss: 33.338661\n",
      "Loss tensor(2145.2996, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(127.7476, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 4 [2560/8000 (32%)]\tLoss: 33.525341\n",
      "Loss tensor(2124.0100, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(130.3854, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 4 [3840/8000 (48%)]\tLoss: 33.241829\n",
      "Loss tensor(2131.0505, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(130.4122, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 4 [5120/8000 (64%)]\tLoss: 32.910591\n",
      "Train Epoch: 4 [6400/8000 (80%)]\tLoss: 33.215317\n",
      "Train Epoch: 4 [7680/8000 (96%)]\tLoss: 32.785988\n",
      "====> Epoch: 4 Average loss: 33.2795\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 32.730179\n",
      "Train Epoch: 5 [1280/8000 (16%)]\tLoss: 33.085453\n",
      "Train Epoch: 5 [2560/8000 (32%)]\tLoss: 33.155254\n",
      "Train Epoch: 5 [3840/8000 (48%)]\tLoss: 32.905884\n",
      "Loss tensor(2113.4895, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(136.9076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 5 [5120/8000 (64%)]\tLoss: 32.873676\n",
      "Train Epoch: 5 [6400/8000 (80%)]\tLoss: 32.966843\n",
      "Train Epoch: 5 [7680/8000 (96%)]\tLoss: 32.711742\n",
      "====> Epoch: 5 Average loss: 33.0590\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 33.004036\n",
      "Train Epoch: 6 [1280/8000 (16%)]\tLoss: 32.789711\n",
      "Loss tensor(2122.2080, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(138.9967, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 6 [2560/8000 (32%)]\tLoss: 33.281052\n",
      "Loss tensor(2092.1577, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(138.2278, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(2127.9985, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(137.1076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 6 [3840/8000 (48%)]\tLoss: 32.893288\n",
      "Loss tensor(2096.6895, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(135.5420, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 6 [5120/8000 (64%)]\tLoss: 33.072559\n",
      "Train Epoch: 6 [6400/8000 (80%)]\tLoss: 32.672733\n",
      "Train Epoch: 6 [7680/8000 (96%)]\tLoss: 32.582691\n",
      "====> Epoch: 6 Average loss: 32.9451\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 32.958897\n",
      "Train Epoch: 7 [1280/8000 (16%)]\tLoss: 33.055870\n",
      "Train Epoch: 7 [2560/8000 (32%)]\tLoss: 32.860313\n",
      "Loss tensor(2098.1885, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(126.0206, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 7 [3840/8000 (48%)]\tLoss: 32.841122\n",
      "Loss tensor(2107.0625, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(125.0682, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 7 [5120/8000 (64%)]\tLoss: 33.035225\n",
      "Train Epoch: 7 [6400/8000 (80%)]\tLoss: 32.634056\n",
      "Train Epoch: 7 [7680/8000 (96%)]\tLoss: 32.677071\n",
      "====> Epoch: 7 Average loss: 32.8762\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 33.003922\n",
      "Train Epoch: 8 [1280/8000 (16%)]\tLoss: 32.731613\n",
      "Train Epoch: 8 [2560/8000 (32%)]\tLoss: 32.722206\n",
      "Train Epoch: 8 [3840/8000 (48%)]\tLoss: 32.916046\n",
      "Train Epoch: 8 [5120/8000 (64%)]\tLoss: 32.317875\n",
      "Train Epoch: 8 [6400/8000 (80%)]\tLoss: 32.628117\n",
      "Train Epoch: 8 [7680/8000 (96%)]\tLoss: 32.408886\n",
      "====> Epoch: 8 Average loss: 32.8020\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 32.769245\n",
      "Train Epoch: 9 [1280/8000 (16%)]\tLoss: 32.852093\n",
      "Train Epoch: 9 [2560/8000 (32%)]\tLoss: 32.580952\n",
      "Train Epoch: 9 [3840/8000 (48%)]\tLoss: 32.792904\n",
      "Train Epoch: 9 [5120/8000 (64%)]\tLoss: 32.976570\n",
      "Loss tensor(2100.2822, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(88.8996, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 9 [6400/8000 (80%)]\tLoss: 32.826302\n",
      "Loss tensor(2086.1311, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(84.0582, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 9 [7680/8000 (96%)]\tLoss: 32.736839\n",
      "====> Epoch: 9 Average loss: 32.7266\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 32.981140\n",
      "Train Epoch: 10 [1280/8000 (16%)]\tLoss: 33.045704\n",
      "Train Epoch: 10 [2560/8000 (32%)]\tLoss: 32.481270\n",
      "Train Epoch: 10 [3840/8000 (48%)]\tLoss: 32.637291\n",
      "Train Epoch: 10 [5120/8000 (64%)]\tLoss: 32.505817\n",
      "Train Epoch: 10 [6400/8000 (80%)]\tLoss: 32.932617\n",
      "Train Epoch: 10 [7680/8000 (96%)]\tLoss: 32.453926\n",
      "====> Epoch: 10 Average loss: 32.6713\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 32.686596\n",
      "Train Epoch: 11 [1280/8000 (16%)]\tLoss: 32.458591\n",
      "Train Epoch: 11 [2560/8000 (32%)]\tLoss: 32.896332\n",
      "Train Epoch: 11 [3840/8000 (48%)]\tLoss: 32.864250\n",
      "Train Epoch: 11 [5120/8000 (64%)]\tLoss: 32.789238\n",
      "Train Epoch: 11 [6400/8000 (80%)]\tLoss: 33.029800\n",
      "Train Epoch: 11 [7680/8000 (96%)]\tLoss: 32.738319\n",
      "====> Epoch: 11 Average loss: 32.6134\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 32.507698\n",
      "Train Epoch: 12 [1280/8000 (16%)]\tLoss: 32.724648\n",
      "Train Epoch: 12 [2560/8000 (32%)]\tLoss: 32.914101\n",
      "Loss tensor(2085.1667, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(72.7095, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 12 [3840/8000 (48%)]\tLoss: 32.031536\n",
      "Train Epoch: 12 [5120/8000 (64%)]\tLoss: 32.790722\n",
      "Train Epoch: 12 [6400/8000 (80%)]\tLoss: 32.207726\n",
      "Train Epoch: 12 [7680/8000 (96%)]\tLoss: 32.648026\n",
      "====> Epoch: 12 Average loss: 32.5482\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 32.139030\n",
      "Train Epoch: 13 [1280/8000 (16%)]\tLoss: 32.414551\n",
      "Train Epoch: 13 [2560/8000 (32%)]\tLoss: 32.581562\n",
      "Loss tensor(2086.5281, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(85.1052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(2076.4890, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(85.2367, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 13 [3840/8000 (48%)]\tLoss: 32.582287\n",
      "Loss tensor(2093.6050, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(88.6705, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 13 [5120/8000 (64%)]\tLoss: 32.742897\n",
      "Train Epoch: 13 [6400/8000 (80%)]\tLoss: 32.303127\n",
      "Train Epoch: 13 [7680/8000 (96%)]\tLoss: 32.502144\n",
      "====> Epoch: 13 Average loss: 32.4818\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 32.328934\n",
      "Train Epoch: 14 [1280/8000 (16%)]\tLoss: 32.488411\n",
      "Train Epoch: 14 [2560/8000 (32%)]\tLoss: 32.133926\n",
      "Train Epoch: 14 [3840/8000 (48%)]\tLoss: 32.475452\n",
      "Train Epoch: 14 [5120/8000 (64%)]\tLoss: 32.534649\n",
      "Train Epoch: 14 [6400/8000 (80%)]\tLoss: 32.611126\n",
      "Train Epoch: 14 [7680/8000 (96%)]\tLoss: 32.771339\n",
      "====> Epoch: 14 Average loss: 32.4014\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 32.400639\n",
      "Train Epoch: 15 [1280/8000 (16%)]\tLoss: 32.498199\n",
      "Train Epoch: 15 [2560/8000 (32%)]\tLoss: 32.051144\n",
      "Train Epoch: 15 [3840/8000 (48%)]\tLoss: 32.051926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [5120/8000 (64%)]\tLoss: 32.483093\n",
      "Loss tensor(2040.5205, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(193.4373, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 15 [6400/8000 (80%)]\tLoss: 31.917091\n",
      "Train Epoch: 15 [7680/8000 (96%)]\tLoss: 32.289852\n",
      "====> Epoch: 15 Average loss: 32.2850\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 32.191002\n",
      "Loss tensor(2063.0088, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(224.9011, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 16 [1280/8000 (16%)]\tLoss: 31.904879\n",
      "Train Epoch: 16 [2560/8000 (32%)]\tLoss: 31.908775\n",
      "Train Epoch: 16 [3840/8000 (48%)]\tLoss: 32.006447\n",
      "Train Epoch: 16 [5120/8000 (64%)]\tLoss: 31.993176\n",
      "Train Epoch: 16 [6400/8000 (80%)]\tLoss: 32.048378\n",
      "Loss tensor(2064.6353, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(335.9975, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 16 [7680/8000 (96%)]\tLoss: 32.294411\n",
      "====> Epoch: 16 Average loss: 32.0902\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 31.702215\n",
      "Train Epoch: 17 [1280/8000 (16%)]\tLoss: 31.897001\n",
      "Loss tensor(2083.1050, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(365.2008, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 17 [2560/8000 (32%)]\tLoss: 31.626741\n",
      "Train Epoch: 17 [3840/8000 (48%)]\tLoss: 31.957336\n",
      "Train Epoch: 17 [5120/8000 (64%)]\tLoss: 31.602013\n",
      "Train Epoch: 17 [6400/8000 (80%)]\tLoss: 31.918121\n",
      "Train Epoch: 17 [7680/8000 (96%)]\tLoss: 32.044907\n",
      "====> Epoch: 17 Average loss: 31.8581\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 31.933657\n",
      "Train Epoch: 18 [1280/8000 (16%)]\tLoss: 31.680470\n",
      "Train Epoch: 18 [2560/8000 (32%)]\tLoss: 31.533716\n",
      "Train Epoch: 18 [3840/8000 (48%)]\tLoss: 31.930984\n",
      "Loss tensor(2010.6388, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(478.4171, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 18 [5120/8000 (64%)]\tLoss: 31.559126\n",
      "Train Epoch: 18 [6400/8000 (80%)]\tLoss: 31.228451\n",
      "Train Epoch: 18 [7680/8000 (96%)]\tLoss: 31.910847\n",
      "====> Epoch: 18 Average loss: 31.6686\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 31.698555\n",
      "Train Epoch: 19 [1280/8000 (16%)]\tLoss: 31.591757\n",
      "Train Epoch: 19 [2560/8000 (32%)]\tLoss: 31.679407\n",
      "Train Epoch: 19 [3840/8000 (48%)]\tLoss: 31.324322\n",
      "Train Epoch: 19 [5120/8000 (64%)]\tLoss: 31.661755\n",
      "Loss tensor(2023.6313, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(521.4474, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 19 [6400/8000 (80%)]\tLoss: 31.634317\n",
      "Train Epoch: 19 [7680/8000 (96%)]\tLoss: 31.474897\n",
      "====> Epoch: 19 Average loss: 31.5411\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 31.626749\n",
      "Train Epoch: 20 [1280/8000 (16%)]\tLoss: 31.191984\n",
      "Train Epoch: 20 [2560/8000 (32%)]\tLoss: 31.718868\n",
      "Train Epoch: 20 [3840/8000 (48%)]\tLoss: 31.519163\n",
      "Train Epoch: 20 [5120/8000 (64%)]\tLoss: 31.418089\n",
      "Train Epoch: 20 [6400/8000 (80%)]\tLoss: 31.445049\n",
      "Train Epoch: 20 [7680/8000 (96%)]\tLoss: 31.558895\n",
      "====> Epoch: 20 Average loss: 31.4508\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 31.537262\n",
      "Loss tensor(1994.9141, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(515.0781, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(2011.6831, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(513.1566, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 21 [1280/8000 (16%)]\tLoss: 31.432549\n",
      "Train Epoch: 21 [2560/8000 (32%)]\tLoss: 31.502371\n",
      "Train Epoch: 21 [3840/8000 (48%)]\tLoss: 31.251640\n",
      "Train Epoch: 21 [5120/8000 (64%)]\tLoss: 31.219881\n",
      "Train Epoch: 21 [6400/8000 (80%)]\tLoss: 31.376507\n",
      "Train Epoch: 21 [7680/8000 (96%)]\tLoss: 31.204872\n",
      "Loss tensor(1981.1378, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(503.2787, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 21 Average loss: 31.3798\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 31.483139\n",
      "Train Epoch: 22 [1280/8000 (16%)]\tLoss: 31.777824\n",
      "Loss tensor(2012.6984, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(503.7668, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 22 [2560/8000 (32%)]\tLoss: 31.300205\n",
      "Train Epoch: 22 [3840/8000 (48%)]\tLoss: 31.464985\n",
      "Train Epoch: 22 [5120/8000 (64%)]\tLoss: 31.719744\n",
      "Train Epoch: 22 [6400/8000 (80%)]\tLoss: 31.509979\n",
      "Train Epoch: 22 [7680/8000 (96%)]\tLoss: 31.408461\n",
      "====> Epoch: 22 Average loss: 31.3167\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 31.510920\n",
      "Train Epoch: 23 [1280/8000 (16%)]\tLoss: 31.253538\n",
      "Train Epoch: 23 [2560/8000 (32%)]\tLoss: 31.619303\n",
      "Train Epoch: 23 [3840/8000 (48%)]\tLoss: 31.060490\n",
      "Train Epoch: 23 [5120/8000 (64%)]\tLoss: 31.394283\n",
      "Train Epoch: 23 [6400/8000 (80%)]\tLoss: 31.238144\n",
      "Train Epoch: 23 [7680/8000 (96%)]\tLoss: 31.029274\n",
      "====> Epoch: 23 Average loss: 31.2722\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 31.175934\n",
      "Train Epoch: 24 [1280/8000 (16%)]\tLoss: 31.315798\n",
      "Train Epoch: 24 [2560/8000 (32%)]\tLoss: 31.237051\n",
      "Train Epoch: 24 [3840/8000 (48%)]\tLoss: 31.161493\n",
      "Loss tensor(1996.2454, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(514.4486, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 24 [5120/8000 (64%)]\tLoss: 31.173229\n",
      "Train Epoch: 24 [6400/8000 (80%)]\tLoss: 31.503542\n",
      "Train Epoch: 24 [7680/8000 (96%)]\tLoss: 31.192019\n",
      "====> Epoch: 24 Average loss: 31.2184\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 31.252419\n",
      "Train Epoch: 25 [1280/8000 (16%)]\tLoss: 30.931913\n",
      "Loss tensor(2000.7070, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(532.6486, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 25 [2560/8000 (32%)]\tLoss: 31.065315\n",
      "Train Epoch: 25 [3840/8000 (48%)]\tLoss: 30.989235\n",
      "Train Epoch: 25 [5120/8000 (64%)]\tLoss: 31.630297\n",
      "Train Epoch: 25 [6400/8000 (80%)]\tLoss: 31.412930\n",
      "Loss tensor(2004.3710, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(542.0809, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 25 [7680/8000 (96%)]\tLoss: 31.479807\n",
      "====> Epoch: 25 Average loss: 31.1775\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 31.272551\n",
      "Train Epoch: 26 [1280/8000 (16%)]\tLoss: 31.463936\n",
      "Train Epoch: 26 [2560/8000 (32%)]\tLoss: 31.317127\n",
      "Train Epoch: 26 [3840/8000 (48%)]\tLoss: 31.271692\n",
      "Train Epoch: 26 [5120/8000 (64%)]\tLoss: 31.191563\n",
      "Train Epoch: 26 [6400/8000 (80%)]\tLoss: 31.278229\n",
      "Train Epoch: 26 [7680/8000 (96%)]\tLoss: 31.008429\n",
      "====> Epoch: 26 Average loss: 31.1218\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 31.134480\n",
      "Train Epoch: 27 [1280/8000 (16%)]\tLoss: 31.160973\n",
      "Loss tensor(2009.3702, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(562.2452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 27 [2560/8000 (32%)]\tLoss: 31.200010\n",
      "Train Epoch: 27 [3840/8000 (48%)]\tLoss: 31.344130\n",
      "Train Epoch: 27 [5120/8000 (64%)]\tLoss: 30.908180\n",
      "Train Epoch: 27 [6400/8000 (80%)]\tLoss: 31.272520\n",
      "Train Epoch: 27 [7680/8000 (96%)]\tLoss: 31.148590\n",
      "====> Epoch: 27 Average loss: 31.0649\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 30.871605\n",
      "Train Epoch: 28 [1280/8000 (16%)]\tLoss: 30.795479\n",
      "Train Epoch: 28 [2560/8000 (32%)]\tLoss: 30.961308\n",
      "Train Epoch: 28 [3840/8000 (48%)]\tLoss: 31.003437\n",
      "Train Epoch: 28 [5120/8000 (64%)]\tLoss: 30.861841\n",
      "Train Epoch: 28 [6400/8000 (80%)]\tLoss: 31.064379\n",
      "Train Epoch: 28 [7680/8000 (96%)]\tLoss: 30.975859\n",
      "====> Epoch: 28 Average loss: 31.0102\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 31.320587\n",
      "Train Epoch: 29 [1280/8000 (16%)]\tLoss: 30.860075\n",
      "Train Epoch: 29 [2560/8000 (32%)]\tLoss: 30.941450\n",
      "Train Epoch: 29 [3840/8000 (48%)]\tLoss: 30.951565\n",
      "Train Epoch: 29 [5120/8000 (64%)]\tLoss: 31.249235\n",
      "Loss tensor(1987.8656, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(592.3930, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 29 [6400/8000 (80%)]\tLoss: 30.804573\n",
      "Train Epoch: 29 [7680/8000 (96%)]\tLoss: 30.524836\n",
      "====> Epoch: 29 Average loss: 30.9750\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 30.773079\n",
      "Train Epoch: 30 [1280/8000 (16%)]\tLoss: 31.132347\n",
      "Train Epoch: 30 [2560/8000 (32%)]\tLoss: 30.927010\n",
      "Train Epoch: 30 [3840/8000 (48%)]\tLoss: 30.938015\n",
      "Train Epoch: 30 [5120/8000 (64%)]\tLoss: 30.676289\n",
      "Train Epoch: 30 [6400/8000 (80%)]\tLoss: 31.160402\n",
      "Train Epoch: 30 [7680/8000 (96%)]\tLoss: 30.732296\n",
      "====> Epoch: 30 Average loss: 30.9356\n",
      "Train Epoch: 31 [0/8000 (0%)]\tLoss: 30.929583\n",
      "Train Epoch: 31 [1280/8000 (16%)]\tLoss: 31.054882\n",
      "Train Epoch: 31 [2560/8000 (32%)]\tLoss: 30.969221\n",
      "Loss tensor(1954.9248, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(624.0802, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 31 [3840/8000 (48%)]\tLoss: 31.238092\n",
      "Train Epoch: 31 [5120/8000 (64%)]\tLoss: 31.210716\n",
      "Train Epoch: 31 [6400/8000 (80%)]\tLoss: 31.203722\n",
      "Loss tensor(1970.9165, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(626.3223, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 31 [7680/8000 (96%)]\tLoss: 30.469072\n",
      "====> Epoch: 31 Average loss: 30.8988\n",
      "Train Epoch: 32 [0/8000 (0%)]\tLoss: 30.483698\n",
      "Loss tensor(1991.4623, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(638.2074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 32 [1280/8000 (16%)]\tLoss: 30.757393\n",
      "Train Epoch: 32 [2560/8000 (32%)]\tLoss: 30.892847\n",
      "Train Epoch: 32 [3840/8000 (48%)]\tLoss: 30.846094\n",
      "Train Epoch: 32 [5120/8000 (64%)]\tLoss: 30.807045\n",
      "Train Epoch: 32 [6400/8000 (80%)]\tLoss: 30.911850\n",
      "Train Epoch: 32 [7680/8000 (96%)]\tLoss: 31.054108\n",
      "====> Epoch: 32 Average loss: 30.8586\n",
      "Train Epoch: 33 [0/8000 (0%)]\tLoss: 30.795841\n",
      "Train Epoch: 33 [1280/8000 (16%)]\tLoss: 30.765566\n",
      "Train Epoch: 33 [2560/8000 (32%)]\tLoss: 30.800104\n",
      "Train Epoch: 33 [3840/8000 (48%)]\tLoss: 30.655039\n",
      "Train Epoch: 33 [5120/8000 (64%)]\tLoss: 30.919777\n",
      "Train Epoch: 33 [6400/8000 (80%)]\tLoss: 30.698303\n",
      "Train Epoch: 33 [7680/8000 (96%)]\tLoss: 31.128201\n",
      "====> Epoch: 33 Average loss: 30.8135\n",
      "Train Epoch: 34 [0/8000 (0%)]\tLoss: 30.940641\n",
      "Train Epoch: 34 [1280/8000 (16%)]\tLoss: 30.791040\n",
      "Loss tensor(1968.7646, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(689.4481, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 34 [2560/8000 (32%)]\tLoss: 30.761948\n",
      "Train Epoch: 34 [3840/8000 (48%)]\tLoss: 30.532475\n",
      "Train Epoch: 34 [5120/8000 (64%)]\tLoss: 30.857788\n",
      "Train Epoch: 34 [6400/8000 (80%)]\tLoss: 30.748533\n",
      "Train Epoch: 34 [7680/8000 (96%)]\tLoss: 30.597191\n",
      "====> Epoch: 34 Average loss: 30.7942\n",
      "Train Epoch: 35 [0/8000 (0%)]\tLoss: 30.682758\n",
      "Train Epoch: 35 [1280/8000 (16%)]\tLoss: 30.828003\n",
      "Train Epoch: 35 [2560/8000 (32%)]\tLoss: 30.752707\n",
      "Train Epoch: 35 [3840/8000 (48%)]\tLoss: 30.954170\n",
      "Train Epoch: 35 [5120/8000 (64%)]\tLoss: 30.638792\n",
      "Train Epoch: 35 [6400/8000 (80%)]\tLoss: 30.795155\n",
      "Train Epoch: 35 [7680/8000 (96%)]\tLoss: 30.612223\n",
      "Loss tensor(1952.1025, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(704.7590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 35 Average loss: 30.7659\n",
      "Train Epoch: 36 [0/8000 (0%)]\tLoss: 30.692961\n",
      "Loss tensor(1971.6852, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(704.6949, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 36 [1280/8000 (16%)]\tLoss: 30.583899\n",
      "Train Epoch: 36 [2560/8000 (32%)]\tLoss: 30.643423\n",
      "Loss tensor(1973.3789, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(713.3090, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 36 [3840/8000 (48%)]\tLoss: 30.948666\n",
      "Train Epoch: 36 [5120/8000 (64%)]\tLoss: 30.472479\n",
      "Train Epoch: 36 [6400/8000 (80%)]\tLoss: 30.666800\n",
      "Loss tensor(1959.1031, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(707.2333, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 36 [7680/8000 (96%)]\tLoss: 30.614573\n",
      "====> Epoch: 36 Average loss: 30.7347\n",
      "Train Epoch: 37 [0/8000 (0%)]\tLoss: 30.361565\n",
      "Loss tensor(1990.2317, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(705.3228, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 37 [1280/8000 (16%)]\tLoss: 30.912466\n",
      "Loss tensor(1985.7753, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(707.4492, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 37 [2560/8000 (32%)]\tLoss: 30.520391\n",
      "Train Epoch: 37 [3840/8000 (48%)]\tLoss: 30.291412\n",
      "Train Epoch: 37 [5120/8000 (64%)]\tLoss: 30.839325\n",
      "Loss tensor(1989.6625, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(705.3904, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 37 [6400/8000 (80%)]\tLoss: 30.925079\n",
      "Train Epoch: 37 [7680/8000 (96%)]\tLoss: 30.764431\n",
      "====> Epoch: 37 Average loss: 30.7099\n",
      "Train Epoch: 38 [0/8000 (0%)]\tLoss: 30.554714\n",
      "Train Epoch: 38 [1280/8000 (16%)]\tLoss: 30.725992\n",
      "Train Epoch: 38 [2560/8000 (32%)]\tLoss: 30.692410\n",
      "Train Epoch: 38 [3840/8000 (48%)]\tLoss: 30.512558\n",
      "Train Epoch: 38 [5120/8000 (64%)]\tLoss: 30.590446\n",
      "Train Epoch: 38 [6400/8000 (80%)]\tLoss: 30.294960\n",
      "Train Epoch: 38 [7680/8000 (96%)]\tLoss: 30.805405\n",
      "====> Epoch: 38 Average loss: 30.6835\n",
      "Train Epoch: 39 [0/8000 (0%)]\tLoss: 30.944103\n",
      "Loss tensor(1981.4575, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(715.0696, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 39 [1280/8000 (16%)]\tLoss: 30.771683\n",
      "Loss tensor(1963.7383, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(707.9915, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 39 [2560/8000 (32%)]\tLoss: 30.633003\n",
      "Train Epoch: 39 [3840/8000 (48%)]\tLoss: 30.595224\n",
      "Train Epoch: 39 [5120/8000 (64%)]\tLoss: 30.597065\n",
      "Loss tensor(1942.4738, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(704.0641, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 39 [6400/8000 (80%)]\tLoss: 30.375181\n",
      "Train Epoch: 39 [7680/8000 (96%)]\tLoss: 30.688143\n",
      "====> Epoch: 39 Average loss: 30.6652\n",
      "Train Epoch: 40 [0/8000 (0%)]\tLoss: 30.493860\n",
      "Train Epoch: 40 [1280/8000 (16%)]\tLoss: 30.660677\n",
      "Train Epoch: 40 [2560/8000 (32%)]\tLoss: 30.555332\n",
      "Train Epoch: 40 [3840/8000 (48%)]\tLoss: 30.472443\n",
      "Train Epoch: 40 [5120/8000 (64%)]\tLoss: 30.359926\n",
      "Train Epoch: 40 [6400/8000 (80%)]\tLoss: 30.629051\n",
      "Train Epoch: 40 [7680/8000 (96%)]\tLoss: 30.680447\n",
      "====> Epoch: 40 Average loss: 30.6398\n",
      "Train Epoch: 41 [0/8000 (0%)]\tLoss: 30.575443\n",
      "Loss tensor(1948.7864, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(718.2300, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 41 [1280/8000 (16%)]\tLoss: 30.699036\n",
      "Train Epoch: 41 [2560/8000 (32%)]\tLoss: 30.728979\n",
      "Train Epoch: 41 [3840/8000 (48%)]\tLoss: 30.755711\n",
      "Train Epoch: 41 [5120/8000 (64%)]\tLoss: 30.593792\n",
      "Train Epoch: 41 [6400/8000 (80%)]\tLoss: 30.969824\n",
      "Train Epoch: 41 [7680/8000 (96%)]\tLoss: 30.573883\n",
      "====> Epoch: 41 Average loss: 30.6191\n",
      "Train Epoch: 42 [0/8000 (0%)]\tLoss: 30.921558\n",
      "Train Epoch: 42 [1280/8000 (16%)]\tLoss: 30.667095\n",
      "Train Epoch: 42 [2560/8000 (32%)]\tLoss: 30.602276\n",
      "Train Epoch: 42 [3840/8000 (48%)]\tLoss: 30.444994\n",
      "Loss tensor(1930.5642, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(716.1901, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 42 [5120/8000 (64%)]\tLoss: 30.557312\n",
      "Train Epoch: 42 [6400/8000 (80%)]\tLoss: 30.415178\n",
      "Train Epoch: 42 [7680/8000 (96%)]\tLoss: 30.688198\n",
      "====> Epoch: 42 Average loss: 30.6011\n",
      "Train Epoch: 43 [0/8000 (0%)]\tLoss: 30.756287\n",
      "Train Epoch: 43 [1280/8000 (16%)]\tLoss: 30.707644\n",
      "Train Epoch: 43 [2560/8000 (32%)]\tLoss: 30.673456\n",
      "Train Epoch: 43 [3840/8000 (48%)]\tLoss: 30.571686\n",
      "Train Epoch: 43 [5120/8000 (64%)]\tLoss: 30.594240\n",
      "Train Epoch: 43 [6400/8000 (80%)]\tLoss: 30.549486\n",
      "Train Epoch: 43 [7680/8000 (96%)]\tLoss: 30.474682\n",
      "====> Epoch: 43 Average loss: 30.5803\n",
      "Train Epoch: 44 [0/8000 (0%)]\tLoss: 30.880817\n",
      "Train Epoch: 44 [1280/8000 (16%)]\tLoss: 30.404940\n",
      "Train Epoch: 44 [2560/8000 (32%)]\tLoss: 30.666935\n",
      "Train Epoch: 44 [3840/8000 (48%)]\tLoss: 30.443411\n",
      "Train Epoch: 44 [5120/8000 (64%)]\tLoss: 30.650238\n",
      "Train Epoch: 44 [6400/8000 (80%)]\tLoss: 30.446236\n",
      "Loss tensor(1946.0349, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(730.1487, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 44 [7680/8000 (96%)]\tLoss: 30.730797\n",
      "====> Epoch: 44 Average loss: 30.5719\n",
      "Train Epoch: 45 [0/8000 (0%)]\tLoss: 30.610001\n",
      "Train Epoch: 45 [1280/8000 (16%)]\tLoss: 30.707602\n",
      "Train Epoch: 45 [2560/8000 (32%)]\tLoss: 30.280653\n",
      "Train Epoch: 45 [3840/8000 (48%)]\tLoss: 30.687696\n",
      "Train Epoch: 45 [5120/8000 (64%)]\tLoss: 30.747562\n",
      "Train Epoch: 45 [6400/8000 (80%)]\tLoss: 30.759026\n",
      "Train Epoch: 45 [7680/8000 (96%)]\tLoss: 30.549847\n",
      "====> Epoch: 45 Average loss: 30.5482\n",
      "Train Epoch: 46 [0/8000 (0%)]\tLoss: 30.447496\n",
      "Train Epoch: 46 [1280/8000 (16%)]\tLoss: 30.893927\n",
      "Train Epoch: 46 [2560/8000 (32%)]\tLoss: 30.710447\n",
      "Loss tensor(1956.4240, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(726.3988, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 46 [3840/8000 (48%)]\tLoss: 30.543026\n",
      "Loss tensor(1943.3774, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(727.9569, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 46 [5120/8000 (64%)]\tLoss: 30.418623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [6400/8000 (80%)]\tLoss: 30.483778\n",
      "Train Epoch: 46 [7680/8000 (96%)]\tLoss: 30.670544\n",
      "====> Epoch: 46 Average loss: 30.5347\n",
      "Train Epoch: 47 [0/8000 (0%)]\tLoss: 30.314503\n",
      "Train Epoch: 47 [1280/8000 (16%)]\tLoss: 30.375837\n",
      "Train Epoch: 47 [2560/8000 (32%)]\tLoss: 30.545235\n",
      "Loss tensor(1959.0258, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(716.4166, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 47 [3840/8000 (48%)]\tLoss: 31.013294\n",
      "Train Epoch: 47 [5120/8000 (64%)]\tLoss: 30.814672\n",
      "Train Epoch: 47 [6400/8000 (80%)]\tLoss: 30.442041\n",
      "Train Epoch: 47 [7680/8000 (96%)]\tLoss: 30.833576\n",
      "====> Epoch: 47 Average loss: 30.5167\n",
      "Train Epoch: 48 [0/8000 (0%)]\tLoss: 30.561993\n",
      "Train Epoch: 48 [1280/8000 (16%)]\tLoss: 30.633310\n",
      "Train Epoch: 48 [2560/8000 (32%)]\tLoss: 30.844923\n",
      "Train Epoch: 48 [3840/8000 (48%)]\tLoss: 30.964382\n",
      "Train Epoch: 48 [5120/8000 (64%)]\tLoss: 30.638268\n",
      "Loss tensor(1941.8373, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(720.7924, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 48 [6400/8000 (80%)]\tLoss: 30.371969\n",
      "Loss tensor(1951.2070, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(717.2182, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 48 [7680/8000 (96%)]\tLoss: 30.539459\n",
      "====> Epoch: 48 Average loss: 30.5081\n",
      "Train Epoch: 49 [0/8000 (0%)]\tLoss: 30.148302\n",
      "Train Epoch: 49 [1280/8000 (16%)]\tLoss: 30.639336\n",
      "Train Epoch: 49 [2560/8000 (32%)]\tLoss: 30.516855\n",
      "Train Epoch: 49 [3840/8000 (48%)]\tLoss: 30.719915\n",
      "Train Epoch: 49 [5120/8000 (64%)]\tLoss: 30.288031\n",
      "Train Epoch: 49 [6400/8000 (80%)]\tLoss: 30.614571\n",
      "Loss tensor(1939.3533, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(729.9254, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 49 [7680/8000 (96%)]\tLoss: 30.939674\n",
      "====> Epoch: 49 Average loss: 30.4940\n",
      "Train Epoch: 50 [0/8000 (0%)]\tLoss: 30.722021\n",
      "Loss tensor(1933.5664, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(737.3251, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 50 [1280/8000 (16%)]\tLoss: 30.284065\n",
      "Train Epoch: 50 [2560/8000 (32%)]\tLoss: 30.593159\n",
      "Train Epoch: 50 [3840/8000 (48%)]\tLoss: 30.476742\n",
      "Loss tensor(1969.3289, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(721.1892, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 50 [5120/8000 (64%)]\tLoss: 30.315525\n",
      "Loss tensor(1965.3922, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(720.4366, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 50 [6400/8000 (80%)]\tLoss: 30.568752\n",
      "Loss tensor(1946.0691, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(731.1019, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 50 [7680/8000 (96%)]\tLoss: 30.446899\n",
      "====> Epoch: 50 Average loss: 30.4751\n",
      "Train Epoch: 51 [0/8000 (0%)]\tLoss: 30.380255\n",
      "Loss tensor(1950.7590, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(745.6764, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 51 [1280/8000 (16%)]\tLoss: 30.935577\n",
      "Train Epoch: 51 [2560/8000 (32%)]\tLoss: 30.526987\n",
      "Train Epoch: 51 [3840/8000 (48%)]\tLoss: 30.148319\n",
      "Train Epoch: 51 [5120/8000 (64%)]\tLoss: 30.358183\n",
      "Loss tensor(1949.3420, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(727.1046, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 51 [6400/8000 (80%)]\tLoss: 30.300404\n",
      "Train Epoch: 51 [7680/8000 (96%)]\tLoss: 30.467735\n",
      "====> Epoch: 51 Average loss: 30.4576\n",
      "Train Epoch: 52 [0/8000 (0%)]\tLoss: 30.157274\n",
      "Train Epoch: 52 [1280/8000 (16%)]\tLoss: 30.103830\n",
      "Train Epoch: 52 [2560/8000 (32%)]\tLoss: 30.291615\n",
      "Train Epoch: 52 [3840/8000 (48%)]\tLoss: 30.382648\n",
      "Train Epoch: 52 [5120/8000 (64%)]\tLoss: 30.491587\n",
      "Train Epoch: 52 [6400/8000 (80%)]\tLoss: 30.380589\n",
      "Train Epoch: 52 [7680/8000 (96%)]\tLoss: 29.993814\n",
      "====> Epoch: 52 Average loss: 30.4427\n",
      "Train Epoch: 53 [0/8000 (0%)]\tLoss: 30.207214\n",
      "Train Epoch: 53 [1280/8000 (16%)]\tLoss: 30.460911\n",
      "Train Epoch: 53 [2560/8000 (32%)]\tLoss: 30.323324\n",
      "Train Epoch: 53 [3840/8000 (48%)]\tLoss: 30.362627\n",
      "Train Epoch: 53 [5120/8000 (64%)]\tLoss: 30.584192\n",
      "Train Epoch: 53 [6400/8000 (80%)]\tLoss: 30.357033\n",
      "Train Epoch: 53 [7680/8000 (96%)]\tLoss: 30.483383\n",
      "====> Epoch: 53 Average loss: 30.4280\n",
      "Train Epoch: 54 [0/8000 (0%)]\tLoss: 30.499556\n",
      "Train Epoch: 54 [1280/8000 (16%)]\tLoss: 30.289640\n",
      "Train Epoch: 54 [2560/8000 (32%)]\tLoss: 30.182989\n",
      "Train Epoch: 54 [3840/8000 (48%)]\tLoss: 30.119549\n",
      "Train Epoch: 54 [5120/8000 (64%)]\tLoss: 30.556524\n",
      "Train Epoch: 54 [6400/8000 (80%)]\tLoss: 30.309599\n",
      "Train Epoch: 54 [7680/8000 (96%)]\tLoss: 30.453772\n",
      "====> Epoch: 54 Average loss: 30.4274\n",
      "Train Epoch: 55 [0/8000 (0%)]\tLoss: 30.384199\n",
      "Loss tensor(1953.9263, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(722.9875, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 55 [1280/8000 (16%)]\tLoss: 30.843140\n",
      "Train Epoch: 55 [2560/8000 (32%)]\tLoss: 30.506702\n",
      "Train Epoch: 55 [3840/8000 (48%)]\tLoss: 30.386406\n",
      "Train Epoch: 55 [5120/8000 (64%)]\tLoss: 30.204407\n",
      "Train Epoch: 55 [6400/8000 (80%)]\tLoss: 30.258844\n",
      "Train Epoch: 55 [7680/8000 (96%)]\tLoss: 30.527958\n",
      "====> Epoch: 55 Average loss: 30.4094\n",
      "Train Epoch: 56 [0/8000 (0%)]\tLoss: 30.446604\n",
      "Loss tensor(1947.8317, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(711.0967, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 56 [1280/8000 (16%)]\tLoss: 30.395334\n",
      "Loss tensor(1927.6299, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(715.3351, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 56 [2560/8000 (32%)]\tLoss: 30.318195\n",
      "Train Epoch: 56 [3840/8000 (48%)]\tLoss: 30.404272\n",
      "Train Epoch: 56 [5120/8000 (64%)]\tLoss: 30.604076\n",
      "Train Epoch: 56 [6400/8000 (80%)]\tLoss: 30.582609\n",
      "Train Epoch: 56 [7680/8000 (96%)]\tLoss: 30.158430\n",
      "====> Epoch: 56 Average loss: 30.4037\n",
      "Train Epoch: 57 [0/8000 (0%)]\tLoss: 30.337671\n",
      "Train Epoch: 57 [1280/8000 (16%)]\tLoss: 30.464102\n",
      "Train Epoch: 57 [2560/8000 (32%)]\tLoss: 30.523075\n",
      "Train Epoch: 57 [3840/8000 (48%)]\tLoss: 30.573141\n",
      "Train Epoch: 57 [5120/8000 (64%)]\tLoss: 30.096992\n",
      "Loss tensor(1962.8070, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(711.8287, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1937.0079, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(702.0718, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 57 [6400/8000 (80%)]\tLoss: 30.471111\n",
      "Train Epoch: 57 [7680/8000 (96%)]\tLoss: 30.360981\n",
      "====> Epoch: 57 Average loss: 30.4025\n",
      "Train Epoch: 58 [0/8000 (0%)]\tLoss: 30.531862\n",
      "Train Epoch: 58 [1280/8000 (16%)]\tLoss: 30.743732\n",
      "Loss tensor(1944.1932, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(701.1951, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 58 [2560/8000 (32%)]\tLoss: 30.596922\n",
      "Train Epoch: 58 [3840/8000 (48%)]\tLoss: 30.231915\n",
      "Train Epoch: 58 [5120/8000 (64%)]\tLoss: 30.193024\n",
      "Train Epoch: 58 [6400/8000 (80%)]\tLoss: 30.162546\n",
      "Loss tensor(1949.4652, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(704.6053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 58 [7680/8000 (96%)]\tLoss: 30.460394\n",
      "====> Epoch: 58 Average loss: 30.4011\n",
      "Train Epoch: 59 [0/8000 (0%)]\tLoss: 30.373344\n",
      "Train Epoch: 59 [1280/8000 (16%)]\tLoss: 30.223234\n",
      "Train Epoch: 59 [2560/8000 (32%)]\tLoss: 30.428850\n",
      "Train Epoch: 59 [3840/8000 (48%)]\tLoss: 30.409378\n",
      "Train Epoch: 59 [5120/8000 (64%)]\tLoss: 30.533360\n",
      "Train Epoch: 59 [6400/8000 (80%)]\tLoss: 30.156319\n",
      "Train Epoch: 59 [7680/8000 (96%)]\tLoss: 30.607132\n",
      "====> Epoch: 59 Average loss: 30.3842\n",
      "Train Epoch: 60 [0/8000 (0%)]\tLoss: 30.218983\n",
      "Loss tensor(1937.7195, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(711.0897, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 60 [1280/8000 (16%)]\tLoss: 30.772636\n",
      "Train Epoch: 60 [2560/8000 (32%)]\tLoss: 30.225826\n",
      "Train Epoch: 60 [3840/8000 (48%)]\tLoss: 30.243040\n",
      "Train Epoch: 60 [5120/8000 (64%)]\tLoss: 30.544830\n",
      "Train Epoch: 60 [6400/8000 (80%)]\tLoss: 30.456059\n",
      "Train Epoch: 60 [7680/8000 (96%)]\tLoss: 30.428644\n",
      "====> Epoch: 60 Average loss: 30.3762\n",
      "Train Epoch: 61 [0/8000 (0%)]\tLoss: 30.104189\n",
      "Train Epoch: 61 [1280/8000 (16%)]\tLoss: 30.355911\n",
      "Train Epoch: 61 [2560/8000 (32%)]\tLoss: 30.374113\n",
      "Train Epoch: 61 [3840/8000 (48%)]\tLoss: 30.225695\n",
      "Train Epoch: 61 [5120/8000 (64%)]\tLoss: 30.197336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 61 [6400/8000 (80%)]\tLoss: 30.573870\n",
      "Loss tensor(1945.0001, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(701.8715, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 61 [7680/8000 (96%)]\tLoss: 30.218060\n",
      "====> Epoch: 61 Average loss: 30.3713\n",
      "Train Epoch: 62 [0/8000 (0%)]\tLoss: 30.270601\n",
      "Loss tensor(1946.6039, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(701.8499, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1939.7749, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(698.5843, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 62 [1280/8000 (16%)]\tLoss: 30.434690\n",
      "Train Epoch: 62 [2560/8000 (32%)]\tLoss: 30.542582\n",
      "Train Epoch: 62 [3840/8000 (48%)]\tLoss: 30.363226\n",
      "Train Epoch: 62 [5120/8000 (64%)]\tLoss: 30.298567\n",
      "Train Epoch: 62 [6400/8000 (80%)]\tLoss: 30.569963\n",
      "Train Epoch: 62 [7680/8000 (96%)]\tLoss: 30.497553\n",
      "====> Epoch: 62 Average loss: 30.3700\n",
      "Train Epoch: 63 [0/8000 (0%)]\tLoss: 30.201057\n",
      "Train Epoch: 63 [1280/8000 (16%)]\tLoss: 30.452023\n",
      "Train Epoch: 63 [2560/8000 (32%)]\tLoss: 30.453125\n",
      "Train Epoch: 63 [3840/8000 (48%)]\tLoss: 30.373848\n",
      "Train Epoch: 63 [5120/8000 (64%)]\tLoss: 30.128389\n",
      "Train Epoch: 63 [6400/8000 (80%)]\tLoss: 30.224003\n",
      "Train Epoch: 63 [7680/8000 (96%)]\tLoss: 30.388042\n",
      "====> Epoch: 63 Average loss: 30.3573\n",
      "Train Epoch: 64 [0/8000 (0%)]\tLoss: 30.170668\n",
      "Train Epoch: 64 [1280/8000 (16%)]\tLoss: 30.486931\n",
      "Loss tensor(1949.8657, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(698.6041, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 64 [2560/8000 (32%)]\tLoss: 30.222301\n",
      "Train Epoch: 64 [3840/8000 (48%)]\tLoss: 30.457394\n",
      "Train Epoch: 64 [5120/8000 (64%)]\tLoss: 30.066458\n",
      "Loss tensor(1955.0585, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(701.5117, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 64 [6400/8000 (80%)]\tLoss: 30.328625\n",
      "Train Epoch: 64 [7680/8000 (96%)]\tLoss: 30.546490\n",
      "====> Epoch: 64 Average loss: 30.3494\n",
      "Train Epoch: 65 [0/8000 (0%)]\tLoss: 30.239819\n",
      "Train Epoch: 65 [1280/8000 (16%)]\tLoss: 30.224499\n",
      "Loss tensor(1952.1846, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(689.6418, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 65 [2560/8000 (32%)]\tLoss: 30.350817\n",
      "Train Epoch: 65 [3840/8000 (48%)]\tLoss: 30.265854\n",
      "Train Epoch: 65 [5120/8000 (64%)]\tLoss: 30.382652\n",
      "Train Epoch: 65 [6400/8000 (80%)]\tLoss: 30.433353\n",
      "Loss tensor(1923.2493, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(686.9989, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 65 [7680/8000 (96%)]\tLoss: 30.362032\n",
      "====> Epoch: 65 Average loss: 30.3417\n",
      "Train Epoch: 66 [0/8000 (0%)]\tLoss: 30.603632\n",
      "Train Epoch: 66 [1280/8000 (16%)]\tLoss: 30.334251\n",
      "Train Epoch: 66 [2560/8000 (32%)]\tLoss: 30.456604\n",
      "Train Epoch: 66 [3840/8000 (48%)]\tLoss: 30.265970\n",
      "Train Epoch: 66 [5120/8000 (64%)]\tLoss: 30.588474\n",
      "Train Epoch: 66 [6400/8000 (80%)]\tLoss: 30.204296\n",
      "Train Epoch: 66 [7680/8000 (96%)]\tLoss: 30.344551\n",
      "====> Epoch: 66 Average loss: 30.3382\n",
      "Train Epoch: 67 [0/8000 (0%)]\tLoss: 30.125252\n",
      "Train Epoch: 67 [1280/8000 (16%)]\tLoss: 30.140764\n",
      "Train Epoch: 67 [2560/8000 (32%)]\tLoss: 30.137865\n",
      "Train Epoch: 67 [3840/8000 (48%)]\tLoss: 30.384501\n",
      "Train Epoch: 67 [5120/8000 (64%)]\tLoss: 30.301641\n",
      "Loss tensor(1942.5530, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(688.5568, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 67 [6400/8000 (80%)]\tLoss: 30.099728\n",
      "Loss tensor(1940.3680, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(691.8078, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 67 [7680/8000 (96%)]\tLoss: 29.963243\n",
      "====> Epoch: 67 Average loss: 30.3344\n",
      "Train Epoch: 68 [0/8000 (0%)]\tLoss: 30.512001\n",
      "Train Epoch: 68 [1280/8000 (16%)]\tLoss: 30.085375\n",
      "Train Epoch: 68 [2560/8000 (32%)]\tLoss: 30.339596\n",
      "Train Epoch: 68 [3840/8000 (48%)]\tLoss: 30.352671\n",
      "Train Epoch: 68 [5120/8000 (64%)]\tLoss: 30.219814\n",
      "Train Epoch: 68 [6400/8000 (80%)]\tLoss: 30.633820\n",
      "Train Epoch: 68 [7680/8000 (96%)]\tLoss: 30.220882\n",
      "====> Epoch: 68 Average loss: 30.3450\n",
      "Train Epoch: 69 [0/8000 (0%)]\tLoss: 30.404018\n",
      "Train Epoch: 69 [1280/8000 (16%)]\tLoss: 30.038553\n",
      "Loss tensor(1941.8439, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(681.3929, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1943.1870, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(683.5731, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 69 [2560/8000 (32%)]\tLoss: 30.727505\n",
      "Loss tensor(1941.0201, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(686.7603, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 69 [3840/8000 (48%)]\tLoss: 30.310495\n",
      "Train Epoch: 69 [5120/8000 (64%)]\tLoss: 30.712074\n",
      "Train Epoch: 69 [6400/8000 (80%)]\tLoss: 30.237869\n",
      "Train Epoch: 69 [7680/8000 (96%)]\tLoss: 30.561800\n",
      "====> Epoch: 69 Average loss: 30.3340\n",
      "Train Epoch: 70 [0/8000 (0%)]\tLoss: 30.328295\n",
      "Train Epoch: 70 [1280/8000 (16%)]\tLoss: 30.356716\n",
      "Train Epoch: 70 [2560/8000 (32%)]\tLoss: 30.471294\n",
      "Train Epoch: 70 [3840/8000 (48%)]\tLoss: 30.214226\n",
      "Loss tensor(1926.0015, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(679.6539, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 70 [5120/8000 (64%)]\tLoss: 30.177923\n",
      "Train Epoch: 70 [6400/8000 (80%)]\tLoss: 30.319141\n",
      "Train Epoch: 70 [7680/8000 (96%)]\tLoss: 30.048349\n",
      "Loss tensor(1929.4972, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(662.3364, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 70 Average loss: 30.3257\n",
      "Train Epoch: 71 [0/8000 (0%)]\tLoss: 30.436230\n",
      "Train Epoch: 71 [1280/8000 (16%)]\tLoss: 30.374310\n",
      "Train Epoch: 71 [2560/8000 (32%)]\tLoss: 30.518394\n",
      "Train Epoch: 71 [3840/8000 (48%)]\tLoss: 30.097570\n",
      "Train Epoch: 71 [5120/8000 (64%)]\tLoss: 30.065081\n",
      "Train Epoch: 71 [6400/8000 (80%)]\tLoss: 30.485033\n",
      "Train Epoch: 71 [7680/8000 (96%)]\tLoss: 30.291426\n",
      "====> Epoch: 71 Average loss: 30.3247\n",
      "Train Epoch: 72 [0/8000 (0%)]\tLoss: 30.293299\n",
      "Train Epoch: 72 [1280/8000 (16%)]\tLoss: 30.426865\n",
      "Train Epoch: 72 [2560/8000 (32%)]\tLoss: 30.445864\n",
      "Train Epoch: 72 [3840/8000 (48%)]\tLoss: 30.355844\n",
      "Loss tensor(1931.7532, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(669.2329, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1944.8373, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(670.8743, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 72 [5120/8000 (64%)]\tLoss: 30.277786\n",
      "Train Epoch: 72 [6400/8000 (80%)]\tLoss: 30.033730\n",
      "Train Epoch: 72 [7680/8000 (96%)]\tLoss: 30.417669\n",
      "====> Epoch: 72 Average loss: 30.3188\n",
      "Train Epoch: 73 [0/8000 (0%)]\tLoss: 30.529202\n",
      "Train Epoch: 73 [1280/8000 (16%)]\tLoss: 30.092182\n",
      "Train Epoch: 73 [2560/8000 (32%)]\tLoss: 30.273905\n",
      "Train Epoch: 73 [3840/8000 (48%)]\tLoss: 30.411594\n",
      "Train Epoch: 73 [5120/8000 (64%)]\tLoss: 30.450642\n",
      "Loss tensor(1925.7256, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(665.9047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 73 [6400/8000 (80%)]\tLoss: 29.813442\n",
      "Train Epoch: 73 [7680/8000 (96%)]\tLoss: 30.299873\n",
      "====> Epoch: 73 Average loss: 30.3197\n",
      "Train Epoch: 74 [0/8000 (0%)]\tLoss: 30.269016\n",
      "Train Epoch: 74 [1280/8000 (16%)]\tLoss: 30.244659\n",
      "Train Epoch: 74 [2560/8000 (32%)]\tLoss: 30.372625\n",
      "Train Epoch: 74 [3840/8000 (48%)]\tLoss: 30.003534\n",
      "Train Epoch: 74 [5120/8000 (64%)]\tLoss: 30.356186\n",
      "Train Epoch: 74 [6400/8000 (80%)]\tLoss: 30.316729\n",
      "Loss tensor(1936.3496, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(663.2330, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 74 [7680/8000 (96%)]\tLoss: 30.098669\n",
      "====> Epoch: 74 Average loss: 30.3195\n",
      "Train Epoch: 75 [0/8000 (0%)]\tLoss: 30.736900\n",
      "Loss tensor(1948.6952, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(663.3180, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 75 [1280/8000 (16%)]\tLoss: 30.080116\n",
      "Train Epoch: 75 [2560/8000 (32%)]\tLoss: 30.084248\n",
      "Train Epoch: 75 [3840/8000 (48%)]\tLoss: 30.499178\n",
      "Train Epoch: 75 [5120/8000 (64%)]\tLoss: 30.009018\n",
      "Train Epoch: 75 [6400/8000 (80%)]\tLoss: 30.080748\n",
      "Train Epoch: 75 [7680/8000 (96%)]\tLoss: 30.299763\n",
      "====> Epoch: 75 Average loss: 30.3290\n",
      "Train Epoch: 76 [0/8000 (0%)]\tLoss: 30.346758\n",
      "Train Epoch: 76 [1280/8000 (16%)]\tLoss: 29.967918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1937.5157, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(655.7897, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 76 [2560/8000 (32%)]\tLoss: 30.371096\n",
      "Train Epoch: 76 [3840/8000 (48%)]\tLoss: 30.476610\n",
      "Train Epoch: 76 [5120/8000 (64%)]\tLoss: 30.273237\n",
      "Train Epoch: 76 [6400/8000 (80%)]\tLoss: 30.198557\n",
      "Train Epoch: 76 [7680/8000 (96%)]\tLoss: 30.116903\n",
      "====> Epoch: 76 Average loss: 30.3066\n",
      "Train Epoch: 77 [0/8000 (0%)]\tLoss: 30.373886\n",
      "Train Epoch: 77 [1280/8000 (16%)]\tLoss: 29.924881\n",
      "Train Epoch: 77 [2560/8000 (32%)]\tLoss: 30.378931\n",
      "Train Epoch: 77 [3840/8000 (48%)]\tLoss: 30.057209\n",
      "Train Epoch: 77 [5120/8000 (64%)]\tLoss: 30.419506\n",
      "Train Epoch: 77 [6400/8000 (80%)]\tLoss: 30.050024\n",
      "Loss tensor(1940.1313, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(663.4603, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 77 [7680/8000 (96%)]\tLoss: 30.133621\n",
      "====> Epoch: 77 Average loss: 30.3036\n",
      "Train Epoch: 78 [0/8000 (0%)]\tLoss: 30.088583\n",
      "Train Epoch: 78 [1280/8000 (16%)]\tLoss: 30.186020\n",
      "Train Epoch: 78 [2560/8000 (32%)]\tLoss: 30.246979\n",
      "Train Epoch: 78 [3840/8000 (48%)]\tLoss: 30.279692\n",
      "Train Epoch: 78 [5120/8000 (64%)]\tLoss: 30.233952\n",
      "Train Epoch: 78 [6400/8000 (80%)]\tLoss: 30.529505\n",
      "Loss tensor(1945.6309, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(656.7827, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 78 [7680/8000 (96%)]\tLoss: 30.430775\n",
      "====> Epoch: 78 Average loss: 30.3048\n",
      "Train Epoch: 79 [0/8000 (0%)]\tLoss: 29.849228\n",
      "Loss tensor(1947.0643, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(656.8505, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 79 [1280/8000 (16%)]\tLoss: 30.108232\n",
      "Train Epoch: 79 [2560/8000 (32%)]\tLoss: 30.121418\n",
      "Train Epoch: 79 [3840/8000 (48%)]\tLoss: 30.249279\n",
      "Train Epoch: 79 [5120/8000 (64%)]\tLoss: 30.208286\n",
      "Train Epoch: 79 [6400/8000 (80%)]\tLoss: 30.479534\n",
      "Train Epoch: 79 [7680/8000 (96%)]\tLoss: 29.897852\n",
      "====> Epoch: 79 Average loss: 30.3041\n",
      "Train Epoch: 80 [0/8000 (0%)]\tLoss: 30.459621\n",
      "Train Epoch: 80 [1280/8000 (16%)]\tLoss: 30.368925\n",
      "Train Epoch: 80 [2560/8000 (32%)]\tLoss: 30.558741\n",
      "Loss tensor(1923.2518, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(647.9681, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 80 [3840/8000 (48%)]\tLoss: 30.430000\n",
      "Train Epoch: 80 [5120/8000 (64%)]\tLoss: 30.215714\n",
      "Train Epoch: 80 [6400/8000 (80%)]\tLoss: 30.141258\n",
      "Train Epoch: 80 [7680/8000 (96%)]\tLoss: 30.156303\n",
      "====> Epoch: 80 Average loss: 30.3124\n",
      "Train Epoch: 81 [0/8000 (0%)]\tLoss: 30.491219\n",
      "Train Epoch: 81 [1280/8000 (16%)]\tLoss: 30.049784\n",
      "Train Epoch: 81 [2560/8000 (32%)]\tLoss: 30.331997\n",
      "Train Epoch: 81 [3840/8000 (48%)]\tLoss: 30.408009\n",
      "Train Epoch: 81 [5120/8000 (64%)]\tLoss: 30.485470\n",
      "Train Epoch: 81 [6400/8000 (80%)]\tLoss: 30.165440\n",
      "Train Epoch: 81 [7680/8000 (96%)]\tLoss: 29.915319\n",
      "====> Epoch: 81 Average loss: 30.3116\n",
      "Train Epoch: 82 [0/8000 (0%)]\tLoss: 30.361092\n",
      "Loss tensor(1918.8503, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(644.2162, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 82 [1280/8000 (16%)]\tLoss: 30.329865\n",
      "Train Epoch: 82 [2560/8000 (32%)]\tLoss: 30.127808\n",
      "Loss tensor(1933.3232, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(646.5676, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 82 [3840/8000 (48%)]\tLoss: 30.022697\n",
      "Loss tensor(1937.1414, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(643.8550, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1961.7263, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(647.9357, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1927.1661, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(647.1775, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 82 [5120/8000 (64%)]\tLoss: 30.111589\n",
      "Train Epoch: 82 [6400/8000 (80%)]\tLoss: 29.732584\n",
      "Train Epoch: 82 [7680/8000 (96%)]\tLoss: 30.262629\n",
      "====> Epoch: 82 Average loss: 30.3200\n",
      "Train Epoch: 83 [0/8000 (0%)]\tLoss: 30.425514\n",
      "Train Epoch: 83 [1280/8000 (16%)]\tLoss: 30.251936\n",
      "Train Epoch: 83 [2560/8000 (32%)]\tLoss: 30.570467\n",
      "Loss tensor(1933.3260, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(636.3279, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1932.8062, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(635.5473, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 83 [3840/8000 (48%)]\tLoss: 30.269079\n",
      "Train Epoch: 83 [5120/8000 (64%)]\tLoss: 30.322205\n",
      "Train Epoch: 83 [6400/8000 (80%)]\tLoss: 30.348511\n",
      "Loss tensor(1946.5323, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(642.7123, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 83 [7680/8000 (96%)]\tLoss: 30.071131\n",
      "====> Epoch: 83 Average loss: 30.3031\n",
      "Train Epoch: 84 [0/8000 (0%)]\tLoss: 30.220495\n",
      "Loss tensor(1937.5518, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(643.6375, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 84 [1280/8000 (16%)]\tLoss: 30.355436\n",
      "Train Epoch: 84 [2560/8000 (32%)]\tLoss: 30.339651\n",
      "Train Epoch: 84 [3840/8000 (48%)]\tLoss: 30.541176\n",
      "Train Epoch: 84 [5120/8000 (64%)]\tLoss: 30.025587\n",
      "Train Epoch: 84 [6400/8000 (80%)]\tLoss: 29.844469\n",
      "Train Epoch: 84 [7680/8000 (96%)]\tLoss: 30.442333\n",
      "====> Epoch: 84 Average loss: 30.3115\n",
      "Train Epoch: 85 [0/8000 (0%)]\tLoss: 30.119810\n",
      "Train Epoch: 85 [1280/8000 (16%)]\tLoss: 30.174473\n",
      "Train Epoch: 85 [2560/8000 (32%)]\tLoss: 30.274118\n",
      "Train Epoch: 85 [3840/8000 (48%)]\tLoss: 30.356840\n",
      "Train Epoch: 85 [5120/8000 (64%)]\tLoss: 30.373486\n",
      "Loss tensor(1937.3201, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(638.4603, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 85 [6400/8000 (80%)]\tLoss: 30.159966\n",
      "Train Epoch: 85 [7680/8000 (96%)]\tLoss: 30.457512\n",
      "====> Epoch: 85 Average loss: 30.3034\n",
      "Train Epoch: 86 [0/8000 (0%)]\tLoss: 30.549095\n",
      "Loss tensor(1946.2670, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(645.4413, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 86 [1280/8000 (16%)]\tLoss: 30.346754\n",
      "Train Epoch: 86 [2560/8000 (32%)]\tLoss: 30.070850\n",
      "Train Epoch: 86 [3840/8000 (48%)]\tLoss: 30.186703\n",
      "Train Epoch: 86 [5120/8000 (64%)]\tLoss: 30.233479\n",
      "Train Epoch: 86 [6400/8000 (80%)]\tLoss: 30.410105\n",
      "Train Epoch: 86 [7680/8000 (96%)]\tLoss: 30.225639\n",
      "====> Epoch: 86 Average loss: 30.2920\n",
      "Train Epoch: 87 [0/8000 (0%)]\tLoss: 30.180044\n",
      "Train Epoch: 87 [1280/8000 (16%)]\tLoss: 30.097673\n",
      "Train Epoch: 87 [2560/8000 (32%)]\tLoss: 30.192163\n",
      "Train Epoch: 87 [3840/8000 (48%)]\tLoss: 30.199965\n",
      "Train Epoch: 87 [5120/8000 (64%)]\tLoss: 30.483755\n",
      "Train Epoch: 87 [6400/8000 (80%)]\tLoss: 30.423414\n",
      "Loss tensor(1942.4984, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(630.6101, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 87 [7680/8000 (96%)]\tLoss: 30.124203\n",
      "====> Epoch: 87 Average loss: 30.3067\n",
      "Train Epoch: 88 [0/8000 (0%)]\tLoss: 30.173573\n",
      "Loss tensor(1937.3044, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(629.4084, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1958.2046, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(631.3483, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 88 [1280/8000 (16%)]\tLoss: 30.731035\n",
      "Train Epoch: 88 [2560/8000 (32%)]\tLoss: 30.136318\n",
      "Train Epoch: 88 [3840/8000 (48%)]\tLoss: 30.067202\n",
      "Train Epoch: 88 [5120/8000 (64%)]\tLoss: 30.606403\n",
      "Train Epoch: 88 [6400/8000 (80%)]\tLoss: 30.100548\n",
      "Loss tensor(1940.2777, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(632.9464, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 88 [7680/8000 (96%)]\tLoss: 30.083323\n",
      "====> Epoch: 88 Average loss: 30.2985\n",
      "Train Epoch: 89 [0/8000 (0%)]\tLoss: 30.219017\n",
      "Train Epoch: 89 [1280/8000 (16%)]\tLoss: 30.286509\n",
      "Train Epoch: 89 [2560/8000 (32%)]\tLoss: 30.191319\n",
      "Train Epoch: 89 [3840/8000 (48%)]\tLoss: 30.099865\n",
      "Train Epoch: 89 [5120/8000 (64%)]\tLoss: 30.461367\n",
      "Loss tensor(1939.8979, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(622.5930, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 89 [6400/8000 (80%)]\tLoss: 30.237579\n",
      "Train Epoch: 89 [7680/8000 (96%)]\tLoss: 30.336828\n",
      "====> Epoch: 89 Average loss: 30.3010\n",
      "Train Epoch: 90 [0/8000 (0%)]\tLoss: 30.370991\n",
      "Train Epoch: 90 [1280/8000 (16%)]\tLoss: 30.418240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 [2560/8000 (32%)]\tLoss: 30.141520\n",
      "Train Epoch: 90 [3840/8000 (48%)]\tLoss: 30.208429\n",
      "Train Epoch: 90 [5120/8000 (64%)]\tLoss: 30.171877\n",
      "Train Epoch: 90 [6400/8000 (80%)]\tLoss: 30.273438\n",
      "Loss tensor(1959.6921, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(636.1770, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 90 [7680/8000 (96%)]\tLoss: 30.034443\n",
      "====> Epoch: 90 Average loss: 30.2990\n",
      "Train Epoch: 91 [0/8000 (0%)]\tLoss: 30.175894\n",
      "Train Epoch: 91 [1280/8000 (16%)]\tLoss: 30.538231\n",
      "Train Epoch: 91 [2560/8000 (32%)]\tLoss: 30.319336\n",
      "Train Epoch: 91 [3840/8000 (48%)]\tLoss: 30.639622\n",
      "Train Epoch: 91 [5120/8000 (64%)]\tLoss: 30.078983\n",
      "Train Epoch: 91 [6400/8000 (80%)]\tLoss: 30.198975\n",
      "Train Epoch: 91 [7680/8000 (96%)]\tLoss: 30.229504\n",
      "====> Epoch: 91 Average loss: 30.3068\n",
      "Train Epoch: 92 [0/8000 (0%)]\tLoss: 30.210400\n",
      "Loss tensor(1951.3401, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(635.0835, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 92 [1280/8000 (16%)]\tLoss: 29.962620\n",
      "Train Epoch: 92 [2560/8000 (32%)]\tLoss: 30.053410\n",
      "Train Epoch: 92 [3840/8000 (48%)]\tLoss: 30.491613\n",
      "Train Epoch: 92 [5120/8000 (64%)]\tLoss: 30.283997\n",
      "Train Epoch: 92 [6400/8000 (80%)]\tLoss: 30.293917\n",
      "Train Epoch: 92 [7680/8000 (96%)]\tLoss: 30.217960\n",
      "====> Epoch: 92 Average loss: 30.2988\n",
      "Train Epoch: 93 [0/8000 (0%)]\tLoss: 30.409386\n",
      "Loss tensor(1913.7794, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(639.3301, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 93 [1280/8000 (16%)]\tLoss: 30.052584\n",
      "Train Epoch: 93 [2560/8000 (32%)]\tLoss: 29.935530\n",
      "Train Epoch: 93 [3840/8000 (48%)]\tLoss: 30.357191\n",
      "Train Epoch: 93 [5120/8000 (64%)]\tLoss: 30.521461\n",
      "Train Epoch: 93 [6400/8000 (80%)]\tLoss: 30.123388\n",
      "Train Epoch: 93 [7680/8000 (96%)]\tLoss: 30.127033\n",
      "====> Epoch: 93 Average loss: 30.3047\n",
      "Train Epoch: 94 [0/8000 (0%)]\tLoss: 30.069977\n",
      "Train Epoch: 94 [1280/8000 (16%)]\tLoss: 30.392385\n",
      "Train Epoch: 94 [2560/8000 (32%)]\tLoss: 30.220171\n",
      "Loss tensor(1929.2479, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(646.8352, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 94 [3840/8000 (48%)]\tLoss: 30.158052\n",
      "Train Epoch: 94 [5120/8000 (64%)]\tLoss: 30.106279\n",
      "Train Epoch: 94 [6400/8000 (80%)]\tLoss: 30.244961\n",
      "Train Epoch: 94 [7680/8000 (96%)]\tLoss: 30.331194\n",
      "====> Epoch: 94 Average loss: 30.3048\n",
      "Train Epoch: 95 [0/8000 (0%)]\tLoss: 30.103968\n",
      "Train Epoch: 95 [1280/8000 (16%)]\tLoss: 30.366226\n",
      "Train Epoch: 95 [2560/8000 (32%)]\tLoss: 30.445719\n",
      "Train Epoch: 95 [3840/8000 (48%)]\tLoss: 30.166807\n",
      "Train Epoch: 95 [5120/8000 (64%)]\tLoss: 30.396654\n",
      "Train Epoch: 95 [6400/8000 (80%)]\tLoss: 29.874954\n",
      "Train Epoch: 95 [7680/8000 (96%)]\tLoss: 30.282780\n",
      "====> Epoch: 95 Average loss: 30.3000\n",
      "Train Epoch: 96 [0/8000 (0%)]\tLoss: 30.062336\n",
      "Loss tensor(1925.7511, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(645.5601, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 96 [1280/8000 (16%)]\tLoss: 30.223343\n",
      "Train Epoch: 96 [2560/8000 (32%)]\tLoss: 30.249479\n",
      "Train Epoch: 96 [3840/8000 (48%)]\tLoss: 30.186140\n",
      "Train Epoch: 96 [5120/8000 (64%)]\tLoss: 30.253475\n",
      "Loss tensor(1937.0129, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(651.5621, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 96 [6400/8000 (80%)]\tLoss: 30.189075\n",
      "Train Epoch: 96 [7680/8000 (96%)]\tLoss: 30.503426\n",
      "====> Epoch: 96 Average loss: 30.2998\n",
      "Train Epoch: 97 [0/8000 (0%)]\tLoss: 29.995741\n",
      "Train Epoch: 97 [1280/8000 (16%)]\tLoss: 30.202126\n",
      "Train Epoch: 97 [2560/8000 (32%)]\tLoss: 30.431190\n",
      "Train Epoch: 97 [3840/8000 (48%)]\tLoss: 30.324186\n",
      "Train Epoch: 97 [5120/8000 (64%)]\tLoss: 30.271454\n",
      "Train Epoch: 97 [6400/8000 (80%)]\tLoss: 30.340700\n",
      "Train Epoch: 97 [7680/8000 (96%)]\tLoss: 30.036282\n",
      "Loss tensor(1951.0919, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(650.1323, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 97 Average loss: 30.2987\n",
      "Train Epoch: 98 [0/8000 (0%)]\tLoss: 30.175373\n",
      "Train Epoch: 98 [1280/8000 (16%)]\tLoss: 30.471426\n",
      "Train Epoch: 98 [2560/8000 (32%)]\tLoss: 30.465954\n",
      "Loss tensor(1955.3455, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(654.0872, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1949.5057, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(655.1006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 98 [3840/8000 (48%)]\tLoss: 30.138054\n",
      "Loss tensor(1922.3375, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(644.4810, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 98 [5120/8000 (64%)]\tLoss: 30.246431\n",
      "Train Epoch: 98 [6400/8000 (80%)]\tLoss: 30.151167\n",
      "Train Epoch: 98 [7680/8000 (96%)]\tLoss: 29.996428\n",
      "====> Epoch: 98 Average loss: 30.2996\n",
      "Train Epoch: 99 [0/8000 (0%)]\tLoss: 30.104429\n",
      "Train Epoch: 99 [1280/8000 (16%)]\tLoss: 30.091785\n",
      "Train Epoch: 99 [2560/8000 (32%)]\tLoss: 30.035488\n",
      "Train Epoch: 99 [3840/8000 (48%)]\tLoss: 30.271761\n",
      "Train Epoch: 99 [5120/8000 (64%)]\tLoss: 30.171530\n",
      "Train Epoch: 99 [6400/8000 (80%)]\tLoss: 30.158646\n",
      "Train Epoch: 99 [7680/8000 (96%)]\tLoss: 30.468931\n",
      "====> Epoch: 99 Average loss: 30.2908\n",
      "Train Epoch: 100 [0/8000 (0%)]\tLoss: 30.311081\n",
      "Train Epoch: 100 [1280/8000 (16%)]\tLoss: 30.370359\n",
      "Train Epoch: 100 [2560/8000 (32%)]\tLoss: 30.771164\n",
      "Train Epoch: 100 [3840/8000 (48%)]\tLoss: 30.191971\n",
      "Loss tensor(1937.7386, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(644.1761, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1946.8087, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(644.9543, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 100 [5120/8000 (64%)]\tLoss: 30.335146\n",
      "Train Epoch: 100 [6400/8000 (80%)]\tLoss: 30.369213\n",
      "Loss tensor(1937.7410, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(658.1998, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 100 [7680/8000 (96%)]\tLoss: 30.363401\n",
      "====> Epoch: 100 Average loss: 30.2924\n",
      "Train Epoch: 101 [0/8000 (0%)]\tLoss: 30.208134\n",
      "Train Epoch: 101 [1280/8000 (16%)]\tLoss: 30.676828\n",
      "Train Epoch: 101 [2560/8000 (32%)]\tLoss: 30.386469\n",
      "Train Epoch: 101 [3840/8000 (48%)]\tLoss: 30.297977\n",
      "Train Epoch: 101 [5120/8000 (64%)]\tLoss: 30.275873\n",
      "Loss tensor(1930.2258, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(651.2649, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 101 [6400/8000 (80%)]\tLoss: 30.042023\n",
      "Train Epoch: 101 [7680/8000 (96%)]\tLoss: 30.193050\n",
      "====> Epoch: 101 Average loss: 30.3004\n",
      "Train Epoch: 102 [0/8000 (0%)]\tLoss: 30.509531\n",
      "Train Epoch: 102 [1280/8000 (16%)]\tLoss: 30.050974\n",
      "Loss tensor(1926.3043, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(646.9836, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 102 [2560/8000 (32%)]\tLoss: 30.382662\n",
      "Train Epoch: 102 [3840/8000 (48%)]\tLoss: 30.552528\n",
      "Train Epoch: 102 [5120/8000 (64%)]\tLoss: 30.276718\n",
      "Train Epoch: 102 [6400/8000 (80%)]\tLoss: 30.318539\n",
      "Train Epoch: 102 [7680/8000 (96%)]\tLoss: 29.939865\n",
      "====> Epoch: 102 Average loss: 30.2908\n",
      "Train Epoch: 103 [0/8000 (0%)]\tLoss: 30.099699\n",
      "Loss tensor(1909.9312, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(652.1980, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 103 [1280/8000 (16%)]\tLoss: 30.534353\n",
      "Train Epoch: 103 [2560/8000 (32%)]\tLoss: 30.353004\n",
      "Loss tensor(1938.8038, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(648.0176, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1953.5723, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(649.6085, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 103 [3840/8000 (48%)]\tLoss: 30.287920\n",
      "Train Epoch: 103 [5120/8000 (64%)]\tLoss: 30.532352\n",
      "Train Epoch: 103 [6400/8000 (80%)]\tLoss: 30.145016\n",
      "Loss tensor(1934.7529, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(652.7565, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1951.2582, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(652.6197, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 103 [7680/8000 (96%)]\tLoss: 30.408131\n",
      "====> Epoch: 103 Average loss: 30.2898\n",
      "Train Epoch: 104 [0/8000 (0%)]\tLoss: 30.073946\n",
      "Train Epoch: 104 [1280/8000 (16%)]\tLoss: 30.093279\n",
      "Train Epoch: 104 [2560/8000 (32%)]\tLoss: 30.081593\n",
      "Train Epoch: 104 [3840/8000 (48%)]\tLoss: 30.396778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 104 [5120/8000 (64%)]\tLoss: 30.267235\n",
      "Train Epoch: 104 [6400/8000 (80%)]\tLoss: 30.154779\n",
      "Loss tensor(1935.2325, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(645.6372, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 104 [7680/8000 (96%)]\tLoss: 30.300640\n",
      "====> Epoch: 104 Average loss: 30.2953\n",
      "Train Epoch: 105 [0/8000 (0%)]\tLoss: 30.447794\n",
      "Train Epoch: 105 [1280/8000 (16%)]\tLoss: 30.137058\n",
      "Loss tensor(1912.7365, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(655.6311, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 105 [2560/8000 (32%)]\tLoss: 30.103338\n",
      "Train Epoch: 105 [3840/8000 (48%)]\tLoss: 30.382751\n",
      "Train Epoch: 105 [5120/8000 (64%)]\tLoss: 30.200260\n",
      "Train Epoch: 105 [6400/8000 (80%)]\tLoss: 30.436899\n",
      "Train Epoch: 105 [7680/8000 (96%)]\tLoss: 30.203049\n",
      "====> Epoch: 105 Average loss: 30.2876\n",
      "Train Epoch: 106 [0/8000 (0%)]\tLoss: 29.842510\n",
      "Train Epoch: 106 [1280/8000 (16%)]\tLoss: 30.469685\n",
      "Train Epoch: 106 [2560/8000 (32%)]\tLoss: 30.374067\n",
      "Train Epoch: 106 [3840/8000 (48%)]\tLoss: 30.555304\n",
      "Train Epoch: 106 [5120/8000 (64%)]\tLoss: 30.135126\n",
      "Train Epoch: 106 [6400/8000 (80%)]\tLoss: 30.417675\n",
      "Loss tensor(1916.6263, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(641.5739, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 106 [7680/8000 (96%)]\tLoss: 30.010729\n",
      "====> Epoch: 106 Average loss: 30.2943\n",
      "Train Epoch: 107 [0/8000 (0%)]\tLoss: 30.435596\n",
      "Train Epoch: 107 [1280/8000 (16%)]\tLoss: 30.452995\n",
      "Train Epoch: 107 [2560/8000 (32%)]\tLoss: 30.559160\n",
      "Train Epoch: 107 [3840/8000 (48%)]\tLoss: 30.389771\n",
      "Train Epoch: 107 [5120/8000 (64%)]\tLoss: 30.108585\n",
      "Train Epoch: 107 [6400/8000 (80%)]\tLoss: 30.478294\n",
      "Train Epoch: 107 [7680/8000 (96%)]\tLoss: 30.412539\n",
      "====> Epoch: 107 Average loss: 30.2861\n",
      "Train Epoch: 108 [0/8000 (0%)]\tLoss: 30.442333\n",
      "Loss tensor(1930.9963, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(648.7323, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 108 [1280/8000 (16%)]\tLoss: 30.169756\n",
      "Loss tensor(1936.5771, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(644.4836, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 108 [2560/8000 (32%)]\tLoss: 30.310734\n",
      "Train Epoch: 108 [3840/8000 (48%)]\tLoss: 30.465338\n",
      "Train Epoch: 108 [5120/8000 (64%)]\tLoss: 30.618980\n",
      "Loss tensor(1932.9724, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(644.6539, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 108 [6400/8000 (80%)]\tLoss: 30.129053\n",
      "Train Epoch: 108 [7680/8000 (96%)]\tLoss: 30.233812\n",
      "====> Epoch: 108 Average loss: 30.2918\n",
      "Train Epoch: 109 [0/8000 (0%)]\tLoss: 30.003670\n",
      "Loss tensor(1931.4000, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(657.1232, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 109 [1280/8000 (16%)]\tLoss: 30.031406\n",
      "Train Epoch: 109 [2560/8000 (32%)]\tLoss: 30.276173\n",
      "Train Epoch: 109 [3840/8000 (48%)]\tLoss: 30.593586\n",
      "Train Epoch: 109 [5120/8000 (64%)]\tLoss: 30.118340\n",
      "Train Epoch: 109 [6400/8000 (80%)]\tLoss: 30.679783\n",
      "Train Epoch: 109 [7680/8000 (96%)]\tLoss: 30.291517\n",
      "====> Epoch: 109 Average loss: 30.2916\n",
      "Train Epoch: 110 [0/8000 (0%)]\tLoss: 30.138659\n",
      "Train Epoch: 110 [1280/8000 (16%)]\tLoss: 30.345152\n",
      "Train Epoch: 110 [2560/8000 (32%)]\tLoss: 30.459530\n",
      "Train Epoch: 110 [3840/8000 (48%)]\tLoss: 30.555773\n",
      "Train Epoch: 110 [5120/8000 (64%)]\tLoss: 30.387682\n",
      "Train Epoch: 110 [6400/8000 (80%)]\tLoss: 30.594084\n",
      "Train Epoch: 110 [7680/8000 (96%)]\tLoss: 30.298719\n",
      "====> Epoch: 110 Average loss: 30.2931\n",
      "Train Epoch: 111 [0/8000 (0%)]\tLoss: 30.330179\n",
      "Train Epoch: 111 [1280/8000 (16%)]\tLoss: 30.311562\n",
      "Train Epoch: 111 [2560/8000 (32%)]\tLoss: 30.579992\n",
      "Train Epoch: 111 [3840/8000 (48%)]\tLoss: 30.531929\n",
      "Train Epoch: 111 [5120/8000 (64%)]\tLoss: 30.278913\n",
      "Train Epoch: 111 [6400/8000 (80%)]\tLoss: 29.906246\n",
      "Loss tensor(1926.6152, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(655.9338, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 111 [7680/8000 (96%)]\tLoss: 30.363379\n",
      "====> Epoch: 111 Average loss: 30.2917\n",
      "Train Epoch: 112 [0/8000 (0%)]\tLoss: 30.008352\n",
      "Train Epoch: 112 [1280/8000 (16%)]\tLoss: 30.038589\n",
      "Train Epoch: 112 [2560/8000 (32%)]\tLoss: 30.341528\n",
      "Train Epoch: 112 [3840/8000 (48%)]\tLoss: 30.117598\n",
      "Train Epoch: 112 [5120/8000 (64%)]\tLoss: 30.393627\n",
      "Train Epoch: 112 [6400/8000 (80%)]\tLoss: 30.180904\n",
      "Loss tensor(1930.7047, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(652.8722, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 112 [7680/8000 (96%)]\tLoss: 30.295435\n",
      "====> Epoch: 112 Average loss: 30.2775\n",
      "Train Epoch: 113 [0/8000 (0%)]\tLoss: 30.193684\n",
      "Train Epoch: 113 [1280/8000 (16%)]\tLoss: 30.274508\n",
      "Loss tensor(1928.7213, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(643.6814, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 113 [2560/8000 (32%)]\tLoss: 30.621250\n",
      "Train Epoch: 113 [3840/8000 (48%)]\tLoss: 30.113073\n",
      "Train Epoch: 113 [5120/8000 (64%)]\tLoss: 30.772532\n",
      "Train Epoch: 113 [6400/8000 (80%)]\tLoss: 30.127075\n",
      "Train Epoch: 113 [7680/8000 (96%)]\tLoss: 30.226496\n",
      "Loss tensor(1924.5161, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(650.1135, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 113 Average loss: 30.2817\n",
      "Train Epoch: 114 [0/8000 (0%)]\tLoss: 30.261206\n",
      "Train Epoch: 114 [1280/8000 (16%)]\tLoss: 30.308239\n",
      "Train Epoch: 114 [2560/8000 (32%)]\tLoss: 30.197069\n",
      "Train Epoch: 114 [3840/8000 (48%)]\tLoss: 30.221006\n",
      "Train Epoch: 114 [5120/8000 (64%)]\tLoss: 30.468143\n",
      "Train Epoch: 114 [6400/8000 (80%)]\tLoss: 30.085188\n",
      "Train Epoch: 114 [7680/8000 (96%)]\tLoss: 30.604820\n",
      "Loss tensor(1938.0878, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(660.8534, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 114 Average loss: 30.2955\n",
      "Train Epoch: 115 [0/8000 (0%)]\tLoss: 30.403160\n",
      "Train Epoch: 115 [1280/8000 (16%)]\tLoss: 30.580103\n",
      "Train Epoch: 115 [2560/8000 (32%)]\tLoss: 30.196476\n",
      "Train Epoch: 115 [3840/8000 (48%)]\tLoss: 29.931389\n",
      "Loss tensor(1953.9734, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(660.1299, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 115 [5120/8000 (64%)]\tLoss: 30.129515\n",
      "Train Epoch: 115 [6400/8000 (80%)]\tLoss: 29.977577\n",
      "Train Epoch: 115 [7680/8000 (96%)]\tLoss: 30.042908\n",
      "====> Epoch: 115 Average loss: 30.2701\n",
      "Train Epoch: 116 [0/8000 (0%)]\tLoss: 30.253567\n",
      "Train Epoch: 116 [1280/8000 (16%)]\tLoss: 30.113516\n",
      "Train Epoch: 116 [2560/8000 (32%)]\tLoss: 30.346666\n",
      "Train Epoch: 116 [3840/8000 (48%)]\tLoss: 30.324408\n",
      "Loss tensor(1935.1777, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(661.7054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 116 [5120/8000 (64%)]\tLoss: 30.237152\n",
      "Train Epoch: 116 [6400/8000 (80%)]\tLoss: 30.293106\n",
      "Train Epoch: 116 [7680/8000 (96%)]\tLoss: 30.155397\n",
      "Loss tensor(1948.7321, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(659.3743, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 116 Average loss: 30.2963\n",
      "Train Epoch: 117 [0/8000 (0%)]\tLoss: 30.049442\n",
      "Train Epoch: 117 [1280/8000 (16%)]\tLoss: 30.191116\n",
      "Train Epoch: 117 [2560/8000 (32%)]\tLoss: 30.005739\n",
      "Loss tensor(1940.3058, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(664.0101, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 117 [3840/8000 (48%)]\tLoss: 30.340895\n",
      "Train Epoch: 117 [5120/8000 (64%)]\tLoss: 30.154747\n",
      "Train Epoch: 117 [6400/8000 (80%)]\tLoss: 30.481131\n",
      "Loss tensor(1942.0388, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(659.6351, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 117 [7680/8000 (96%)]\tLoss: 30.175758\n",
      "====> Epoch: 117 Average loss: 30.2777\n",
      "Train Epoch: 118 [0/8000 (0%)]\tLoss: 30.228487\n",
      "Loss tensor(1936.5397, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(664.2710, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 118 [1280/8000 (16%)]\tLoss: 30.394241\n",
      "Train Epoch: 118 [2560/8000 (32%)]\tLoss: 30.484762\n",
      "Loss tensor(1946.5747, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(655.2488, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 118 [3840/8000 (48%)]\tLoss: 30.395267\n",
      "Train Epoch: 118 [5120/8000 (64%)]\tLoss: 30.282871\n",
      "Train Epoch: 118 [6400/8000 (80%)]\tLoss: 30.472483\n",
      "Train Epoch: 118 [7680/8000 (96%)]\tLoss: 30.381407\n",
      "====> Epoch: 118 Average loss: 30.2782\n",
      "Train Epoch: 119 [0/8000 (0%)]\tLoss: 30.136187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 119 [1280/8000 (16%)]\tLoss: 30.293501\n",
      "Train Epoch: 119 [2560/8000 (32%)]\tLoss: 30.350134\n",
      "Train Epoch: 119 [3840/8000 (48%)]\tLoss: 29.885437\n",
      "Train Epoch: 119 [5120/8000 (64%)]\tLoss: 30.108078\n",
      "Loss tensor(1921.5170, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(657.6395, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 119 [6400/8000 (80%)]\tLoss: 30.194677\n",
      "Train Epoch: 119 [7680/8000 (96%)]\tLoss: 30.004494\n",
      "====> Epoch: 119 Average loss: 30.2816\n",
      "Train Epoch: 120 [0/8000 (0%)]\tLoss: 29.977474\n",
      "Train Epoch: 120 [1280/8000 (16%)]\tLoss: 30.265638\n",
      "Train Epoch: 120 [2560/8000 (32%)]\tLoss: 30.449884\n",
      "Train Epoch: 120 [3840/8000 (48%)]\tLoss: 30.186409\n",
      "Train Epoch: 120 [5120/8000 (64%)]\tLoss: 30.342670\n",
      "Train Epoch: 120 [6400/8000 (80%)]\tLoss: 30.483631\n",
      "Train Epoch: 120 [7680/8000 (96%)]\tLoss: 30.218395\n",
      "====> Epoch: 120 Average loss: 30.2777\n",
      "Train Epoch: 121 [0/8000 (0%)]\tLoss: 30.115330\n",
      "Train Epoch: 121 [1280/8000 (16%)]\tLoss: 30.428734\n",
      "Train Epoch: 121 [2560/8000 (32%)]\tLoss: 30.419394\n",
      "Train Epoch: 121 [3840/8000 (48%)]\tLoss: 30.189077\n",
      "Train Epoch: 121 [5120/8000 (64%)]\tLoss: 30.149281\n",
      "Loss tensor(1959.8701, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(664.5322, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 121 [6400/8000 (80%)]\tLoss: 30.450418\n",
      "Train Epoch: 121 [7680/8000 (96%)]\tLoss: 30.069098\n",
      "====> Epoch: 121 Average loss: 30.2752\n",
      "Train Epoch: 122 [0/8000 (0%)]\tLoss: 30.057318\n",
      "Train Epoch: 122 [1280/8000 (16%)]\tLoss: 30.463568\n",
      "Loss tensor(1939.9019, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(662.8312, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 122 [2560/8000 (32%)]\tLoss: 30.396935\n",
      "Train Epoch: 122 [3840/8000 (48%)]\tLoss: 29.949175\n",
      "Train Epoch: 122 [5120/8000 (64%)]\tLoss: 30.284756\n",
      "Train Epoch: 122 [6400/8000 (80%)]\tLoss: 30.160385\n",
      "Train Epoch: 122 [7680/8000 (96%)]\tLoss: 30.379305\n",
      "====> Epoch: 122 Average loss: 30.2864\n",
      "Train Epoch: 123 [0/8000 (0%)]\tLoss: 30.178038\n",
      "Train Epoch: 123 [1280/8000 (16%)]\tLoss: 30.286316\n",
      "Train Epoch: 123 [2560/8000 (32%)]\tLoss: 30.384493\n",
      "Train Epoch: 123 [3840/8000 (48%)]\tLoss: 30.331440\n",
      "Train Epoch: 123 [5120/8000 (64%)]\tLoss: 30.134918\n",
      "Train Epoch: 123 [6400/8000 (80%)]\tLoss: 30.673738\n",
      "Train Epoch: 123 [7680/8000 (96%)]\tLoss: 30.004005\n",
      "Loss tensor(1938.6304, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(675.3593, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 123 Average loss: 30.2836\n",
      "Train Epoch: 124 [0/8000 (0%)]\tLoss: 29.945799\n",
      "Train Epoch: 124 [1280/8000 (16%)]\tLoss: 30.512093\n",
      "Loss tensor(1941.0570, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(673.0824, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 124 [2560/8000 (32%)]\tLoss: 30.166672\n",
      "Train Epoch: 124 [3840/8000 (48%)]\tLoss: 30.599960\n",
      "Train Epoch: 124 [5120/8000 (64%)]\tLoss: 29.852730\n",
      "Train Epoch: 124 [6400/8000 (80%)]\tLoss: 30.431126\n",
      "Train Epoch: 124 [7680/8000 (96%)]\tLoss: 30.455360\n",
      "====> Epoch: 124 Average loss: 30.2747\n",
      "Train Epoch: 125 [0/8000 (0%)]\tLoss: 30.239674\n",
      "Loss tensor(1926.1906, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(670.4122, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 125 [1280/8000 (16%)]\tLoss: 30.352730\n",
      "Train Epoch: 125 [2560/8000 (32%)]\tLoss: 30.022049\n",
      "Train Epoch: 125 [3840/8000 (48%)]\tLoss: 30.096079\n",
      "Train Epoch: 125 [5120/8000 (64%)]\tLoss: 30.149300\n",
      "Loss tensor(1926.2280, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(670.2206, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 125 [6400/8000 (80%)]\tLoss: 30.097313\n",
      "Train Epoch: 125 [7680/8000 (96%)]\tLoss: 30.161760\n",
      "====> Epoch: 125 Average loss: 30.2813\n",
      "Train Epoch: 126 [0/8000 (0%)]\tLoss: 30.156101\n",
      "Train Epoch: 126 [1280/8000 (16%)]\tLoss: 30.194784\n",
      "Train Epoch: 126 [2560/8000 (32%)]\tLoss: 30.077726\n",
      "Train Epoch: 126 [3840/8000 (48%)]\tLoss: 30.098677\n",
      "Train Epoch: 126 [5120/8000 (64%)]\tLoss: 30.163481\n",
      "Loss tensor(1929.5538, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(675.6544, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 126 [6400/8000 (80%)]\tLoss: 30.303350\n",
      "Loss tensor(1945.5750, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(676.6419, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 126 [7680/8000 (96%)]\tLoss: 30.250725\n",
      "====> Epoch: 126 Average loss: 30.2730\n",
      "Train Epoch: 127 [0/8000 (0%)]\tLoss: 30.494104\n",
      "Train Epoch: 127 [1280/8000 (16%)]\tLoss: 30.265570\n",
      "Loss tensor(1926.9479, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(675.0769, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 127 [2560/8000 (32%)]\tLoss: 30.294540\n",
      "Train Epoch: 127 [3840/8000 (48%)]\tLoss: 30.029461\n",
      "Train Epoch: 127 [5120/8000 (64%)]\tLoss: 30.288816\n",
      "Train Epoch: 127 [6400/8000 (80%)]\tLoss: 30.169849\n",
      "Train Epoch: 127 [7680/8000 (96%)]\tLoss: 30.316698\n",
      "====> Epoch: 127 Average loss: 30.2687\n",
      "Train Epoch: 128 [0/8000 (0%)]\tLoss: 30.587788\n",
      "Train Epoch: 128 [1280/8000 (16%)]\tLoss: 30.086149\n",
      "Loss tensor(1950.6387, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(672.6201, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 128 [2560/8000 (32%)]\tLoss: 29.665775\n",
      "Train Epoch: 128 [3840/8000 (48%)]\tLoss: 30.277565\n",
      "Train Epoch: 128 [5120/8000 (64%)]\tLoss: 29.950951\n",
      "Train Epoch: 128 [6400/8000 (80%)]\tLoss: 29.899071\n",
      "Train Epoch: 128 [7680/8000 (96%)]\tLoss: 30.204136\n",
      "====> Epoch: 128 Average loss: 30.2519\n",
      "Train Epoch: 129 [0/8000 (0%)]\tLoss: 30.424652\n",
      "Train Epoch: 129 [1280/8000 (16%)]\tLoss: 30.503639\n",
      "Train Epoch: 129 [2560/8000 (32%)]\tLoss: 30.347418\n",
      "Train Epoch: 129 [3840/8000 (48%)]\tLoss: 30.299313\n",
      "Loss tensor(1936.3420, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(675.6807, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 129 [5120/8000 (64%)]\tLoss: 30.006874\n",
      "Train Epoch: 129 [6400/8000 (80%)]\tLoss: 30.451101\n",
      "Loss tensor(1939.2345, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(672.9426, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 129 [7680/8000 (96%)]\tLoss: 30.372925\n",
      "====> Epoch: 129 Average loss: 30.2544\n",
      "Train Epoch: 130 [0/8000 (0%)]\tLoss: 30.070398\n",
      "Train Epoch: 130 [1280/8000 (16%)]\tLoss: 30.153727\n",
      "Train Epoch: 130 [2560/8000 (32%)]\tLoss: 30.569855\n",
      "Train Epoch: 130 [3840/8000 (48%)]\tLoss: 30.431408\n",
      "Train Epoch: 130 [5120/8000 (64%)]\tLoss: 30.086573\n",
      "Loss tensor(1949.5579, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(676.0581, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 130 [6400/8000 (80%)]\tLoss: 30.220238\n",
      "Loss tensor(1934.4502, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(678.5065, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 130 [7680/8000 (96%)]\tLoss: 30.129549\n",
      "====> Epoch: 130 Average loss: 30.2539\n",
      "Train Epoch: 131 [0/8000 (0%)]\tLoss: 30.246893\n",
      "Train Epoch: 131 [1280/8000 (16%)]\tLoss: 30.137978\n",
      "Train Epoch: 131 [2560/8000 (32%)]\tLoss: 30.396978\n",
      "Loss tensor(1920.2216, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(678.0780, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 131 [3840/8000 (48%)]\tLoss: 30.313650\n",
      "Train Epoch: 131 [5120/8000 (64%)]\tLoss: 30.566450\n",
      "Train Epoch: 131 [6400/8000 (80%)]\tLoss: 30.316809\n",
      "Loss tensor(1944.1589, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(686.3171, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 131 [7680/8000 (96%)]\tLoss: 30.014357\n",
      "====> Epoch: 131 Average loss: 30.2556\n",
      "Train Epoch: 132 [0/8000 (0%)]\tLoss: 30.310600\n",
      "Train Epoch: 132 [1280/8000 (16%)]\tLoss: 29.984465\n",
      "Train Epoch: 132 [2560/8000 (32%)]\tLoss: 30.073032\n",
      "Train Epoch: 132 [3840/8000 (48%)]\tLoss: 30.129583\n",
      "Loss tensor(1935.2836, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(681.0636, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 132 [5120/8000 (64%)]\tLoss: 30.568237\n",
      "Train Epoch: 132 [6400/8000 (80%)]\tLoss: 30.091997\n",
      "Train Epoch: 132 [7680/8000 (96%)]\tLoss: 30.210014\n",
      "====> Epoch: 132 Average loss: 30.2479\n",
      "Train Epoch: 133 [0/8000 (0%)]\tLoss: 30.576515\n",
      "Train Epoch: 133 [1280/8000 (16%)]\tLoss: 30.327328\n",
      "Train Epoch: 133 [2560/8000 (32%)]\tLoss: 30.268055\n",
      "Train Epoch: 133 [3840/8000 (48%)]\tLoss: 30.133625\n",
      "Train Epoch: 133 [5120/8000 (64%)]\tLoss: 30.010983\n",
      "Train Epoch: 133 [6400/8000 (80%)]\tLoss: 30.112150\n",
      "Loss tensor(1940.9650, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(691.2553, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 133 [7680/8000 (96%)]\tLoss: 30.117626\n",
      "====> Epoch: 133 Average loss: 30.2463\n",
      "Train Epoch: 134 [0/8000 (0%)]\tLoss: 30.373518\n",
      "Train Epoch: 134 [1280/8000 (16%)]\tLoss: 30.107586\n",
      "Train Epoch: 134 [2560/8000 (32%)]\tLoss: 30.522360\n",
      "Train Epoch: 134 [3840/8000 (48%)]\tLoss: 30.042023\n",
      "Loss tensor(1925.4126, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(697.4074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 134 [5120/8000 (64%)]\tLoss: 30.430143\n",
      "Train Epoch: 134 [6400/8000 (80%)]\tLoss: 30.544811\n",
      "Train Epoch: 134 [7680/8000 (96%)]\tLoss: 30.043188\n",
      "====> Epoch: 134 Average loss: 30.2721\n",
      "Train Epoch: 135 [0/8000 (0%)]\tLoss: 30.523899\n",
      "Train Epoch: 135 [1280/8000 (16%)]\tLoss: 30.208305\n",
      "Train Epoch: 135 [2560/8000 (32%)]\tLoss: 30.144720\n",
      "Train Epoch: 135 [3840/8000 (48%)]\tLoss: 29.887394\n",
      "Train Epoch: 135 [5120/8000 (64%)]\tLoss: 30.283993\n",
      "Loss tensor(1930.2827, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(692.3483, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 135 [6400/8000 (80%)]\tLoss: 30.160667\n",
      "Train Epoch: 135 [7680/8000 (96%)]\tLoss: 30.178425\n",
      "====> Epoch: 135 Average loss: 30.2638\n",
      "Train Epoch: 136 [0/8000 (0%)]\tLoss: 30.423296\n",
      "Train Epoch: 136 [1280/8000 (16%)]\tLoss: 30.088896\n",
      "Train Epoch: 136 [2560/8000 (32%)]\tLoss: 29.999208\n",
      "Train Epoch: 136 [3840/8000 (48%)]\tLoss: 29.940109\n",
      "Loss tensor(1933.2075, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(690.0865, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 136 [5120/8000 (64%)]\tLoss: 30.463198\n",
      "Train Epoch: 136 [6400/8000 (80%)]\tLoss: 30.126015\n",
      "Train Epoch: 136 [7680/8000 (96%)]\tLoss: 30.182644\n",
      "====> Epoch: 136 Average loss: 30.2391\n",
      "Train Epoch: 137 [0/8000 (0%)]\tLoss: 29.843630\n",
      "Loss tensor(1927.9637, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(697.8356, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 137 [1280/8000 (16%)]\tLoss: 30.330648\n",
      "Loss tensor(1939.8711, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(697.1737, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 137 [2560/8000 (32%)]\tLoss: 30.286268\n",
      "Train Epoch: 137 [3840/8000 (48%)]\tLoss: 30.190027\n",
      "Train Epoch: 137 [5120/8000 (64%)]\tLoss: 29.937988\n",
      "Loss tensor(1917.9260, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(694.0751, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 137 [6400/8000 (80%)]\tLoss: 30.052183\n",
      "Train Epoch: 137 [7680/8000 (96%)]\tLoss: 30.292931\n",
      "====> Epoch: 137 Average loss: 30.2579\n",
      "Train Epoch: 138 [0/8000 (0%)]\tLoss: 30.178410\n",
      "Train Epoch: 138 [1280/8000 (16%)]\tLoss: 30.237906\n",
      "Train Epoch: 138 [2560/8000 (32%)]\tLoss: 30.517406\n",
      "Train Epoch: 138 [3840/8000 (48%)]\tLoss: 30.611746\n",
      "Train Epoch: 138 [5120/8000 (64%)]\tLoss: 30.191065\n",
      "Loss tensor(1933.4788, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(702.9178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 138 [6400/8000 (80%)]\tLoss: 30.079117\n",
      "Train Epoch: 138 [7680/8000 (96%)]\tLoss: 30.222414\n",
      "====> Epoch: 138 Average loss: 30.2367\n",
      "Train Epoch: 139 [0/8000 (0%)]\tLoss: 30.148905\n",
      "Train Epoch: 139 [1280/8000 (16%)]\tLoss: 30.292660\n",
      "Train Epoch: 139 [2560/8000 (32%)]\tLoss: 29.941864\n",
      "Train Epoch: 139 [3840/8000 (48%)]\tLoss: 30.262222\n",
      "Train Epoch: 139 [5120/8000 (64%)]\tLoss: 30.266512\n",
      "Train Epoch: 139 [6400/8000 (80%)]\tLoss: 30.425444\n",
      "Train Epoch: 139 [7680/8000 (96%)]\tLoss: 30.074886\n",
      "====> Epoch: 139 Average loss: 30.2735\n",
      "Train Epoch: 140 [0/8000 (0%)]\tLoss: 30.358982\n",
      "Train Epoch: 140 [1280/8000 (16%)]\tLoss: 30.305286\n",
      "Train Epoch: 140 [2560/8000 (32%)]\tLoss: 30.185595\n",
      "Train Epoch: 140 [3840/8000 (48%)]\tLoss: 30.501104\n",
      "Train Epoch: 140 [5120/8000 (64%)]\tLoss: 30.230320\n",
      "Train Epoch: 140 [6400/8000 (80%)]\tLoss: 30.331530\n",
      "Loss tensor(1933.0112, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(719.2255, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 140 [7680/8000 (96%)]\tLoss: 30.201290\n",
      "====> Epoch: 140 Average loss: 30.2622\n",
      "Train Epoch: 141 [0/8000 (0%)]\tLoss: 30.411243\n",
      "Train Epoch: 141 [1280/8000 (16%)]\tLoss: 30.182415\n",
      "Train Epoch: 141 [2560/8000 (32%)]\tLoss: 29.949102\n",
      "Train Epoch: 141 [3840/8000 (48%)]\tLoss: 30.345648\n",
      "Loss tensor(1936.3356, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(727.3331, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 141 [5120/8000 (64%)]\tLoss: 30.163858\n",
      "Train Epoch: 141 [6400/8000 (80%)]\tLoss: 30.794901\n",
      "Train Epoch: 141 [7680/8000 (96%)]\tLoss: 30.280573\n",
      "====> Epoch: 141 Average loss: 30.2418\n",
      "Train Epoch: 142 [0/8000 (0%)]\tLoss: 30.072651\n",
      "Loss tensor(1930.4843, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(724.2647, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 142 [1280/8000 (16%)]\tLoss: 30.224859\n",
      "Train Epoch: 142 [2560/8000 (32%)]\tLoss: 30.065779\n",
      "Train Epoch: 142 [3840/8000 (48%)]\tLoss: 30.286707\n",
      "Train Epoch: 142 [5120/8000 (64%)]\tLoss: 30.428316\n",
      "Train Epoch: 142 [6400/8000 (80%)]\tLoss: 30.227297\n",
      "Train Epoch: 142 [7680/8000 (96%)]\tLoss: 30.289442\n",
      "====> Epoch: 142 Average loss: 30.2414\n",
      "Train Epoch: 143 [0/8000 (0%)]\tLoss: 30.030903\n",
      "Train Epoch: 143 [1280/8000 (16%)]\tLoss: 30.015259\n",
      "Train Epoch: 143 [2560/8000 (32%)]\tLoss: 30.145535\n",
      "Loss tensor(1908.3154, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(726.6980, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 143 [3840/8000 (48%)]\tLoss: 30.033861\n",
      "Train Epoch: 143 [5120/8000 (64%)]\tLoss: 30.072039\n",
      "Train Epoch: 143 [6400/8000 (80%)]\tLoss: 30.176903\n",
      "Train Epoch: 143 [7680/8000 (96%)]\tLoss: 30.503967\n",
      "====> Epoch: 143 Average loss: 30.2502\n",
      "Train Epoch: 144 [0/8000 (0%)]\tLoss: 30.325979\n",
      "Train Epoch: 144 [1280/8000 (16%)]\tLoss: 30.319132\n",
      "Train Epoch: 144 [2560/8000 (32%)]\tLoss: 29.997822\n",
      "Train Epoch: 144 [3840/8000 (48%)]\tLoss: 30.554134\n",
      "Train Epoch: 144 [5120/8000 (64%)]\tLoss: 29.948156\n",
      "Train Epoch: 144 [6400/8000 (80%)]\tLoss: 30.381159\n",
      "Train Epoch: 144 [7680/8000 (96%)]\tLoss: 30.187017\n",
      "====> Epoch: 144 Average loss: 30.2528\n",
      "Train Epoch: 145 [0/8000 (0%)]\tLoss: 30.158375\n",
      "Train Epoch: 145 [1280/8000 (16%)]\tLoss: 30.329048\n",
      "Loss tensor(1945.7379, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(740.3021, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 145 [2560/8000 (32%)]\tLoss: 30.339064\n",
      "Train Epoch: 145 [3840/8000 (48%)]\tLoss: 30.021048\n",
      "Train Epoch: 145 [5120/8000 (64%)]\tLoss: 30.234619\n",
      "Train Epoch: 145 [6400/8000 (80%)]\tLoss: 30.409206\n",
      "Train Epoch: 145 [7680/8000 (96%)]\tLoss: 30.355726\n",
      "====> Epoch: 145 Average loss: 30.2390\n",
      "Train Epoch: 146 [0/8000 (0%)]\tLoss: 30.266886\n",
      "Train Epoch: 146 [1280/8000 (16%)]\tLoss: 30.323013\n",
      "Train Epoch: 146 [2560/8000 (32%)]\tLoss: 30.506468\n",
      "Train Epoch: 146 [3840/8000 (48%)]\tLoss: 30.211260\n",
      "Train Epoch: 146 [5120/8000 (64%)]\tLoss: 30.085306\n",
      "Train Epoch: 146 [6400/8000 (80%)]\tLoss: 30.403725\n",
      "Train Epoch: 146 [7680/8000 (96%)]\tLoss: 30.231123\n",
      "====> Epoch: 146 Average loss: 30.2553\n",
      "Train Epoch: 147 [0/8000 (0%)]\tLoss: 29.879675\n",
      "Train Epoch: 147 [1280/8000 (16%)]\tLoss: 30.409929\n",
      "Train Epoch: 147 [2560/8000 (32%)]\tLoss: 30.212746\n",
      "Train Epoch: 147 [3840/8000 (48%)]\tLoss: 30.477779\n",
      "Train Epoch: 147 [5120/8000 (64%)]\tLoss: 30.307308\n",
      "Train Epoch: 147 [6400/8000 (80%)]\tLoss: 30.307011\n",
      "Loss tensor(1933.0137, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(756.5203, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 147 [7680/8000 (96%)]\tLoss: 30.210522\n",
      "====> Epoch: 147 Average loss: 30.2463\n",
      "Train Epoch: 148 [0/8000 (0%)]\tLoss: 30.065077\n",
      "Loss tensor(1945.0496, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(755.2943, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 148 [1280/8000 (16%)]\tLoss: 30.236790\n",
      "Train Epoch: 148 [2560/8000 (32%)]\tLoss: 30.162130\n",
      "Train Epoch: 148 [3840/8000 (48%)]\tLoss: 30.256968\n",
      "Train Epoch: 148 [5120/8000 (64%)]\tLoss: 30.042212\n",
      "Train Epoch: 148 [6400/8000 (80%)]\tLoss: 30.309191\n",
      "Train Epoch: 148 [7680/8000 (96%)]\tLoss: 30.253490\n",
      "====> Epoch: 148 Average loss: 30.2576\n",
      "Train Epoch: 149 [0/8000 (0%)]\tLoss: 30.202806\n",
      "Loss tensor(1940.2478, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(767.9320, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 149 [1280/8000 (16%)]\tLoss: 30.297649\n",
      "Train Epoch: 149 [2560/8000 (32%)]\tLoss: 30.094959\n",
      "Train Epoch: 149 [3840/8000 (48%)]\tLoss: 30.301762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 149 [5120/8000 (64%)]\tLoss: 30.345293\n",
      "Loss tensor(1934.9355, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(771.5923, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 149 [6400/8000 (80%)]\tLoss: 30.184240\n",
      "Train Epoch: 149 [7680/8000 (96%)]\tLoss: 30.336370\n",
      "====> Epoch: 149 Average loss: 30.2304\n",
      "Train Epoch: 150 [0/8000 (0%)]\tLoss: 29.892378\n",
      "Train Epoch: 150 [1280/8000 (16%)]\tLoss: 30.539679\n",
      "Train Epoch: 150 [2560/8000 (32%)]\tLoss: 30.210812\n",
      "Loss tensor(1952.8275, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(788.1963, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 150 [3840/8000 (48%)]\tLoss: 29.929457\n",
      "Train Epoch: 150 [5120/8000 (64%)]\tLoss: 29.997778\n",
      "Train Epoch: 150 [6400/8000 (80%)]\tLoss: 30.377636\n",
      "Train Epoch: 150 [7680/8000 (96%)]\tLoss: 30.319044\n",
      "====> Epoch: 150 Average loss: 30.2597\n",
      "Train Epoch: 151 [0/8000 (0%)]\tLoss: 30.277872\n",
      "Train Epoch: 151 [1280/8000 (16%)]\tLoss: 30.096237\n",
      "Train Epoch: 151 [2560/8000 (32%)]\tLoss: 30.318127\n",
      "Train Epoch: 151 [3840/8000 (48%)]\tLoss: 30.043907\n",
      "Train Epoch: 151 [5120/8000 (64%)]\tLoss: 30.019480\n",
      "Train Epoch: 151 [6400/8000 (80%)]\tLoss: 30.258646\n",
      "Train Epoch: 151 [7680/8000 (96%)]\tLoss: 30.394318\n",
      "====> Epoch: 151 Average loss: 30.2434\n",
      "Train Epoch: 152 [0/8000 (0%)]\tLoss: 30.086945\n",
      "Train Epoch: 152 [1280/8000 (16%)]\tLoss: 30.105381\n",
      "Train Epoch: 152 [2560/8000 (32%)]\tLoss: 30.626841\n",
      "Loss tensor(1932.8864, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(813.7677, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 152 [3840/8000 (48%)]\tLoss: 30.212784\n",
      "Loss tensor(1938.8572, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(814.3301, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 152 [5120/8000 (64%)]\tLoss: 29.942696\n",
      "Train Epoch: 152 [6400/8000 (80%)]\tLoss: 30.586252\n",
      "Train Epoch: 152 [7680/8000 (96%)]\tLoss: 30.206350\n",
      "====> Epoch: 152 Average loss: 30.2433\n",
      "Train Epoch: 153 [0/8000 (0%)]\tLoss: 30.326963\n",
      "Train Epoch: 153 [1280/8000 (16%)]\tLoss: 30.000944\n",
      "Loss tensor(1956.9293, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(815.3081, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 153 [2560/8000 (32%)]\tLoss: 30.185966\n",
      "Train Epoch: 153 [3840/8000 (48%)]\tLoss: 30.352983\n",
      "Train Epoch: 153 [5120/8000 (64%)]\tLoss: 30.044306\n",
      "Loss tensor(1931.1378, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(828.2993, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1897.1360, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(826.2031, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 153 [6400/8000 (80%)]\tLoss: 30.477983\n",
      "Train Epoch: 153 [7680/8000 (96%)]\tLoss: 29.981302\n",
      "Loss tensor(1940.5857, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(825.6947, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 153 Average loss: 30.2528\n",
      "Train Epoch: 154 [0/8000 (0%)]\tLoss: 30.066160\n",
      "Loss tensor(1975.7850, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(830.7146, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 154 [1280/8000 (16%)]\tLoss: 30.367147\n",
      "Train Epoch: 154 [2560/8000 (32%)]\tLoss: 30.414635\n",
      "Train Epoch: 154 [3840/8000 (48%)]\tLoss: 30.091408\n",
      "Train Epoch: 154 [5120/8000 (64%)]\tLoss: 30.365469\n",
      "Train Epoch: 154 [6400/8000 (80%)]\tLoss: 30.360849\n",
      "Train Epoch: 154 [7680/8000 (96%)]\tLoss: 30.243458\n",
      "====> Epoch: 154 Average loss: 30.2322\n",
      "Train Epoch: 155 [0/8000 (0%)]\tLoss: 30.337442\n",
      "Train Epoch: 155 [1280/8000 (16%)]\tLoss: 30.667635\n",
      "Train Epoch: 155 [2560/8000 (32%)]\tLoss: 30.219326\n",
      "Train Epoch: 155 [3840/8000 (48%)]\tLoss: 30.379578\n",
      "Train Epoch: 155 [5120/8000 (64%)]\tLoss: 30.420122\n",
      "Loss tensor(1912.1140, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(828.7774, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 155 [6400/8000 (80%)]\tLoss: 30.403994\n",
      "Loss tensor(1919.5388, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(829.3105, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 155 [7680/8000 (96%)]\tLoss: 30.422674\n",
      "====> Epoch: 155 Average loss: 30.2373\n",
      "Train Epoch: 156 [0/8000 (0%)]\tLoss: 30.109940\n",
      "Train Epoch: 156 [1280/8000 (16%)]\tLoss: 29.892838\n",
      "Train Epoch: 156 [2560/8000 (32%)]\tLoss: 30.108208\n",
      "Train Epoch: 156 [3840/8000 (48%)]\tLoss: 30.450098\n",
      "Train Epoch: 156 [5120/8000 (64%)]\tLoss: 30.365944\n",
      "Train Epoch: 156 [6400/8000 (80%)]\tLoss: 30.286152\n",
      "Train Epoch: 156 [7680/8000 (96%)]\tLoss: 30.143225\n",
      "====> Epoch: 156 Average loss: 30.2434\n",
      "Train Epoch: 157 [0/8000 (0%)]\tLoss: 30.052244\n",
      "Train Epoch: 157 [1280/8000 (16%)]\tLoss: 30.088406\n",
      "Loss tensor(1948.8516, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(847.7368, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 157 [2560/8000 (32%)]\tLoss: 30.135059\n",
      "Train Epoch: 157 [3840/8000 (48%)]\tLoss: 30.529764\n",
      "Train Epoch: 157 [5120/8000 (64%)]\tLoss: 30.183023\n",
      "Train Epoch: 157 [6400/8000 (80%)]\tLoss: 30.039478\n",
      "Loss tensor(1929.8124, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(843.5447, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 157 [7680/8000 (96%)]\tLoss: 30.557125\n",
      "====> Epoch: 157 Average loss: 30.2514\n",
      "Train Epoch: 158 [0/8000 (0%)]\tLoss: 30.185524\n",
      "Loss tensor(1936.0223, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(846.7104, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 158 [1280/8000 (16%)]\tLoss: 30.067423\n",
      "Loss tensor(1913.4628, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(848.5863, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 158 [2560/8000 (32%)]\tLoss: 30.171877\n",
      "Train Epoch: 158 [3840/8000 (48%)]\tLoss: 30.266678\n",
      "Loss tensor(1953.0537, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(851.8298, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 158 [5120/8000 (64%)]\tLoss: 30.349363\n",
      "Train Epoch: 158 [6400/8000 (80%)]\tLoss: 30.463165\n",
      "Train Epoch: 158 [7680/8000 (96%)]\tLoss: 30.244993\n",
      "====> Epoch: 158 Average loss: 30.2539\n",
      "Train Epoch: 159 [0/8000 (0%)]\tLoss: 30.130207\n",
      "Train Epoch: 159 [1280/8000 (16%)]\tLoss: 30.230631\n",
      "Train Epoch: 159 [2560/8000 (32%)]\tLoss: 30.188368\n",
      "Train Epoch: 159 [3840/8000 (48%)]\tLoss: 30.406767\n",
      "Train Epoch: 159 [5120/8000 (64%)]\tLoss: 30.617413\n",
      "Loss tensor(1936.7982, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(863.8264, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 159 [6400/8000 (80%)]\tLoss: 29.954617\n",
      "Train Epoch: 159 [7680/8000 (96%)]\tLoss: 30.019661\n",
      "====> Epoch: 159 Average loss: 30.2330\n",
      "Train Epoch: 160 [0/8000 (0%)]\tLoss: 30.350094\n",
      "Train Epoch: 160 [1280/8000 (16%)]\tLoss: 30.364693\n",
      "Train Epoch: 160 [2560/8000 (32%)]\tLoss: 30.372561\n",
      "Train Epoch: 160 [3840/8000 (48%)]\tLoss: 30.011124\n",
      "Train Epoch: 160 [5120/8000 (64%)]\tLoss: 29.953682\n",
      "Train Epoch: 160 [6400/8000 (80%)]\tLoss: 30.465544\n",
      "Train Epoch: 160 [7680/8000 (96%)]\tLoss: 30.434456\n",
      "====> Epoch: 160 Average loss: 30.2304\n",
      "Train Epoch: 161 [0/8000 (0%)]\tLoss: 30.286587\n",
      "Train Epoch: 161 [1280/8000 (16%)]\tLoss: 30.445427\n",
      "Train Epoch: 161 [2560/8000 (32%)]\tLoss: 30.434093\n",
      "Loss tensor(1920.3138, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(886.6606, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 161 [3840/8000 (48%)]\tLoss: 29.979673\n",
      "Train Epoch: 161 [5120/8000 (64%)]\tLoss: 30.163479\n",
      "Train Epoch: 161 [6400/8000 (80%)]\tLoss: 30.054754\n",
      "Train Epoch: 161 [7680/8000 (96%)]\tLoss: 30.270344\n",
      "====> Epoch: 161 Average loss: 30.2407\n",
      "Train Epoch: 162 [0/8000 (0%)]\tLoss: 29.984795\n",
      "Train Epoch: 162 [1280/8000 (16%)]\tLoss: 29.826298\n",
      "Train Epoch: 162 [2560/8000 (32%)]\tLoss: 30.101774\n",
      "Train Epoch: 162 [3840/8000 (48%)]\tLoss: 30.169031\n",
      "Train Epoch: 162 [5120/8000 (64%)]\tLoss: 30.378536\n",
      "Loss tensor(1947.9083, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(890.6346, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 162 [6400/8000 (80%)]\tLoss: 30.565557\n",
      "Train Epoch: 162 [7680/8000 (96%)]\tLoss: 30.004276\n",
      "====> Epoch: 162 Average loss: 30.2403\n",
      "Loss tensor(1930.1969, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(889.3299, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 163 [0/8000 (0%)]\tLoss: 30.159327\n",
      "Train Epoch: 163 [1280/8000 (16%)]\tLoss: 29.978790\n",
      "Loss tensor(1938.5602, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(887.2706, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 163 [2560/8000 (32%)]\tLoss: 29.872269\n",
      "Loss tensor(1941.2703, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(899.7646, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 163 [3840/8000 (48%)]\tLoss: 30.619692\n",
      "Train Epoch: 163 [5120/8000 (64%)]\tLoss: 29.935865\n",
      "Train Epoch: 163 [6400/8000 (80%)]\tLoss: 30.458591\n",
      "Loss tensor(1942.6326, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(906.1521, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 163 [7680/8000 (96%)]\tLoss: 30.266562\n",
      "====> Epoch: 163 Average loss: 30.2320\n",
      "Train Epoch: 164 [0/8000 (0%)]\tLoss: 30.204062\n",
      "Train Epoch: 164 [1280/8000 (16%)]\tLoss: 30.142086\n",
      "Train Epoch: 164 [2560/8000 (32%)]\tLoss: 30.444914\n",
      "Train Epoch: 164 [3840/8000 (48%)]\tLoss: 30.064056\n",
      "Train Epoch: 164 [5120/8000 (64%)]\tLoss: 30.039015\n",
      "Train Epoch: 164 [6400/8000 (80%)]\tLoss: 30.487152\n",
      "Train Epoch: 164 [7680/8000 (96%)]\tLoss: 30.269753\n",
      "====> Epoch: 164 Average loss: 30.2415\n",
      "Train Epoch: 165 [0/8000 (0%)]\tLoss: 30.235355\n",
      "Loss tensor(1910.8712, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(914.3403, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 165 [1280/8000 (16%)]\tLoss: 30.310740\n",
      "Train Epoch: 165 [2560/8000 (32%)]\tLoss: 30.304129\n",
      "Train Epoch: 165 [3840/8000 (48%)]\tLoss: 30.105040\n",
      "Train Epoch: 165 [5120/8000 (64%)]\tLoss: 30.026623\n",
      "Train Epoch: 165 [6400/8000 (80%)]\tLoss: 30.694187\n",
      "Loss tensor(1926.9393, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(921.7408, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 165 [7680/8000 (96%)]\tLoss: 30.277130\n",
      "====> Epoch: 165 Average loss: 30.2257\n",
      "Train Epoch: 166 [0/8000 (0%)]\tLoss: 30.082432\n",
      "Loss tensor(1947.2478, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(920.9111, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 166 [1280/8000 (16%)]\tLoss: 29.893744\n",
      "Train Epoch: 166 [2560/8000 (32%)]\tLoss: 29.780161\n",
      "Train Epoch: 166 [3840/8000 (48%)]\tLoss: 30.083261\n",
      "Train Epoch: 166 [5120/8000 (64%)]\tLoss: 30.261591\n",
      "Loss tensor(1927.3580, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(918.7951, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 166 [6400/8000 (80%)]\tLoss: 30.502468\n",
      "Train Epoch: 166 [7680/8000 (96%)]\tLoss: 30.246750\n",
      "====> Epoch: 166 Average loss: 30.2237\n",
      "Train Epoch: 167 [0/8000 (0%)]\tLoss: 30.052067\n",
      "Train Epoch: 167 [1280/8000 (16%)]\tLoss: 30.333021\n",
      "Train Epoch: 167 [2560/8000 (32%)]\tLoss: 30.090044\n",
      "Train Epoch: 167 [3840/8000 (48%)]\tLoss: 30.198906\n",
      "Train Epoch: 167 [5120/8000 (64%)]\tLoss: 30.502392\n",
      "Loss tensor(1926.0334, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(935.7244, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 167 [6400/8000 (80%)]\tLoss: 30.275286\n",
      "Train Epoch: 167 [7680/8000 (96%)]\tLoss: 30.496838\n",
      "====> Epoch: 167 Average loss: 30.2286\n",
      "Train Epoch: 168 [0/8000 (0%)]\tLoss: 30.047438\n",
      "Train Epoch: 168 [1280/8000 (16%)]\tLoss: 30.443752\n",
      "Train Epoch: 168 [2560/8000 (32%)]\tLoss: 30.009380\n",
      "Train Epoch: 168 [3840/8000 (48%)]\tLoss: 29.982643\n",
      "Train Epoch: 168 [5120/8000 (64%)]\tLoss: 30.325384\n",
      "Train Epoch: 168 [6400/8000 (80%)]\tLoss: 30.639208\n",
      "Train Epoch: 168 [7680/8000 (96%)]\tLoss: 30.630325\n",
      "Loss tensor(1933.2441, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(937.2191, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 168 Average loss: 30.2226\n",
      "Train Epoch: 169 [0/8000 (0%)]\tLoss: 30.347589\n",
      "Train Epoch: 169 [1280/8000 (16%)]\tLoss: 30.112892\n",
      "Loss tensor(1933.3053, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(940.1700, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 169 [2560/8000 (32%)]\tLoss: 30.377747\n",
      "Train Epoch: 169 [3840/8000 (48%)]\tLoss: 30.340664\n",
      "Loss tensor(1925.8629, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(943.8740, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 169 [5120/8000 (64%)]\tLoss: 30.769119\n",
      "Train Epoch: 169 [6400/8000 (80%)]\tLoss: 30.456081\n",
      "Train Epoch: 169 [7680/8000 (96%)]\tLoss: 30.450708\n",
      "====> Epoch: 169 Average loss: 30.2360\n",
      "Train Epoch: 170 [0/8000 (0%)]\tLoss: 30.324806\n",
      "Train Epoch: 170 [1280/8000 (16%)]\tLoss: 30.259684\n",
      "Train Epoch: 170 [2560/8000 (32%)]\tLoss: 30.247572\n",
      "Loss tensor(1936.7056, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(951.7892, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1942.4908, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(948.8091, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 170 [3840/8000 (48%)]\tLoss: 30.119377\n",
      "Train Epoch: 170 [5120/8000 (64%)]\tLoss: 30.078516\n",
      "Train Epoch: 170 [6400/8000 (80%)]\tLoss: 30.247704\n",
      "Train Epoch: 170 [7680/8000 (96%)]\tLoss: 30.205194\n",
      "====> Epoch: 170 Average loss: 30.2352\n",
      "Train Epoch: 171 [0/8000 (0%)]\tLoss: 30.203503\n",
      "Train Epoch: 171 [1280/8000 (16%)]\tLoss: 30.310871\n",
      "Train Epoch: 171 [2560/8000 (32%)]\tLoss: 30.263729\n",
      "Loss tensor(1910.9978, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(957.6890, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 171 [3840/8000 (48%)]\tLoss: 30.137499\n",
      "Loss tensor(1925.8464, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(958.1216, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 171 [5120/8000 (64%)]\tLoss: 30.271141\n",
      "Train Epoch: 171 [6400/8000 (80%)]\tLoss: 30.043404\n",
      "Train Epoch: 171 [7680/8000 (96%)]\tLoss: 30.219799\n",
      "====> Epoch: 171 Average loss: 30.2118\n",
      "Train Epoch: 172 [0/8000 (0%)]\tLoss: 30.147646\n",
      "Train Epoch: 172 [1280/8000 (16%)]\tLoss: 29.859018\n",
      "Train Epoch: 172 [2560/8000 (32%)]\tLoss: 30.306902\n",
      "Loss tensor(1933.0776, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(970.6613, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 172 [3840/8000 (48%)]\tLoss: 29.976797\n",
      "Loss tensor(1936.2385, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(971.7689, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 172 [5120/8000 (64%)]\tLoss: 30.262026\n",
      "Train Epoch: 172 [6400/8000 (80%)]\tLoss: 30.820019\n",
      "Train Epoch: 172 [7680/8000 (96%)]\tLoss: 30.347092\n",
      "====> Epoch: 172 Average loss: 30.2112\n",
      "Train Epoch: 173 [0/8000 (0%)]\tLoss: 30.591259\n",
      "Train Epoch: 173 [1280/8000 (16%)]\tLoss: 30.354603\n",
      "Train Epoch: 173 [2560/8000 (32%)]\tLoss: 30.317646\n",
      "Train Epoch: 173 [3840/8000 (48%)]\tLoss: 30.312901\n",
      "Loss tensor(1930.4932, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(965.8699, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 173 [5120/8000 (64%)]\tLoss: 30.282528\n",
      "Train Epoch: 173 [6400/8000 (80%)]\tLoss: 30.302784\n",
      "Loss tensor(1915.7531, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(967.3021, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 173 [7680/8000 (96%)]\tLoss: 30.219589\n",
      "====> Epoch: 173 Average loss: 30.2207\n",
      "Train Epoch: 174 [0/8000 (0%)]\tLoss: 30.064459\n",
      "Train Epoch: 174 [1280/8000 (16%)]\tLoss: 29.831436\n",
      "Loss tensor(1936.2571, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(963.7998, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 174 [2560/8000 (32%)]\tLoss: 30.259336\n",
      "Train Epoch: 174 [3840/8000 (48%)]\tLoss: 30.149000\n",
      "Train Epoch: 174 [5120/8000 (64%)]\tLoss: 30.415554\n",
      "Train Epoch: 174 [6400/8000 (80%)]\tLoss: 30.165386\n",
      "Loss tensor(1924.7894, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(972.8793, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 174 [7680/8000 (96%)]\tLoss: 29.991871\n",
      "====> Epoch: 174 Average loss: 30.2220\n",
      "Train Epoch: 175 [0/8000 (0%)]\tLoss: 30.254461\n",
      "Train Epoch: 175 [1280/8000 (16%)]\tLoss: 30.087490\n",
      "Train Epoch: 175 [2560/8000 (32%)]\tLoss: 29.985527\n",
      "Loss tensor(1934.7469, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(977.8808, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 175 [3840/8000 (48%)]\tLoss: 30.312212\n",
      "Train Epoch: 175 [5120/8000 (64%)]\tLoss: 30.099535\n",
      "Train Epoch: 175 [6400/8000 (80%)]\tLoss: 30.463568\n",
      "Train Epoch: 175 [7680/8000 (96%)]\tLoss: 30.211226\n",
      "====> Epoch: 175 Average loss: 30.2370\n",
      "Train Epoch: 176 [0/8000 (0%)]\tLoss: 30.213848\n",
      "Train Epoch: 176 [1280/8000 (16%)]\tLoss: 30.176449\n",
      "Train Epoch: 176 [2560/8000 (32%)]\tLoss: 30.185787\n",
      "Train Epoch: 176 [3840/8000 (48%)]\tLoss: 30.380703\n",
      "Loss tensor(1919.3652, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(974.7040, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 176 [5120/8000 (64%)]\tLoss: 30.209454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1956.0244, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(978.3880, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 176 [6400/8000 (80%)]\tLoss: 30.326506\n",
      "Train Epoch: 176 [7680/8000 (96%)]\tLoss: 30.384596\n",
      "====> Epoch: 176 Average loss: 30.2415\n",
      "Train Epoch: 177 [0/8000 (0%)]\tLoss: 30.497307\n",
      "Train Epoch: 177 [1280/8000 (16%)]\tLoss: 30.398623\n",
      "Train Epoch: 177 [2560/8000 (32%)]\tLoss: 30.511187\n",
      "Train Epoch: 177 [3840/8000 (48%)]\tLoss: 30.364887\n",
      "Train Epoch: 177 [5120/8000 (64%)]\tLoss: 30.205818\n",
      "Train Epoch: 177 [6400/8000 (80%)]\tLoss: 30.305574\n",
      "Train Epoch: 177 [7680/8000 (96%)]\tLoss: 29.939323\n",
      "====> Epoch: 177 Average loss: 30.2395\n",
      "Train Epoch: 178 [0/8000 (0%)]\tLoss: 30.341686\n",
      "Train Epoch: 178 [1280/8000 (16%)]\tLoss: 30.237795\n",
      "Loss tensor(1928.2671, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(984.5076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 178 [2560/8000 (32%)]\tLoss: 30.267153\n",
      "Train Epoch: 178 [3840/8000 (48%)]\tLoss: 30.336704\n",
      "Train Epoch: 178 [5120/8000 (64%)]\tLoss: 30.023727\n",
      "Train Epoch: 178 [6400/8000 (80%)]\tLoss: 30.532885\n",
      "Train Epoch: 178 [7680/8000 (96%)]\tLoss: 30.286713\n",
      "====> Epoch: 178 Average loss: 30.2330\n",
      "Train Epoch: 179 [0/8000 (0%)]\tLoss: 30.305132\n",
      "Train Epoch: 179 [1280/8000 (16%)]\tLoss: 30.009745\n",
      "Train Epoch: 179 [2560/8000 (32%)]\tLoss: 30.174000\n",
      "Train Epoch: 179 [3840/8000 (48%)]\tLoss: 29.984831\n",
      "Train Epoch: 179 [5120/8000 (64%)]\tLoss: 30.188095\n",
      "Train Epoch: 179 [6400/8000 (80%)]\tLoss: 30.146429\n",
      "Loss tensor(1932.1279, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(981.2888, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1920.6969, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(978.4238, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 179 [7680/8000 (96%)]\tLoss: 30.522673\n",
      "====> Epoch: 179 Average loss: 30.2354\n",
      "Train Epoch: 180 [0/8000 (0%)]\tLoss: 30.038925\n",
      "Train Epoch: 180 [1280/8000 (16%)]\tLoss: 30.438934\n",
      "Train Epoch: 180 [2560/8000 (32%)]\tLoss: 30.334879\n",
      "Train Epoch: 180 [3840/8000 (48%)]\tLoss: 30.430582\n",
      "Train Epoch: 180 [5120/8000 (64%)]\tLoss: 30.354239\n",
      "Train Epoch: 180 [6400/8000 (80%)]\tLoss: 29.757685\n",
      "Train Epoch: 180 [7680/8000 (96%)]\tLoss: 30.455910\n",
      "====> Epoch: 180 Average loss: 30.2289\n",
      "Train Epoch: 181 [0/8000 (0%)]\tLoss: 29.944067\n",
      "Train Epoch: 181 [1280/8000 (16%)]\tLoss: 30.317354\n",
      "Train Epoch: 181 [2560/8000 (32%)]\tLoss: 30.393383\n",
      "Train Epoch: 181 [3840/8000 (48%)]\tLoss: 30.239511\n",
      "Loss tensor(1911.7250, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(978.4319, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 181 [5120/8000 (64%)]\tLoss: 29.835464\n",
      "Train Epoch: 181 [6400/8000 (80%)]\tLoss: 30.235872\n",
      "Train Epoch: 181 [7680/8000 (96%)]\tLoss: 30.481636\n",
      "====> Epoch: 181 Average loss: 30.2138\n",
      "Train Epoch: 182 [0/8000 (0%)]\tLoss: 30.013577\n",
      "Train Epoch: 182 [1280/8000 (16%)]\tLoss: 30.317749\n",
      "Train Epoch: 182 [2560/8000 (32%)]\tLoss: 29.964617\n",
      "Train Epoch: 182 [3840/8000 (48%)]\tLoss: 30.205137\n",
      "Train Epoch: 182 [5120/8000 (64%)]\tLoss: 30.204937\n",
      "Train Epoch: 182 [6400/8000 (80%)]\tLoss: 30.160128\n",
      "Train Epoch: 182 [7680/8000 (96%)]\tLoss: 29.972567\n",
      "====> Epoch: 182 Average loss: 30.2369\n",
      "Train Epoch: 183 [0/8000 (0%)]\tLoss: 30.235718\n",
      "Train Epoch: 183 [1280/8000 (16%)]\tLoss: 29.864113\n",
      "Loss tensor(1928.3148, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(964.5913, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1957.8275, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(972.2525, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 183 [2560/8000 (32%)]\tLoss: 30.335157\n",
      "Train Epoch: 183 [3840/8000 (48%)]\tLoss: 30.686510\n",
      "Loss tensor(1924.0812, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(969.9497, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 183 [5120/8000 (64%)]\tLoss: 30.317238\n",
      "Train Epoch: 183 [6400/8000 (80%)]\tLoss: 30.265972\n",
      "Train Epoch: 183 [7680/8000 (96%)]\tLoss: 30.131226\n",
      "====> Epoch: 183 Average loss: 30.2146\n",
      "Train Epoch: 184 [0/8000 (0%)]\tLoss: 30.011065\n",
      "Loss tensor(1936.5635, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(962.1796, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 184 [1280/8000 (16%)]\tLoss: 30.394463\n",
      "Train Epoch: 184 [2560/8000 (32%)]\tLoss: 30.218035\n",
      "Train Epoch: 184 [3840/8000 (48%)]\tLoss: 29.840553\n",
      "Train Epoch: 184 [5120/8000 (64%)]\tLoss: 30.634600\n",
      "Train Epoch: 184 [6400/8000 (80%)]\tLoss: 29.838491\n",
      "Train Epoch: 184 [7680/8000 (96%)]\tLoss: 30.123880\n",
      "====> Epoch: 184 Average loss: 30.2262\n",
      "Train Epoch: 185 [0/8000 (0%)]\tLoss: 30.176878\n",
      "Train Epoch: 185 [1280/8000 (16%)]\tLoss: 29.821203\n",
      "Train Epoch: 185 [2560/8000 (32%)]\tLoss: 30.264347\n",
      "Loss tensor(1921.9271, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(967.2027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 185 [3840/8000 (48%)]\tLoss: 30.070475\n",
      "Train Epoch: 185 [5120/8000 (64%)]\tLoss: 30.510580\n",
      "Loss tensor(1934.1215, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(968.2551, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 185 [6400/8000 (80%)]\tLoss: 30.107046\n",
      "Train Epoch: 185 [7680/8000 (96%)]\tLoss: 30.367390\n",
      "====> Epoch: 185 Average loss: 30.2195\n",
      "Train Epoch: 186 [0/8000 (0%)]\tLoss: 30.451992\n",
      "Train Epoch: 186 [1280/8000 (16%)]\tLoss: 30.121111\n",
      "Train Epoch: 186 [2560/8000 (32%)]\tLoss: 29.981890\n",
      "Train Epoch: 186 [3840/8000 (48%)]\tLoss: 30.260941\n",
      "Train Epoch: 186 [5120/8000 (64%)]\tLoss: 30.216032\n",
      "Train Epoch: 186 [6400/8000 (80%)]\tLoss: 30.261658\n",
      "Train Epoch: 186 [7680/8000 (96%)]\tLoss: 30.171961\n",
      "Loss tensor(1941.6132, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(949.6221, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 186 Average loss: 30.2072\n",
      "Train Epoch: 187 [0/8000 (0%)]\tLoss: 30.815311\n",
      "Train Epoch: 187 [1280/8000 (16%)]\tLoss: 30.584869\n",
      "Train Epoch: 187 [2560/8000 (32%)]\tLoss: 29.901051\n",
      "Train Epoch: 187 [3840/8000 (48%)]\tLoss: 30.697199\n",
      "Train Epoch: 187 [5120/8000 (64%)]\tLoss: 30.120823\n",
      "Train Epoch: 187 [6400/8000 (80%)]\tLoss: 29.812653\n",
      "Train Epoch: 187 [7680/8000 (96%)]\tLoss: 30.023413\n",
      "====> Epoch: 187 Average loss: 30.2390\n",
      "Train Epoch: 188 [0/8000 (0%)]\tLoss: 30.589811\n",
      "Train Epoch: 188 [1280/8000 (16%)]\tLoss: 30.118774\n",
      "Loss tensor(1943.0677, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(951.3325, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 188 [2560/8000 (32%)]\tLoss: 30.240026\n",
      "Train Epoch: 188 [3840/8000 (48%)]\tLoss: 30.504528\n",
      "Train Epoch: 188 [5120/8000 (64%)]\tLoss: 30.082314\n",
      "Train Epoch: 188 [6400/8000 (80%)]\tLoss: 29.992319\n",
      "Train Epoch: 188 [7680/8000 (96%)]\tLoss: 30.161160\n",
      "====> Epoch: 188 Average loss: 30.2258\n",
      "Train Epoch: 189 [0/8000 (0%)]\tLoss: 30.572041\n",
      "Train Epoch: 189 [1280/8000 (16%)]\tLoss: 30.305309\n",
      "Train Epoch: 189 [2560/8000 (32%)]\tLoss: 30.206327\n",
      "Train Epoch: 189 [3840/8000 (48%)]\tLoss: 30.223537\n",
      "Train Epoch: 189 [5120/8000 (64%)]\tLoss: 29.952751\n",
      "Train Epoch: 189 [6400/8000 (80%)]\tLoss: 30.056505\n",
      "Train Epoch: 189 [7680/8000 (96%)]\tLoss: 30.072676\n",
      "====> Epoch: 189 Average loss: 30.2228\n",
      "Train Epoch: 190 [0/8000 (0%)]\tLoss: 30.288155\n",
      "Train Epoch: 190 [1280/8000 (16%)]\tLoss: 29.965544\n",
      "Train Epoch: 190 [2560/8000 (32%)]\tLoss: 30.435585\n",
      "Train Epoch: 190 [3840/8000 (48%)]\tLoss: 30.419037\n",
      "Train Epoch: 190 [5120/8000 (64%)]\tLoss: 30.063860\n",
      "Train Epoch: 190 [6400/8000 (80%)]\tLoss: 30.259212\n",
      "Train Epoch: 190 [7680/8000 (96%)]\tLoss: 29.986290\n",
      "====> Epoch: 190 Average loss: 30.2384\n",
      "Train Epoch: 191 [0/8000 (0%)]\tLoss: 30.159466\n",
      "Loss tensor(1921.5065, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(946.5294, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 191 [1280/8000 (16%)]\tLoss: 30.320681\n",
      "Train Epoch: 191 [2560/8000 (32%)]\tLoss: 30.155071\n",
      "Train Epoch: 191 [3840/8000 (48%)]\tLoss: 29.982616\n",
      "Train Epoch: 191 [5120/8000 (64%)]\tLoss: 30.310944\n",
      "Train Epoch: 191 [6400/8000 (80%)]\tLoss: 30.255754\n",
      "Train Epoch: 191 [7680/8000 (96%)]\tLoss: 30.104795\n",
      "====> Epoch: 191 Average loss: 30.2430\n",
      "Train Epoch: 192 [0/8000 (0%)]\tLoss: 29.993948\n",
      "Train Epoch: 192 [1280/8000 (16%)]\tLoss: 30.238594\n",
      "Train Epoch: 192 [2560/8000 (32%)]\tLoss: 30.096193\n",
      "Loss tensor(1931.4261, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(940.5854, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 192 [3840/8000 (48%)]\tLoss: 30.157478\n",
      "Loss tensor(1932.8217, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(940.3632, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 192 [5120/8000 (64%)]\tLoss: 30.426603\n",
      "Loss tensor(1927.6677, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(937.8827, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 192 [6400/8000 (80%)]\tLoss: 30.597786\n",
      "Train Epoch: 192 [7680/8000 (96%)]\tLoss: 30.182104\n",
      "====> Epoch: 192 Average loss: 30.2187\n",
      "Train Epoch: 193 [0/8000 (0%)]\tLoss: 30.199282\n",
      "Train Epoch: 193 [1280/8000 (16%)]\tLoss: 30.182293\n",
      "Train Epoch: 193 [2560/8000 (32%)]\tLoss: 30.330584\n",
      "Train Epoch: 193 [3840/8000 (48%)]\tLoss: 30.441896\n",
      "Train Epoch: 193 [5120/8000 (64%)]\tLoss: 30.153681\n",
      "Train Epoch: 193 [6400/8000 (80%)]\tLoss: 30.229630\n",
      "Train Epoch: 193 [7680/8000 (96%)]\tLoss: 30.312428\n",
      "====> Epoch: 193 Average loss: 30.2177\n",
      "Train Epoch: 194 [0/8000 (0%)]\tLoss: 30.354300\n",
      "Train Epoch: 194 [1280/8000 (16%)]\tLoss: 29.866707\n",
      "Train Epoch: 194 [2560/8000 (32%)]\tLoss: 30.065908\n",
      "Loss tensor(1940.7588, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(924.9858, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 194 [3840/8000 (48%)]\tLoss: 30.398048\n",
      "Train Epoch: 194 [5120/8000 (64%)]\tLoss: 30.307459\n",
      "Train Epoch: 194 [6400/8000 (80%)]\tLoss: 30.170813\n",
      "Train Epoch: 194 [7680/8000 (96%)]\tLoss: 30.003643\n",
      "====> Epoch: 194 Average loss: 30.2316\n",
      "Train Epoch: 195 [0/8000 (0%)]\tLoss: 30.504986\n",
      "Train Epoch: 195 [1280/8000 (16%)]\tLoss: 30.317080\n",
      "Loss tensor(1939.0575, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(924.5215, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 195 [2560/8000 (32%)]\tLoss: 30.306734\n",
      "Train Epoch: 195 [3840/8000 (48%)]\tLoss: 30.239529\n",
      "Loss tensor(1949.4648, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(917.2788, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 195 [5120/8000 (64%)]\tLoss: 30.177923\n",
      "Loss tensor(1915.2615, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(922.7697, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 195 [6400/8000 (80%)]\tLoss: 30.492111\n",
      "Train Epoch: 195 [7680/8000 (96%)]\tLoss: 30.085537\n",
      "====> Epoch: 195 Average loss: 30.2203\n",
      "Train Epoch: 196 [0/8000 (0%)]\tLoss: 30.443848\n",
      "Train Epoch: 196 [1280/8000 (16%)]\tLoss: 30.058180\n",
      "Train Epoch: 196 [2560/8000 (32%)]\tLoss: 30.179350\n",
      "Train Epoch: 196 [3840/8000 (48%)]\tLoss: 29.721611\n",
      "Train Epoch: 196 [5120/8000 (64%)]\tLoss: 29.846003\n",
      "Loss tensor(1919.9690, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(914.5994, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 196 [6400/8000 (80%)]\tLoss: 29.860706\n",
      "Train Epoch: 196 [7680/8000 (96%)]\tLoss: 30.207123\n",
      "====> Epoch: 196 Average loss: 30.2058\n",
      "Train Epoch: 197 [0/8000 (0%)]\tLoss: 30.031775\n",
      "Train Epoch: 197 [1280/8000 (16%)]\tLoss: 29.987431\n",
      "Train Epoch: 197 [2560/8000 (32%)]\tLoss: 30.291893\n",
      "Loss tensor(1950.8402, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(920.1815, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 197 [3840/8000 (48%)]\tLoss: 30.149700\n",
      "Train Epoch: 197 [5120/8000 (64%)]\tLoss: 30.297756\n",
      "Train Epoch: 197 [6400/8000 (80%)]\tLoss: 30.231125\n",
      "Train Epoch: 197 [7680/8000 (96%)]\tLoss: 30.063065\n",
      "====> Epoch: 197 Average loss: 30.2366\n",
      "Train Epoch: 198 [0/8000 (0%)]\tLoss: 30.125036\n",
      "Loss tensor(1952.0117, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(922.9893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1934.1987, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(929.3754, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 198 [1280/8000 (16%)]\tLoss: 30.221855\n",
      "Train Epoch: 198 [2560/8000 (32%)]\tLoss: 30.501556\n",
      "Train Epoch: 198 [3840/8000 (48%)]\tLoss: 30.005194\n",
      "Train Epoch: 198 [5120/8000 (64%)]\tLoss: 30.088350\n",
      "Train Epoch: 198 [6400/8000 (80%)]\tLoss: 30.614988\n",
      "Train Epoch: 198 [7680/8000 (96%)]\tLoss: 29.938721\n",
      "====> Epoch: 198 Average loss: 30.2273\n",
      "Train Epoch: 199 [0/8000 (0%)]\tLoss: 30.303558\n",
      "Train Epoch: 199 [1280/8000 (16%)]\tLoss: 30.461214\n",
      "Train Epoch: 199 [2560/8000 (32%)]\tLoss: 30.120922\n",
      "Train Epoch: 199 [3840/8000 (48%)]\tLoss: 30.452520\n",
      "Loss tensor(1935.7015, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(923.5970, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 199 [5120/8000 (64%)]\tLoss: 30.101885\n",
      "Train Epoch: 199 [6400/8000 (80%)]\tLoss: 30.027428\n",
      "Train Epoch: 199 [7680/8000 (96%)]\tLoss: 30.167807\n",
      "====> Epoch: 199 Average loss: 30.2474\n",
      "Train Epoch: 200 [0/8000 (0%)]\tLoss: 30.352076\n",
      "Train Epoch: 200 [1280/8000 (16%)]\tLoss: 30.586929\n",
      "Train Epoch: 200 [2560/8000 (32%)]\tLoss: 30.213760\n",
      "Train Epoch: 200 [3840/8000 (48%)]\tLoss: 30.609447\n",
      "Loss tensor(1919.7133, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(922.9464, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 200 [5120/8000 (64%)]\tLoss: 30.394253\n",
      "Loss tensor(1938.0450, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(926.2513, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1931.5494, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(923.7107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 200 [6400/8000 (80%)]\tLoss: 30.658463\n",
      "Train Epoch: 200 [7680/8000 (96%)]\tLoss: 29.917406\n",
      "====> Epoch: 200 Average loss: 30.2265\n",
      "Train Epoch: 201 [0/8000 (0%)]\tLoss: 29.991283\n",
      "Train Epoch: 201 [1280/8000 (16%)]\tLoss: 30.117987\n",
      "Train Epoch: 201 [2560/8000 (32%)]\tLoss: 30.358232\n",
      "Loss tensor(1926.9783, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(918.7651, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 201 [3840/8000 (48%)]\tLoss: 29.955738\n",
      "Train Epoch: 201 [5120/8000 (64%)]\tLoss: 30.269682\n",
      "Train Epoch: 201 [6400/8000 (80%)]\tLoss: 30.430138\n",
      "Train Epoch: 201 [7680/8000 (96%)]\tLoss: 30.152027\n",
      "====> Epoch: 201 Average loss: 30.1969\n",
      "Train Epoch: 202 [0/8000 (0%)]\tLoss: 30.241585\n",
      "Train Epoch: 202 [1280/8000 (16%)]\tLoss: 30.084538\n",
      "Train Epoch: 202 [2560/8000 (32%)]\tLoss: 30.008270\n",
      "Train Epoch: 202 [3840/8000 (48%)]\tLoss: 30.645975\n",
      "Loss tensor(1918.0388, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(914.0164, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 202 [5120/8000 (64%)]\tLoss: 30.440207\n",
      "Train Epoch: 202 [6400/8000 (80%)]\tLoss: 30.060291\n",
      "Train Epoch: 202 [7680/8000 (96%)]\tLoss: 30.204439\n",
      "====> Epoch: 202 Average loss: 30.2135\n",
      "Train Epoch: 203 [0/8000 (0%)]\tLoss: 30.068424\n",
      "Train Epoch: 203 [1280/8000 (16%)]\tLoss: 30.402983\n",
      "Loss tensor(1912.6833, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(914.6616, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1954.4193, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(917.3859, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 203 [2560/8000 (32%)]\tLoss: 30.099939\n",
      "Train Epoch: 203 [3840/8000 (48%)]\tLoss: 30.461134\n",
      "Train Epoch: 203 [5120/8000 (64%)]\tLoss: 30.117926\n",
      "Train Epoch: 203 [6400/8000 (80%)]\tLoss: 30.280016\n",
      "Train Epoch: 203 [7680/8000 (96%)]\tLoss: 30.215370\n",
      "====> Epoch: 203 Average loss: 30.2415\n",
      "Train Epoch: 204 [0/8000 (0%)]\tLoss: 30.361423\n",
      "Train Epoch: 204 [1280/8000 (16%)]\tLoss: 29.916672\n",
      "Loss tensor(1920.4651, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(901.7345, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 204 [2560/8000 (32%)]\tLoss: 30.031649\n",
      "Loss tensor(1928.3809, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(906.1677, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 204 [3840/8000 (48%)]\tLoss: 30.195831\n",
      "Loss tensor(1931.2555, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(915.3961, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 204 [5120/8000 (64%)]\tLoss: 30.472588\n",
      "Train Epoch: 204 [6400/8000 (80%)]\tLoss: 30.144331\n",
      "Loss tensor(1946.8733, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(910.8142, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 204 [7680/8000 (96%)]\tLoss: 30.055828\n",
      "====> Epoch: 204 Average loss: 30.2304\n",
      "Loss tensor(1927.7664, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(904.1126, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 205 [0/8000 (0%)]\tLoss: 30.121349\n",
      "Train Epoch: 205 [1280/8000 (16%)]\tLoss: 30.215111\n",
      "Loss tensor(1921.7960, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(903.7485, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 205 [2560/8000 (32%)]\tLoss: 30.141235\n",
      "Train Epoch: 205 [3840/8000 (48%)]\tLoss: 30.751143\n",
      "Train Epoch: 205 [5120/8000 (64%)]\tLoss: 30.122019\n",
      "Loss tensor(1941.4021, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(900.5050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 205 [6400/8000 (80%)]\tLoss: 30.334408\n",
      "Train Epoch: 205 [7680/8000 (96%)]\tLoss: 30.477596\n",
      "====> Epoch: 205 Average loss: 30.2369\n",
      "Train Epoch: 206 [0/8000 (0%)]\tLoss: 30.171154\n",
      "Loss tensor(1939.4956, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(893.2966, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1926.1981, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(894.2764, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 206 [1280/8000 (16%)]\tLoss: 29.983736\n",
      "Loss tensor(1931.1654, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(896.6805, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 206 [2560/8000 (32%)]\tLoss: 30.199753\n",
      "Train Epoch: 206 [3840/8000 (48%)]\tLoss: 29.966976\n",
      "Train Epoch: 206 [5120/8000 (64%)]\tLoss: 30.099329\n",
      "Train Epoch: 206 [6400/8000 (80%)]\tLoss: 30.337442\n",
      "Train Epoch: 206 [7680/8000 (96%)]\tLoss: 30.334511\n",
      "====> Epoch: 206 Average loss: 30.2346\n",
      "Train Epoch: 207 [0/8000 (0%)]\tLoss: 30.236963\n",
      "Train Epoch: 207 [1280/8000 (16%)]\tLoss: 30.044157\n",
      "Train Epoch: 207 [2560/8000 (32%)]\tLoss: 30.312346\n",
      "Train Epoch: 207 [3840/8000 (48%)]\tLoss: 30.379551\n",
      "Loss tensor(1920.7786, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(897.1268, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 207 [5120/8000 (64%)]\tLoss: 29.832306\n",
      "Train Epoch: 207 [6400/8000 (80%)]\tLoss: 29.868490\n",
      "Train Epoch: 207 [7680/8000 (96%)]\tLoss: 30.043007\n",
      "====> Epoch: 207 Average loss: 30.2139\n",
      "Train Epoch: 208 [0/8000 (0%)]\tLoss: 30.237850\n",
      "Train Epoch: 208 [1280/8000 (16%)]\tLoss: 30.262913\n",
      "Train Epoch: 208 [2560/8000 (32%)]\tLoss: 30.075548\n",
      "Train Epoch: 208 [3840/8000 (48%)]\tLoss: 29.967529\n",
      "Train Epoch: 208 [5120/8000 (64%)]\tLoss: 30.364878\n",
      "Train Epoch: 208 [6400/8000 (80%)]\tLoss: 30.419588\n",
      "Train Epoch: 208 [7680/8000 (96%)]\tLoss: 30.461245\n",
      "====> Epoch: 208 Average loss: 30.2310\n",
      "Train Epoch: 209 [0/8000 (0%)]\tLoss: 30.286758\n",
      "Train Epoch: 209 [1280/8000 (16%)]\tLoss: 30.092663\n",
      "Train Epoch: 209 [2560/8000 (32%)]\tLoss: 30.205956\n",
      "Train Epoch: 209 [3840/8000 (48%)]\tLoss: 30.335562\n",
      "Train Epoch: 209 [5120/8000 (64%)]\tLoss: 29.976572\n",
      "Train Epoch: 209 [6400/8000 (80%)]\tLoss: 30.243715\n",
      "Train Epoch: 209 [7680/8000 (96%)]\tLoss: 30.398695\n",
      "====> Epoch: 209 Average loss: 30.2270\n",
      "Train Epoch: 210 [0/8000 (0%)]\tLoss: 30.443405\n",
      "Train Epoch: 210 [1280/8000 (16%)]\tLoss: 30.124920\n",
      "Train Epoch: 210 [2560/8000 (32%)]\tLoss: 30.470234\n",
      "Train Epoch: 210 [3840/8000 (48%)]\tLoss: 30.452400\n",
      "Loss tensor(1931.0514, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(884.3610, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 210 [5120/8000 (64%)]\tLoss: 30.350182\n",
      "Train Epoch: 210 [6400/8000 (80%)]\tLoss: 30.074869\n",
      "Train Epoch: 210 [7680/8000 (96%)]\tLoss: 30.396254\n",
      "====> Epoch: 210 Average loss: 30.2540\n",
      "Train Epoch: 211 [0/8000 (0%)]\tLoss: 30.276455\n",
      "Train Epoch: 211 [1280/8000 (16%)]\tLoss: 30.523932\n",
      "Loss tensor(1898.5164, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(874.6973, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 211 [2560/8000 (32%)]\tLoss: 30.187946\n",
      "Train Epoch: 211 [3840/8000 (48%)]\tLoss: 30.384331\n",
      "Train Epoch: 211 [5120/8000 (64%)]\tLoss: 30.379217\n",
      "Train Epoch: 211 [6400/8000 (80%)]\tLoss: 30.303370\n",
      "Train Epoch: 211 [7680/8000 (96%)]\tLoss: 30.150284\n",
      "====> Epoch: 211 Average loss: 30.2161\n",
      "Train Epoch: 212 [0/8000 (0%)]\tLoss: 29.982147\n",
      "Train Epoch: 212 [1280/8000 (16%)]\tLoss: 30.041845\n",
      "Train Epoch: 212 [2560/8000 (32%)]\tLoss: 30.096300\n",
      "Train Epoch: 212 [3840/8000 (48%)]\tLoss: 30.288385\n",
      "Loss tensor(1907.3634, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(870.0087, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 212 [5120/8000 (64%)]\tLoss: 30.026997\n",
      "Train Epoch: 212 [6400/8000 (80%)]\tLoss: 30.297134\n",
      "Train Epoch: 212 [7680/8000 (96%)]\tLoss: 30.068089\n",
      "====> Epoch: 212 Average loss: 30.2403\n",
      "Train Epoch: 213 [0/8000 (0%)]\tLoss: 29.942467\n",
      "Train Epoch: 213 [1280/8000 (16%)]\tLoss: 29.811899\n",
      "Train Epoch: 213 [2560/8000 (32%)]\tLoss: 30.465387\n",
      "Train Epoch: 213 [3840/8000 (48%)]\tLoss: 30.469559\n",
      "Train Epoch: 213 [5120/8000 (64%)]\tLoss: 29.960371\n",
      "Train Epoch: 213 [6400/8000 (80%)]\tLoss: 30.191841\n",
      "Train Epoch: 213 [7680/8000 (96%)]\tLoss: 30.274366\n",
      "====> Epoch: 213 Average loss: 30.2330\n",
      "Train Epoch: 214 [0/8000 (0%)]\tLoss: 30.187330\n",
      "Train Epoch: 214 [1280/8000 (16%)]\tLoss: 29.972214\n",
      "Train Epoch: 214 [2560/8000 (32%)]\tLoss: 30.378822\n",
      "Train Epoch: 214 [3840/8000 (48%)]\tLoss: 30.159409\n",
      "Train Epoch: 214 [5120/8000 (64%)]\tLoss: 30.096836\n",
      "Train Epoch: 214 [6400/8000 (80%)]\tLoss: 30.345181\n",
      "Loss tensor(1909.0345, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(859.3814, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 214 [7680/8000 (96%)]\tLoss: 30.209955\n",
      "====> Epoch: 214 Average loss: 30.2230\n",
      "Train Epoch: 215 [0/8000 (0%)]\tLoss: 30.542542\n",
      "Train Epoch: 215 [1280/8000 (16%)]\tLoss: 30.129000\n",
      "Train Epoch: 215 [2560/8000 (32%)]\tLoss: 30.221491\n",
      "Train Epoch: 215 [3840/8000 (48%)]\tLoss: 30.260998\n",
      "Train Epoch: 215 [5120/8000 (64%)]\tLoss: 30.211344\n",
      "Loss tensor(1912.5088, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(853.8893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 215 [6400/8000 (80%)]\tLoss: 30.222404\n",
      "Loss tensor(1935.1846, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(849.2621, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 215 [7680/8000 (96%)]\tLoss: 30.057304\n",
      "====> Epoch: 215 Average loss: 30.2344\n",
      "Train Epoch: 216 [0/8000 (0%)]\tLoss: 30.253742\n",
      "Train Epoch: 216 [1280/8000 (16%)]\tLoss: 30.063200\n",
      "Train Epoch: 216 [2560/8000 (32%)]\tLoss: 30.291651\n",
      "Train Epoch: 216 [3840/8000 (48%)]\tLoss: 30.033730\n",
      "Loss tensor(1923.4963, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(852.3498, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 216 [5120/8000 (64%)]\tLoss: 30.458010\n",
      "Train Epoch: 216 [6400/8000 (80%)]\tLoss: 30.597792\n",
      "Train Epoch: 216 [7680/8000 (96%)]\tLoss: 30.605593\n",
      "====> Epoch: 216 Average loss: 30.2212\n",
      "Train Epoch: 217 [0/8000 (0%)]\tLoss: 29.917427\n",
      "Train Epoch: 217 [1280/8000 (16%)]\tLoss: 29.863504\n",
      "Train Epoch: 217 [2560/8000 (32%)]\tLoss: 30.308037\n",
      "Train Epoch: 217 [3840/8000 (48%)]\tLoss: 30.173279\n",
      "Loss tensor(1945.4246, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(859.6754, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 217 [5120/8000 (64%)]\tLoss: 30.198706\n",
      "Train Epoch: 217 [6400/8000 (80%)]\tLoss: 30.292822\n",
      "Train Epoch: 217 [7680/8000 (96%)]\tLoss: 30.479015\n",
      "====> Epoch: 217 Average loss: 30.2365\n",
      "Train Epoch: 218 [0/8000 (0%)]\tLoss: 30.188030\n",
      "Train Epoch: 218 [1280/8000 (16%)]\tLoss: 30.350698\n",
      "Train Epoch: 218 [2560/8000 (32%)]\tLoss: 30.438356\n",
      "Loss tensor(1954.3905, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(864.5990, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 218 [3840/8000 (48%)]\tLoss: 30.307652\n",
      "Train Epoch: 218 [5120/8000 (64%)]\tLoss: 30.229116\n",
      "Train Epoch: 218 [6400/8000 (80%)]\tLoss: 30.071402\n",
      "Loss tensor(1913.3737, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(856.7029, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 218 [7680/8000 (96%)]\tLoss: 30.166370\n",
      "====> Epoch: 218 Average loss: 30.2243\n",
      "Train Epoch: 219 [0/8000 (0%)]\tLoss: 30.534491\n",
      "Train Epoch: 219 [1280/8000 (16%)]\tLoss: 30.227598\n",
      "Train Epoch: 219 [2560/8000 (32%)]\tLoss: 29.880089\n",
      "Loss tensor(1938.1169, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(851.8882, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 219 [3840/8000 (48%)]\tLoss: 29.831013\n",
      "Train Epoch: 219 [5120/8000 (64%)]\tLoss: 30.315405\n",
      "Train Epoch: 219 [6400/8000 (80%)]\tLoss: 30.264662\n",
      "Train Epoch: 219 [7680/8000 (96%)]\tLoss: 30.578648\n",
      "====> Epoch: 219 Average loss: 30.2253\n",
      "Train Epoch: 220 [0/8000 (0%)]\tLoss: 30.299603\n",
      "Train Epoch: 220 [1280/8000 (16%)]\tLoss: 29.995094\n",
      "Train Epoch: 220 [2560/8000 (32%)]\tLoss: 30.108768\n",
      "Train Epoch: 220 [3840/8000 (48%)]\tLoss: 30.343058\n",
      "Train Epoch: 220 [5120/8000 (64%)]\tLoss: 29.982851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 220 [6400/8000 (80%)]\tLoss: 30.437637\n",
      "Train Epoch: 220 [7680/8000 (96%)]\tLoss: 30.389069\n",
      "====> Epoch: 220 Average loss: 30.2125\n",
      "Train Epoch: 221 [0/8000 (0%)]\tLoss: 30.112547\n",
      "Loss tensor(1926.1881, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(849.7743, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 221 [1280/8000 (16%)]\tLoss: 30.093647\n",
      "Train Epoch: 221 [2560/8000 (32%)]\tLoss: 30.263144\n",
      "Train Epoch: 221 [3840/8000 (48%)]\tLoss: 30.293217\n",
      "Loss tensor(1942.7620, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(846.6974, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 221 [5120/8000 (64%)]\tLoss: 30.279497\n",
      "Train Epoch: 221 [6400/8000 (80%)]\tLoss: 30.602644\n",
      "Train Epoch: 221 [7680/8000 (96%)]\tLoss: 30.262640\n",
      "====> Epoch: 221 Average loss: 30.2246\n",
      "Train Epoch: 222 [0/8000 (0%)]\tLoss: 30.013979\n",
      "Train Epoch: 222 [1280/8000 (16%)]\tLoss: 30.156506\n",
      "Train Epoch: 222 [2560/8000 (32%)]\tLoss: 30.275339\n",
      "Train Epoch: 222 [3840/8000 (48%)]\tLoss: 30.343925\n",
      "Train Epoch: 222 [5120/8000 (64%)]\tLoss: 30.334019\n",
      "Train Epoch: 222 [6400/8000 (80%)]\tLoss: 30.239866\n",
      "Train Epoch: 222 [7680/8000 (96%)]\tLoss: 30.131672\n",
      "====> Epoch: 222 Average loss: 30.2188\n",
      "Train Epoch: 223 [0/8000 (0%)]\tLoss: 29.953075\n",
      "Train Epoch: 223 [1280/8000 (16%)]\tLoss: 30.219925\n",
      "Train Epoch: 223 [2560/8000 (32%)]\tLoss: 30.291212\n",
      "Train Epoch: 223 [3840/8000 (48%)]\tLoss: 30.107109\n",
      "Train Epoch: 223 [5120/8000 (64%)]\tLoss: 30.631111\n",
      "Train Epoch: 223 [6400/8000 (80%)]\tLoss: 30.306793\n",
      "Loss tensor(1928.0221, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(857.3837, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 223 [7680/8000 (96%)]\tLoss: 30.067156\n",
      "====> Epoch: 223 Average loss: 30.2229\n",
      "Train Epoch: 224 [0/8000 (0%)]\tLoss: 30.096081\n",
      "Train Epoch: 224 [1280/8000 (16%)]\tLoss: 30.417185\n",
      "Train Epoch: 224 [2560/8000 (32%)]\tLoss: 29.859375\n",
      "Loss tensor(1956.7362, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(847.2070, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 224 [3840/8000 (48%)]\tLoss: 30.539351\n",
      "Train Epoch: 224 [5120/8000 (64%)]\tLoss: 29.965607\n",
      "Train Epoch: 224 [6400/8000 (80%)]\tLoss: 30.522406\n",
      "Train Epoch: 224 [7680/8000 (96%)]\tLoss: 29.906023\n",
      "====> Epoch: 224 Average loss: 30.2295\n",
      "Train Epoch: 225 [0/8000 (0%)]\tLoss: 30.131325\n",
      "Train Epoch: 225 [1280/8000 (16%)]\tLoss: 30.114397\n",
      "Train Epoch: 225 [2560/8000 (32%)]\tLoss: 30.215420\n",
      "Train Epoch: 225 [3840/8000 (48%)]\tLoss: 30.162968\n",
      "Loss tensor(1921.1942, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(844.3243, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 225 [5120/8000 (64%)]\tLoss: 30.356462\n",
      "Train Epoch: 225 [6400/8000 (80%)]\tLoss: 30.209015\n",
      "Train Epoch: 225 [7680/8000 (96%)]\tLoss: 30.355381\n",
      "====> Epoch: 225 Average loss: 30.2285\n",
      "Train Epoch: 226 [0/8000 (0%)]\tLoss: 30.032848\n",
      "Train Epoch: 226 [1280/8000 (16%)]\tLoss: 30.191202\n",
      "Train Epoch: 226 [2560/8000 (32%)]\tLoss: 30.378105\n",
      "Loss tensor(1914.2800, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(844.3521, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 226 [3840/8000 (48%)]\tLoss: 30.302340\n",
      "Train Epoch: 226 [5120/8000 (64%)]\tLoss: 29.890648\n",
      "Train Epoch: 226 [6400/8000 (80%)]\tLoss: 30.063553\n",
      "Train Epoch: 226 [7680/8000 (96%)]\tLoss: 30.631704\n",
      "====> Epoch: 226 Average loss: 30.2126\n",
      "Train Epoch: 227 [0/8000 (0%)]\tLoss: 30.205059\n",
      "Loss tensor(1928.8899, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(843.6361, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 227 [1280/8000 (16%)]\tLoss: 30.407019\n",
      "Train Epoch: 227 [2560/8000 (32%)]\tLoss: 30.197443\n",
      "Train Epoch: 227 [3840/8000 (48%)]\tLoss: 29.904892\n",
      "Train Epoch: 227 [5120/8000 (64%)]\tLoss: 29.952213\n",
      "Train Epoch: 227 [6400/8000 (80%)]\tLoss: 30.341366\n",
      "Train Epoch: 227 [7680/8000 (96%)]\tLoss: 30.447615\n",
      "====> Epoch: 227 Average loss: 30.2211\n",
      "Train Epoch: 228 [0/8000 (0%)]\tLoss: 30.168301\n",
      "Train Epoch: 228 [1280/8000 (16%)]\tLoss: 30.432478\n",
      "Train Epoch: 228 [2560/8000 (32%)]\tLoss: 30.153383\n",
      "Train Epoch: 228 [3840/8000 (48%)]\tLoss: 30.942406\n",
      "Train Epoch: 228 [5120/8000 (64%)]\tLoss: 29.663565\n",
      "Train Epoch: 228 [6400/8000 (80%)]\tLoss: 30.146238\n",
      "Train Epoch: 228 [7680/8000 (96%)]\tLoss: 30.368189\n",
      "====> Epoch: 228 Average loss: 30.2233\n",
      "Train Epoch: 229 [0/8000 (0%)]\tLoss: 30.316879\n",
      "Train Epoch: 229 [1280/8000 (16%)]\tLoss: 29.632437\n",
      "Loss tensor(1948.7474, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(840.9742, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 229 [2560/8000 (32%)]\tLoss: 30.513670\n",
      "Loss tensor(1918.5717, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(845.9470, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 229 [3840/8000 (48%)]\tLoss: 30.188791\n",
      "Train Epoch: 229 [5120/8000 (64%)]\tLoss: 30.298048\n",
      "Train Epoch: 229 [6400/8000 (80%)]\tLoss: 30.122135\n",
      "Train Epoch: 229 [7680/8000 (96%)]\tLoss: 30.106131\n",
      "====> Epoch: 229 Average loss: 30.2354\n",
      "Train Epoch: 230 [0/8000 (0%)]\tLoss: 30.118782\n",
      "Train Epoch: 230 [1280/8000 (16%)]\tLoss: 30.088081\n",
      "Train Epoch: 230 [2560/8000 (32%)]\tLoss: 30.259819\n",
      "Train Epoch: 230 [3840/8000 (48%)]\tLoss: 30.264994\n",
      "Train Epoch: 230 [5120/8000 (64%)]\tLoss: 30.200495\n",
      "Train Epoch: 230 [6400/8000 (80%)]\tLoss: 30.307549\n",
      "Loss tensor(1931.6119, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(838.7069, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 230 [7680/8000 (96%)]\tLoss: 30.421381\n",
      "====> Epoch: 230 Average loss: 30.2304\n",
      "Train Epoch: 231 [0/8000 (0%)]\tLoss: 30.374084\n",
      "Train Epoch: 231 [1280/8000 (16%)]\tLoss: 30.207449\n",
      "Loss tensor(1924.1481, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(839.6324, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 231 [2560/8000 (32%)]\tLoss: 30.087671\n",
      "Train Epoch: 231 [3840/8000 (48%)]\tLoss: 30.360044\n",
      "Train Epoch: 231 [5120/8000 (64%)]\tLoss: 30.188213\n",
      "Train Epoch: 231 [6400/8000 (80%)]\tLoss: 30.517872\n",
      "Train Epoch: 231 [7680/8000 (96%)]\tLoss: 30.072763\n",
      "====> Epoch: 231 Average loss: 30.2278\n",
      "Train Epoch: 232 [0/8000 (0%)]\tLoss: 30.716190\n",
      "Train Epoch: 232 [1280/8000 (16%)]\tLoss: 30.050905\n",
      "Train Epoch: 232 [2560/8000 (32%)]\tLoss: 30.232063\n",
      "Train Epoch: 232 [3840/8000 (48%)]\tLoss: 30.209471\n",
      "Loss tensor(1939.9028, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(845.6990, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 232 [5120/8000 (64%)]\tLoss: 30.925133\n",
      "Loss tensor(1943.5486, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(845.9713, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 232 [6400/8000 (80%)]\tLoss: 29.978228\n",
      "Loss tensor(1933.7839, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(851.8187, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 232 [7680/8000 (96%)]\tLoss: 30.268679\n",
      "====> Epoch: 232 Average loss: 30.2299\n",
      "Train Epoch: 233 [0/8000 (0%)]\tLoss: 30.433296\n",
      "Loss tensor(1929.9574, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(854.7027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 233 [1280/8000 (16%)]\tLoss: 29.888508\n",
      "Train Epoch: 233 [2560/8000 (32%)]\tLoss: 29.877235\n",
      "Train Epoch: 233 [3840/8000 (48%)]\tLoss: 30.396942\n",
      "Train Epoch: 233 [5120/8000 (64%)]\tLoss: 30.325972\n",
      "Train Epoch: 233 [6400/8000 (80%)]\tLoss: 30.303453\n",
      "Train Epoch: 233 [7680/8000 (96%)]\tLoss: 30.427029\n",
      "====> Epoch: 233 Average loss: 30.2147\n",
      "Train Epoch: 234 [0/8000 (0%)]\tLoss: 30.370037\n",
      "Train Epoch: 234 [1280/8000 (16%)]\tLoss: 30.338978\n",
      "Train Epoch: 234 [2560/8000 (32%)]\tLoss: 30.323643\n",
      "Train Epoch: 234 [3840/8000 (48%)]\tLoss: 30.009558\n",
      "Train Epoch: 234 [5120/8000 (64%)]\tLoss: 30.282974\n",
      "Train Epoch: 234 [6400/8000 (80%)]\tLoss: 30.295380\n",
      "Train Epoch: 234 [7680/8000 (96%)]\tLoss: 30.240030\n",
      "====> Epoch: 234 Average loss: 30.2268\n",
      "Train Epoch: 235 [0/8000 (0%)]\tLoss: 30.125666\n",
      "Train Epoch: 235 [1280/8000 (16%)]\tLoss: 30.168375\n",
      "Loss tensor(1917.3616, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(851.7638, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 235 [2560/8000 (32%)]\tLoss: 30.242729\n",
      "Loss tensor(1935.2417, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(850.1066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 235 [3840/8000 (48%)]\tLoss: 30.181541\n",
      "Loss tensor(1950.3383, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(852.1378, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 235 [5120/8000 (64%)]\tLoss: 30.112411\n",
      "Train Epoch: 235 [6400/8000 (80%)]\tLoss: 30.367001\n",
      "Train Epoch: 235 [7680/8000 (96%)]\tLoss: 29.792269\n",
      "====> Epoch: 235 Average loss: 30.2162\n",
      "Train Epoch: 236 [0/8000 (0%)]\tLoss: 30.275410\n",
      "Train Epoch: 236 [1280/8000 (16%)]\tLoss: 30.417864\n",
      "Train Epoch: 236 [2560/8000 (32%)]\tLoss: 30.217501\n",
      "Train Epoch: 236 [3840/8000 (48%)]\tLoss: 30.608110\n",
      "Train Epoch: 236 [5120/8000 (64%)]\tLoss: 30.244062\n",
      "Train Epoch: 236 [6400/8000 (80%)]\tLoss: 29.908197\n",
      "Train Epoch: 236 [7680/8000 (96%)]\tLoss: 30.254759\n",
      "====> Epoch: 236 Average loss: 30.2333\n",
      "Train Epoch: 237 [0/8000 (0%)]\tLoss: 30.310316\n",
      "Train Epoch: 237 [1280/8000 (16%)]\tLoss: 30.328295\n",
      "Train Epoch: 237 [2560/8000 (32%)]\tLoss: 30.541513\n",
      "Train Epoch: 237 [3840/8000 (48%)]\tLoss: 29.883371\n",
      "Train Epoch: 237 [5120/8000 (64%)]\tLoss: 30.136684\n",
      "Train Epoch: 237 [6400/8000 (80%)]\tLoss: 30.333805\n",
      "Loss tensor(1918.9115, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(848.0208, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1924.3414, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(848.3903, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 237 [7680/8000 (96%)]\tLoss: 30.296679\n",
      "====> Epoch: 237 Average loss: 30.2288\n",
      "Train Epoch: 238 [0/8000 (0%)]\tLoss: 30.288467\n",
      "Train Epoch: 238 [1280/8000 (16%)]\tLoss: 30.148544\n",
      "Train Epoch: 238 [2560/8000 (32%)]\tLoss: 30.013605\n",
      "Train Epoch: 238 [3840/8000 (48%)]\tLoss: 29.898859\n",
      "Train Epoch: 238 [5120/8000 (64%)]\tLoss: 30.574324\n",
      "Loss tensor(1938.1830, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(848.0367, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 238 [6400/8000 (80%)]\tLoss: 30.302929\n",
      "Train Epoch: 238 [7680/8000 (96%)]\tLoss: 30.108105\n",
      "====> Epoch: 238 Average loss: 30.2138\n",
      "Train Epoch: 239 [0/8000 (0%)]\tLoss: 30.095926\n",
      "Train Epoch: 239 [1280/8000 (16%)]\tLoss: 30.107515\n",
      "Train Epoch: 239 [2560/8000 (32%)]\tLoss: 29.844805\n",
      "Train Epoch: 239 [3840/8000 (48%)]\tLoss: 30.614491\n",
      "Train Epoch: 239 [5120/8000 (64%)]\tLoss: 30.216179\n",
      "Train Epoch: 239 [6400/8000 (80%)]\tLoss: 30.059736\n",
      "Train Epoch: 239 [7680/8000 (96%)]\tLoss: 30.172966\n",
      "====> Epoch: 239 Average loss: 30.2172\n",
      "Train Epoch: 240 [0/8000 (0%)]\tLoss: 30.447807\n",
      "Train Epoch: 240 [1280/8000 (16%)]\tLoss: 30.480206\n",
      "Train Epoch: 240 [2560/8000 (32%)]\tLoss: 29.910900\n",
      "Train Epoch: 240 [3840/8000 (48%)]\tLoss: 29.916946\n",
      "Train Epoch: 240 [5120/8000 (64%)]\tLoss: 30.596704\n",
      "Train Epoch: 240 [6400/8000 (80%)]\tLoss: 30.145634\n",
      "Train Epoch: 240 [7680/8000 (96%)]\tLoss: 30.388386\n",
      "====> Epoch: 240 Average loss: 30.2187\n",
      "Train Epoch: 241 [0/8000 (0%)]\tLoss: 30.193398\n",
      "Train Epoch: 241 [1280/8000 (16%)]\tLoss: 30.013779\n",
      "Train Epoch: 241 [2560/8000 (32%)]\tLoss: 30.298918\n",
      "Train Epoch: 241 [3840/8000 (48%)]\tLoss: 30.202402\n",
      "Train Epoch: 241 [5120/8000 (64%)]\tLoss: 30.121460\n",
      "Train Epoch: 241 [6400/8000 (80%)]\tLoss: 30.147442\n",
      "Loss tensor(1938.1781, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(835.9696, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 241 [7680/8000 (96%)]\tLoss: 29.852625\n",
      "====> Epoch: 241 Average loss: 30.2200\n",
      "Train Epoch: 242 [0/8000 (0%)]\tLoss: 30.326359\n",
      "Train Epoch: 242 [1280/8000 (16%)]\tLoss: 30.244932\n",
      "Train Epoch: 242 [2560/8000 (32%)]\tLoss: 30.307627\n",
      "Train Epoch: 242 [3840/8000 (48%)]\tLoss: 30.079622\n",
      "Train Epoch: 242 [5120/8000 (64%)]\tLoss: 30.542618\n",
      "Train Epoch: 242 [6400/8000 (80%)]\tLoss: 30.334103\n",
      "Train Epoch: 242 [7680/8000 (96%)]\tLoss: 30.342337\n",
      "====> Epoch: 242 Average loss: 30.2334\n",
      "Train Epoch: 243 [0/8000 (0%)]\tLoss: 29.885357\n",
      "Train Epoch: 243 [1280/8000 (16%)]\tLoss: 30.093163\n",
      "Train Epoch: 243 [2560/8000 (32%)]\tLoss: 30.222042\n",
      "Train Epoch: 243 [3840/8000 (48%)]\tLoss: 30.519550\n",
      "Train Epoch: 243 [5120/8000 (64%)]\tLoss: 30.082682\n",
      "Train Epoch: 243 [6400/8000 (80%)]\tLoss: 30.399632\n",
      "Train Epoch: 243 [7680/8000 (96%)]\tLoss: 29.961866\n",
      "====> Epoch: 243 Average loss: 30.2156\n",
      "Train Epoch: 244 [0/8000 (0%)]\tLoss: 29.985678\n",
      "Loss tensor(1932.0411, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(842.5957, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 244 [1280/8000 (16%)]\tLoss: 30.153641\n",
      "Train Epoch: 244 [2560/8000 (32%)]\tLoss: 30.434788\n",
      "Train Epoch: 244 [3840/8000 (48%)]\tLoss: 30.057041\n",
      "Loss tensor(1925.2761, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(841.8469, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 244 [5120/8000 (64%)]\tLoss: 30.081133\n",
      "Loss tensor(1961.8171, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(838.2131, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 244 [6400/8000 (80%)]\tLoss: 30.128696\n",
      "Loss tensor(1951.1317, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(843.9445, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1915.5435, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(844.6061, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 244 [7680/8000 (96%)]\tLoss: 30.266808\n",
      "====> Epoch: 244 Average loss: 30.2253\n",
      "Train Epoch: 245 [0/8000 (0%)]\tLoss: 30.285286\n",
      "Loss tensor(1931.8838, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(840.8458, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 245 [1280/8000 (16%)]\tLoss: 30.594803\n",
      "Train Epoch: 245 [2560/8000 (32%)]\tLoss: 29.970432\n",
      "Train Epoch: 245 [3840/8000 (48%)]\tLoss: 30.092735\n",
      "Train Epoch: 245 [5120/8000 (64%)]\tLoss: 30.019194\n",
      "Train Epoch: 245 [6400/8000 (80%)]\tLoss: 30.219517\n",
      "Train Epoch: 245 [7680/8000 (96%)]\tLoss: 29.628191\n",
      "====> Epoch: 245 Average loss: 30.2198\n",
      "Train Epoch: 246 [0/8000 (0%)]\tLoss: 30.498201\n",
      "Train Epoch: 246 [1280/8000 (16%)]\tLoss: 30.435791\n",
      "Train Epoch: 246 [2560/8000 (32%)]\tLoss: 30.256678\n",
      "Train Epoch: 246 [3840/8000 (48%)]\tLoss: 30.587555\n",
      "Train Epoch: 246 [5120/8000 (64%)]\tLoss: 30.504469\n",
      "Train Epoch: 246 [6400/8000 (80%)]\tLoss: 30.251778\n",
      "Loss tensor(1933.0526, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(848.4364, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 246 [7680/8000 (96%)]\tLoss: 30.194641\n",
      "====> Epoch: 246 Average loss: 30.2315\n",
      "Train Epoch: 247 [0/8000 (0%)]\tLoss: 30.147175\n",
      "Loss tensor(1937.2758, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(849.1735, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 247 [1280/8000 (16%)]\tLoss: 30.269934\n",
      "Train Epoch: 247 [2560/8000 (32%)]\tLoss: 30.314011\n",
      "Train Epoch: 247 [3840/8000 (48%)]\tLoss: 30.529064\n",
      "Train Epoch: 247 [5120/8000 (64%)]\tLoss: 30.228022\n",
      "Train Epoch: 247 [6400/8000 (80%)]\tLoss: 30.039169\n",
      "Loss tensor(1931.9583, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(855.1724, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 247 [7680/8000 (96%)]\tLoss: 29.884628\n",
      "====> Epoch: 247 Average loss: 30.2034\n",
      "Train Epoch: 248 [0/8000 (0%)]\tLoss: 30.347504\n",
      "Train Epoch: 248 [1280/8000 (16%)]\tLoss: 30.271349\n",
      "Train Epoch: 248 [2560/8000 (32%)]\tLoss: 30.236547\n",
      "Train Epoch: 248 [3840/8000 (48%)]\tLoss: 30.361568\n",
      "Train Epoch: 248 [5120/8000 (64%)]\tLoss: 30.039026\n",
      "Train Epoch: 248 [6400/8000 (80%)]\tLoss: 30.443426\n",
      "Train Epoch: 248 [7680/8000 (96%)]\tLoss: 30.752401\n",
      "====> Epoch: 248 Average loss: 30.2103\n",
      "Train Epoch: 249 [0/8000 (0%)]\tLoss: 30.716387\n",
      "Train Epoch: 249 [1280/8000 (16%)]\tLoss: 30.260509\n",
      "Train Epoch: 249 [2560/8000 (32%)]\tLoss: 30.212063\n",
      "Train Epoch: 249 [3840/8000 (48%)]\tLoss: 29.984539\n",
      "Train Epoch: 249 [5120/8000 (64%)]\tLoss: 30.759529\n",
      "Train Epoch: 249 [6400/8000 (80%)]\tLoss: 30.985102\n",
      "Train Epoch: 249 [7680/8000 (96%)]\tLoss: 30.504858\n",
      "====> Epoch: 249 Average loss: 30.2319\n",
      "Train Epoch: 250 [0/8000 (0%)]\tLoss: 30.597445\n",
      "Loss tensor(1925.7611, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(849.0378, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 250 [1280/8000 (16%)]\tLoss: 30.181759\n",
      "Train Epoch: 250 [2560/8000 (32%)]\tLoss: 30.171717\n",
      "Loss tensor(1937.6592, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(846.3014, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 250 [3840/8000 (48%)]\tLoss: 30.053917\n",
      "Train Epoch: 250 [5120/8000 (64%)]\tLoss: 30.318567\n",
      "Loss tensor(1910.6704, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(839.7830, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 250 [6400/8000 (80%)]\tLoss: 30.198900\n",
      "Train Epoch: 250 [7680/8000 (96%)]\tLoss: 30.471430\n",
      "====> Epoch: 250 Average loss: 30.2270\n",
      "Train Epoch: 251 [0/8000 (0%)]\tLoss: 30.004892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 251 [1280/8000 (16%)]\tLoss: 30.604284\n",
      "Train Epoch: 251 [2560/8000 (32%)]\tLoss: 30.439838\n",
      "Train Epoch: 251 [3840/8000 (48%)]\tLoss: 30.398069\n",
      "Train Epoch: 251 [5120/8000 (64%)]\tLoss: 30.083801\n",
      "Train Epoch: 251 [6400/8000 (80%)]\tLoss: 30.186466\n",
      "Train Epoch: 251 [7680/8000 (96%)]\tLoss: 30.250629\n",
      "====> Epoch: 251 Average loss: 30.2410\n",
      "Train Epoch: 252 [0/8000 (0%)]\tLoss: 30.152929\n",
      "Train Epoch: 252 [1280/8000 (16%)]\tLoss: 30.570299\n",
      "Loss tensor(1963.9041, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(851.9352, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 252 [2560/8000 (32%)]\tLoss: 29.726429\n",
      "Train Epoch: 252 [3840/8000 (48%)]\tLoss: 30.351229\n",
      "Train Epoch: 252 [5120/8000 (64%)]\tLoss: 30.206125\n",
      "Loss tensor(1947.7992, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(849.4653, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 252 [6400/8000 (80%)]\tLoss: 29.975332\n",
      "Loss tensor(1939.6611, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(850.7686, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 252 [7680/8000 (96%)]\tLoss: 30.416342\n",
      "====> Epoch: 252 Average loss: 30.2329\n",
      "Train Epoch: 253 [0/8000 (0%)]\tLoss: 29.922930\n",
      "Train Epoch: 253 [1280/8000 (16%)]\tLoss: 30.156424\n",
      "Loss tensor(1950.1952, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(845.4732, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1915.6393, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(847.4385, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 253 [2560/8000 (32%)]\tLoss: 30.066065\n",
      "Train Epoch: 253 [3840/8000 (48%)]\tLoss: 30.244150\n",
      "Train Epoch: 253 [5120/8000 (64%)]\tLoss: 30.073591\n",
      "Train Epoch: 253 [6400/8000 (80%)]\tLoss: 30.238569\n",
      "Train Epoch: 253 [7680/8000 (96%)]\tLoss: 30.645226\n",
      "====> Epoch: 253 Average loss: 30.2205\n",
      "Train Epoch: 254 [0/8000 (0%)]\tLoss: 30.332579\n",
      "Train Epoch: 254 [1280/8000 (16%)]\tLoss: 30.109180\n",
      "Loss tensor(1922.7736, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(856.3104, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 254 [2560/8000 (32%)]\tLoss: 30.246002\n",
      "Train Epoch: 254 [3840/8000 (48%)]\tLoss: 29.871330\n",
      "Train Epoch: 254 [5120/8000 (64%)]\tLoss: 30.045885\n",
      "Train Epoch: 254 [6400/8000 (80%)]\tLoss: 29.977112\n",
      "Train Epoch: 254 [7680/8000 (96%)]\tLoss: 30.048899\n",
      "====> Epoch: 254 Average loss: 30.2107\n",
      "Train Epoch: 255 [0/8000 (0%)]\tLoss: 30.130915\n",
      "Loss tensor(1925.0533, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(853.1110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1936.9933, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(853.6792, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 255 [1280/8000 (16%)]\tLoss: 30.328030\n",
      "Train Epoch: 255 [2560/8000 (32%)]\tLoss: 29.905293\n",
      "Train Epoch: 255 [3840/8000 (48%)]\tLoss: 30.321413\n",
      "Train Epoch: 255 [5120/8000 (64%)]\tLoss: 30.174656\n",
      "Train Epoch: 255 [6400/8000 (80%)]\tLoss: 30.583984\n",
      "Train Epoch: 255 [7680/8000 (96%)]\tLoss: 30.116808\n",
      "====> Epoch: 255 Average loss: 30.2041\n",
      "Train Epoch: 256 [0/8000 (0%)]\tLoss: 30.177942\n",
      "Loss tensor(1922.2505, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(861.1607, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 256 [1280/8000 (16%)]\tLoss: 30.056870\n",
      "Train Epoch: 256 [2560/8000 (32%)]\tLoss: 30.444921\n",
      "Train Epoch: 256 [3840/8000 (48%)]\tLoss: 30.048479\n",
      "Train Epoch: 256 [5120/8000 (64%)]\tLoss: 30.248852\n",
      "Train Epoch: 256 [6400/8000 (80%)]\tLoss: 30.351185\n",
      "Train Epoch: 256 [7680/8000 (96%)]\tLoss: 30.143391\n",
      "====> Epoch: 256 Average loss: 30.2147\n",
      "Train Epoch: 257 [0/8000 (0%)]\tLoss: 30.372267\n",
      "Train Epoch: 257 [1280/8000 (16%)]\tLoss: 30.260849\n",
      "Train Epoch: 257 [2560/8000 (32%)]\tLoss: 30.286858\n",
      "Loss tensor(1939.3207, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(853.2246, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 257 [3840/8000 (48%)]\tLoss: 30.049061\n",
      "Train Epoch: 257 [5120/8000 (64%)]\tLoss: 30.376301\n",
      "Loss tensor(1925.2889, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(863.8408, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 257 [6400/8000 (80%)]\tLoss: 30.159054\n",
      "Train Epoch: 257 [7680/8000 (96%)]\tLoss: 30.735561\n",
      "====> Epoch: 257 Average loss: 30.1964\n",
      "Train Epoch: 258 [0/8000 (0%)]\tLoss: 29.888237\n",
      "Train Epoch: 258 [1280/8000 (16%)]\tLoss: 29.966345\n",
      "Train Epoch: 258 [2560/8000 (32%)]\tLoss: 30.352283\n",
      "Loss tensor(1927.0128, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(865.6332, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 258 [3840/8000 (48%)]\tLoss: 30.101542\n",
      "Train Epoch: 258 [5120/8000 (64%)]\tLoss: 30.169960\n",
      "Train Epoch: 258 [6400/8000 (80%)]\tLoss: 30.195494\n",
      "Train Epoch: 258 [7680/8000 (96%)]\tLoss: 30.115347\n",
      "====> Epoch: 258 Average loss: 30.2059\n",
      "Train Epoch: 259 [0/8000 (0%)]\tLoss: 30.079985\n",
      "Train Epoch: 259 [1280/8000 (16%)]\tLoss: 29.944107\n",
      "Train Epoch: 259 [2560/8000 (32%)]\tLoss: 30.128962\n",
      "Train Epoch: 259 [3840/8000 (48%)]\tLoss: 29.869900\n",
      "Loss tensor(1938.9197, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(872.3959, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 259 [5120/8000 (64%)]\tLoss: 30.249758\n",
      "Train Epoch: 259 [6400/8000 (80%)]\tLoss: 29.843805\n",
      "Train Epoch: 259 [7680/8000 (96%)]\tLoss: 30.366695\n",
      "====> Epoch: 259 Average loss: 30.2122\n",
      "Train Epoch: 260 [0/8000 (0%)]\tLoss: 29.828211\n",
      "Train Epoch: 260 [1280/8000 (16%)]\tLoss: 30.007322\n",
      "Train Epoch: 260 [2560/8000 (32%)]\tLoss: 30.205339\n",
      "Train Epoch: 260 [3840/8000 (48%)]\tLoss: 29.991247\n",
      "Train Epoch: 260 [5120/8000 (64%)]\tLoss: 30.126667\n",
      "Train Epoch: 260 [6400/8000 (80%)]\tLoss: 30.427788\n",
      "Train Epoch: 260 [7680/8000 (96%)]\tLoss: 30.091236\n",
      "====> Epoch: 260 Average loss: 30.2097\n",
      "Train Epoch: 261 [0/8000 (0%)]\tLoss: 30.158630\n",
      "Train Epoch: 261 [1280/8000 (16%)]\tLoss: 30.614742\n",
      "Train Epoch: 261 [2560/8000 (32%)]\tLoss: 30.175077\n",
      "Train Epoch: 261 [3840/8000 (48%)]\tLoss: 30.283318\n",
      "Train Epoch: 261 [5120/8000 (64%)]\tLoss: 30.026901\n",
      "Train Epoch: 261 [6400/8000 (80%)]\tLoss: 30.375927\n",
      "Train Epoch: 261 [7680/8000 (96%)]\tLoss: 30.272869\n",
      "====> Epoch: 261 Average loss: 30.2083\n",
      "Train Epoch: 262 [0/8000 (0%)]\tLoss: 30.302641\n",
      "Loss tensor(1944.4858, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(887.3890, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 262 [1280/8000 (16%)]\tLoss: 30.064985\n",
      "Train Epoch: 262 [2560/8000 (32%)]\tLoss: 30.821373\n",
      "Train Epoch: 262 [3840/8000 (48%)]\tLoss: 30.324585\n",
      "Train Epoch: 262 [5120/8000 (64%)]\tLoss: 30.074579\n",
      "Loss tensor(1937.8296, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(887.3995, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 262 [6400/8000 (80%)]\tLoss: 30.147732\n",
      "Loss tensor(1932.8590, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(884.5565, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 262 [7680/8000 (96%)]\tLoss: 30.195980\n",
      "====> Epoch: 262 Average loss: 30.2261\n",
      "Train Epoch: 263 [0/8000 (0%)]\tLoss: 29.901522\n",
      "Loss tensor(1928.5707, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(878.7443, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1947.7478, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(881.3868, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 263 [1280/8000 (16%)]\tLoss: 30.761492\n",
      "Loss tensor(1926.2239, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(883.6448, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1912.2415, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(880.9976, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 263 [2560/8000 (32%)]\tLoss: 30.104694\n",
      "Train Epoch: 263 [3840/8000 (48%)]\tLoss: 30.222219\n",
      "Train Epoch: 263 [5120/8000 (64%)]\tLoss: 30.502111\n",
      "Train Epoch: 263 [6400/8000 (80%)]\tLoss: 30.394320\n",
      "Train Epoch: 263 [7680/8000 (96%)]\tLoss: 30.237486\n",
      "====> Epoch: 263 Average loss: 30.2284\n",
      "Train Epoch: 264 [0/8000 (0%)]\tLoss: 30.701094\n",
      "Train Epoch: 264 [1280/8000 (16%)]\tLoss: 30.360367\n",
      "Train Epoch: 264 [2560/8000 (32%)]\tLoss: 30.519598\n",
      "Train Epoch: 264 [3840/8000 (48%)]\tLoss: 30.074898\n",
      "Loss tensor(1925.7203, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(865.9087, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 264 [5120/8000 (64%)]\tLoss: 30.166418\n",
      "Train Epoch: 264 [6400/8000 (80%)]\tLoss: 30.357185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 264 [7680/8000 (96%)]\tLoss: 30.161661\n",
      "====> Epoch: 264 Average loss: 30.2234\n",
      "Train Epoch: 265 [0/8000 (0%)]\tLoss: 30.655792\n",
      "Train Epoch: 265 [1280/8000 (16%)]\tLoss: 30.443710\n",
      "Train Epoch: 265 [2560/8000 (32%)]\tLoss: 30.014900\n",
      "Train Epoch: 265 [3840/8000 (48%)]\tLoss: 30.355537\n",
      "Train Epoch: 265 [5120/8000 (64%)]\tLoss: 30.059919\n",
      "Train Epoch: 265 [6400/8000 (80%)]\tLoss: 30.208008\n",
      "Train Epoch: 265 [7680/8000 (96%)]\tLoss: 30.196247\n",
      "====> Epoch: 265 Average loss: 30.2192\n",
      "Train Epoch: 266 [0/8000 (0%)]\tLoss: 30.274740\n",
      "Train Epoch: 266 [1280/8000 (16%)]\tLoss: 30.329647\n",
      "Loss tensor(1966.7365, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(878.1136, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 266 [2560/8000 (32%)]\tLoss: 30.374557\n",
      "Train Epoch: 266 [3840/8000 (48%)]\tLoss: 29.922493\n",
      "Train Epoch: 266 [5120/8000 (64%)]\tLoss: 30.127111\n",
      "Train Epoch: 266 [6400/8000 (80%)]\tLoss: 30.175995\n",
      "Train Epoch: 266 [7680/8000 (96%)]\tLoss: 30.416454\n",
      "====> Epoch: 266 Average loss: 30.1991\n",
      "Train Epoch: 267 [0/8000 (0%)]\tLoss: 30.185532\n",
      "Train Epoch: 267 [1280/8000 (16%)]\tLoss: 30.249065\n",
      "Train Epoch: 267 [2560/8000 (32%)]\tLoss: 30.321644\n",
      "Train Epoch: 267 [3840/8000 (48%)]\tLoss: 30.416571\n",
      "Train Epoch: 267 [5120/8000 (64%)]\tLoss: 29.939819\n",
      "Train Epoch: 267 [6400/8000 (80%)]\tLoss: 30.245686\n",
      "Loss tensor(1929.8280, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(892.2459, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 267 [7680/8000 (96%)]\tLoss: 29.951267\n",
      "====> Epoch: 267 Average loss: 30.2249\n",
      "Train Epoch: 268 [0/8000 (0%)]\tLoss: 30.270267\n",
      "Train Epoch: 268 [1280/8000 (16%)]\tLoss: 30.337984\n",
      "Train Epoch: 268 [2560/8000 (32%)]\tLoss: 29.888790\n",
      "Train Epoch: 268 [3840/8000 (48%)]\tLoss: 30.372913\n",
      "Train Epoch: 268 [5120/8000 (64%)]\tLoss: 30.236853\n",
      "Train Epoch: 268 [6400/8000 (80%)]\tLoss: 30.820921\n",
      "Loss tensor(1950.7167, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(908.5266, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 268 [7680/8000 (96%)]\tLoss: 30.307302\n",
      "====> Epoch: 268 Average loss: 30.2066\n",
      "Train Epoch: 269 [0/8000 (0%)]\tLoss: 30.069386\n",
      "Train Epoch: 269 [1280/8000 (16%)]\tLoss: 29.944252\n",
      "Loss tensor(1934.1383, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(907.8552, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 269 [2560/8000 (32%)]\tLoss: 30.016243\n",
      "Train Epoch: 269 [3840/8000 (48%)]\tLoss: 30.211046\n",
      "Loss tensor(1922.1171, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(922.4426, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 269 [5120/8000 (64%)]\tLoss: 29.967356\n",
      "Train Epoch: 269 [6400/8000 (80%)]\tLoss: 30.379173\n",
      "Loss tensor(1905.0576, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(920.7434, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 269 [7680/8000 (96%)]\tLoss: 30.057259\n",
      "====> Epoch: 269 Average loss: 30.2014\n",
      "Train Epoch: 270 [0/8000 (0%)]\tLoss: 30.090820\n",
      "Train Epoch: 270 [1280/8000 (16%)]\tLoss: 30.090229\n",
      "Train Epoch: 270 [2560/8000 (32%)]\tLoss: 30.092184\n",
      "Loss tensor(1931.1676, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(914.3370, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 270 [3840/8000 (48%)]\tLoss: 30.002905\n",
      "Train Epoch: 270 [5120/8000 (64%)]\tLoss: 30.392576\n",
      "Train Epoch: 270 [6400/8000 (80%)]\tLoss: 30.336531\n",
      "Train Epoch: 270 [7680/8000 (96%)]\tLoss: 30.067499\n",
      "====> Epoch: 270 Average loss: 30.2240\n",
      "Train Epoch: 271 [0/8000 (0%)]\tLoss: 29.877703\n",
      "Train Epoch: 271 [1280/8000 (16%)]\tLoss: 30.009483\n",
      "Loss tensor(1939.0112, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(928.2368, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 271 [2560/8000 (32%)]\tLoss: 30.080788\n",
      "Train Epoch: 271 [3840/8000 (48%)]\tLoss: 29.947437\n",
      "Train Epoch: 271 [5120/8000 (64%)]\tLoss: 30.489229\n",
      "Train Epoch: 271 [6400/8000 (80%)]\tLoss: 30.046253\n",
      "Train Epoch: 271 [7680/8000 (96%)]\tLoss: 30.075226\n",
      "====> Epoch: 271 Average loss: 30.2243\n",
      "Train Epoch: 272 [0/8000 (0%)]\tLoss: 29.963419\n",
      "Loss tensor(1936.9032, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(953.9106, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 272 [1280/8000 (16%)]\tLoss: 29.971453\n",
      "Loss tensor(1936.6338, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(954.0540, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 272 [2560/8000 (32%)]\tLoss: 30.130114\n",
      "Train Epoch: 272 [3840/8000 (48%)]\tLoss: 30.381056\n",
      "Train Epoch: 272 [5120/8000 (64%)]\tLoss: 29.984089\n",
      "Train Epoch: 272 [6400/8000 (80%)]\tLoss: 29.981108\n",
      "Train Epoch: 272 [7680/8000 (96%)]\tLoss: 30.019749\n",
      "====> Epoch: 272 Average loss: 30.2055\n",
      "Train Epoch: 273 [0/8000 (0%)]\tLoss: 29.928162\n",
      "Train Epoch: 273 [1280/8000 (16%)]\tLoss: 30.094337\n",
      "Train Epoch: 273 [2560/8000 (32%)]\tLoss: 30.230270\n",
      "Train Epoch: 273 [3840/8000 (48%)]\tLoss: 30.199793\n",
      "Train Epoch: 273 [5120/8000 (64%)]\tLoss: 30.288961\n",
      "Train Epoch: 273 [6400/8000 (80%)]\tLoss: 29.837088\n",
      "Train Epoch: 273 [7680/8000 (96%)]\tLoss: 30.062317\n",
      "====> Epoch: 273 Average loss: 30.1803\n",
      "Train Epoch: 274 [0/8000 (0%)]\tLoss: 30.279156\n",
      "Train Epoch: 274 [1280/8000 (16%)]\tLoss: 29.642351\n",
      "Train Epoch: 274 [2560/8000 (32%)]\tLoss: 30.266935\n",
      "Train Epoch: 274 [3840/8000 (48%)]\tLoss: 30.839710\n",
      "Train Epoch: 274 [5120/8000 (64%)]\tLoss: 30.032701\n",
      "Loss tensor(1914.7633, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(982.0182, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 274 [6400/8000 (80%)]\tLoss: 29.905870\n",
      "Train Epoch: 274 [7680/8000 (96%)]\tLoss: 29.880144\n",
      "====> Epoch: 274 Average loss: 30.2016\n",
      "Train Epoch: 275 [0/8000 (0%)]\tLoss: 30.122866\n",
      "Train Epoch: 275 [1280/8000 (16%)]\tLoss: 30.443253\n",
      "Loss tensor(1919.9871, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(991.0047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 275 [2560/8000 (32%)]\tLoss: 30.541018\n",
      "Train Epoch: 275 [3840/8000 (48%)]\tLoss: 30.266968\n",
      "Train Epoch: 275 [5120/8000 (64%)]\tLoss: 30.260252\n",
      "Train Epoch: 275 [6400/8000 (80%)]\tLoss: 30.494038\n",
      "Train Epoch: 275 [7680/8000 (96%)]\tLoss: 30.552061\n",
      "====> Epoch: 275 Average loss: 30.2075\n",
      "Train Epoch: 276 [0/8000 (0%)]\tLoss: 30.265259\n",
      "Train Epoch: 276 [1280/8000 (16%)]\tLoss: 29.964525\n",
      "Train Epoch: 276 [2560/8000 (32%)]\tLoss: 30.060780\n",
      "Train Epoch: 276 [3840/8000 (48%)]\tLoss: 30.562305\n",
      "Train Epoch: 276 [5120/8000 (64%)]\tLoss: 29.846869\n",
      "Train Epoch: 276 [6400/8000 (80%)]\tLoss: 30.051462\n",
      "Train Epoch: 276 [7680/8000 (96%)]\tLoss: 29.906473\n",
      "====> Epoch: 276 Average loss: 30.1760\n",
      "Train Epoch: 277 [0/8000 (0%)]\tLoss: 30.456572\n",
      "Train Epoch: 277 [1280/8000 (16%)]\tLoss: 30.052437\n",
      "Train Epoch: 277 [2560/8000 (32%)]\tLoss: 30.075893\n",
      "Train Epoch: 277 [3840/8000 (48%)]\tLoss: 30.203810\n",
      "Train Epoch: 277 [5120/8000 (64%)]\tLoss: 30.174067\n",
      "Train Epoch: 277 [6400/8000 (80%)]\tLoss: 30.059042\n",
      "Train Epoch: 277 [7680/8000 (96%)]\tLoss: 30.735443\n",
      "====> Epoch: 277 Average loss: 30.1856\n",
      "Train Epoch: 278 [0/8000 (0%)]\tLoss: 30.194809\n",
      "Loss tensor(1966.2426, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1017.4022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1951.0323, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1010.7395, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 278 [1280/8000 (16%)]\tLoss: 31.092154\n",
      "Train Epoch: 278 [2560/8000 (32%)]\tLoss: 30.223345\n",
      "Train Epoch: 278 [3840/8000 (48%)]\tLoss: 30.089008\n",
      "Train Epoch: 278 [5120/8000 (64%)]\tLoss: 30.486683\n",
      "Train Epoch: 278 [6400/8000 (80%)]\tLoss: 30.272945\n",
      "Train Epoch: 278 [7680/8000 (96%)]\tLoss: 30.266710\n",
      "====> Epoch: 278 Average loss: 30.1869\n",
      "Train Epoch: 279 [0/8000 (0%)]\tLoss: 30.183496\n",
      "Train Epoch: 279 [1280/8000 (16%)]\tLoss: 29.790321\n",
      "Train Epoch: 279 [2560/8000 (32%)]\tLoss: 29.915489\n",
      "Train Epoch: 279 [3840/8000 (48%)]\tLoss: 30.165413\n",
      "Train Epoch: 279 [5120/8000 (64%)]\tLoss: 30.215624\n",
      "Train Epoch: 279 [6400/8000 (80%)]\tLoss: 30.256117\n",
      "Loss tensor(1916.0699, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1012.0361, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 279 [7680/8000 (96%)]\tLoss: 29.938593\n",
      "====> Epoch: 279 Average loss: 30.1749\n",
      "Train Epoch: 280 [0/8000 (0%)]\tLoss: 30.422125\n",
      "Train Epoch: 280 [1280/8000 (16%)]\tLoss: 30.456179\n",
      "Train Epoch: 280 [2560/8000 (32%)]\tLoss: 30.036190\n",
      "Train Epoch: 280 [3840/8000 (48%)]\tLoss: 30.568844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 280 [5120/8000 (64%)]\tLoss: 29.864054\n",
      "Train Epoch: 280 [6400/8000 (80%)]\tLoss: 30.044102\n",
      "Train Epoch: 280 [7680/8000 (96%)]\tLoss: 29.688940\n",
      "====> Epoch: 280 Average loss: 30.1817\n",
      "Train Epoch: 281 [0/8000 (0%)]\tLoss: 30.230532\n",
      "Train Epoch: 281 [1280/8000 (16%)]\tLoss: 30.446821\n",
      "Train Epoch: 281 [2560/8000 (32%)]\tLoss: 30.010767\n",
      "Train Epoch: 281 [3840/8000 (48%)]\tLoss: 30.349386\n",
      "Loss tensor(1934.0614, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1032.5482, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1941.5803, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1028.4717, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 281 [5120/8000 (64%)]\tLoss: 30.326490\n",
      "Train Epoch: 281 [6400/8000 (80%)]\tLoss: 30.090363\n",
      "Train Epoch: 281 [7680/8000 (96%)]\tLoss: 30.151628\n",
      "====> Epoch: 281 Average loss: 30.1819\n",
      "Train Epoch: 282 [0/8000 (0%)]\tLoss: 30.077501\n",
      "Train Epoch: 282 [1280/8000 (16%)]\tLoss: 29.942728\n",
      "Train Epoch: 282 [2560/8000 (32%)]\tLoss: 30.153032\n",
      "Loss tensor(1908.7783, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1039.8640, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 282 [3840/8000 (48%)]\tLoss: 30.082104\n",
      "Train Epoch: 282 [5120/8000 (64%)]\tLoss: 30.106550\n",
      "Train Epoch: 282 [6400/8000 (80%)]\tLoss: 30.201836\n",
      "Train Epoch: 282 [7680/8000 (96%)]\tLoss: 30.178122\n",
      "====> Epoch: 282 Average loss: 30.1744\n",
      "Train Epoch: 283 [0/8000 (0%)]\tLoss: 30.251770\n",
      "Loss tensor(1950.5854, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1041.0975, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 283 [1280/8000 (16%)]\tLoss: 29.653917\n",
      "Train Epoch: 283 [2560/8000 (32%)]\tLoss: 30.271242\n",
      "Loss tensor(1930.4711, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1034.4512, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 283 [3840/8000 (48%)]\tLoss: 29.836777\n",
      "Train Epoch: 283 [5120/8000 (64%)]\tLoss: 30.286154\n",
      "Train Epoch: 283 [6400/8000 (80%)]\tLoss: 30.286816\n",
      "Train Epoch: 283 [7680/8000 (96%)]\tLoss: 30.245161\n",
      "====> Epoch: 283 Average loss: 30.1771\n",
      "Train Epoch: 284 [0/8000 (0%)]\tLoss: 30.030338\n",
      "Train Epoch: 284 [1280/8000 (16%)]\tLoss: 29.969021\n",
      "Loss tensor(1929.2656, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1052.4944, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 284 [2560/8000 (32%)]\tLoss: 29.589008\n",
      "Train Epoch: 284 [3840/8000 (48%)]\tLoss: 30.069889\n",
      "Train Epoch: 284 [5120/8000 (64%)]\tLoss: 29.745539\n",
      "Loss tensor(1938.5144, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1054.1335, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 284 [6400/8000 (80%)]\tLoss: 29.926525\n",
      "Train Epoch: 284 [7680/8000 (96%)]\tLoss: 30.357735\n",
      "====> Epoch: 284 Average loss: 30.1840\n",
      "Train Epoch: 285 [0/8000 (0%)]\tLoss: 30.365114\n",
      "Train Epoch: 285 [1280/8000 (16%)]\tLoss: 30.199045\n",
      "Train Epoch: 285 [2560/8000 (32%)]\tLoss: 29.776371\n",
      "Train Epoch: 285 [3840/8000 (48%)]\tLoss: 30.174580\n",
      "Loss tensor(1936.0310, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1072.5537, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 285 [5120/8000 (64%)]\tLoss: 30.271179\n",
      "Train Epoch: 285 [6400/8000 (80%)]\tLoss: 29.859505\n",
      "Train Epoch: 285 [7680/8000 (96%)]\tLoss: 30.246517\n",
      "====> Epoch: 285 Average loss: 30.1628\n",
      "Train Epoch: 286 [0/8000 (0%)]\tLoss: 30.386478\n",
      "Train Epoch: 286 [1280/8000 (16%)]\tLoss: 30.363605\n",
      "Train Epoch: 286 [2560/8000 (32%)]\tLoss: 30.210060\n",
      "Train Epoch: 286 [3840/8000 (48%)]\tLoss: 30.199387\n",
      "Train Epoch: 286 [5120/8000 (64%)]\tLoss: 30.246571\n",
      "Train Epoch: 286 [6400/8000 (80%)]\tLoss: 30.292952\n",
      "Train Epoch: 286 [7680/8000 (96%)]\tLoss: 29.959980\n",
      "====> Epoch: 286 Average loss: 30.1867\n",
      "Train Epoch: 287 [0/8000 (0%)]\tLoss: 29.986963\n",
      "Train Epoch: 287 [1280/8000 (16%)]\tLoss: 29.827702\n",
      "Loss tensor(1918.1860, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1077.8967, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 287 [2560/8000 (32%)]\tLoss: 29.971657\n",
      "Train Epoch: 287 [3840/8000 (48%)]\tLoss: 30.137756\n",
      "Train Epoch: 287 [5120/8000 (64%)]\tLoss: 30.097092\n",
      "Train Epoch: 287 [6400/8000 (80%)]\tLoss: 30.395021\n",
      "Train Epoch: 287 [7680/8000 (96%)]\tLoss: 29.901756\n",
      "====> Epoch: 287 Average loss: 30.1793\n",
      "Train Epoch: 288 [0/8000 (0%)]\tLoss: 30.381588\n",
      "Train Epoch: 288 [1280/8000 (16%)]\tLoss: 30.142717\n",
      "Train Epoch: 288 [2560/8000 (32%)]\tLoss: 30.291914\n",
      "Train Epoch: 288 [3840/8000 (48%)]\tLoss: 29.767345\n",
      "Train Epoch: 288 [5120/8000 (64%)]\tLoss: 30.206795\n",
      "Train Epoch: 288 [6400/8000 (80%)]\tLoss: 30.254940\n",
      "Train Epoch: 288 [7680/8000 (96%)]\tLoss: 30.049137\n",
      "====> Epoch: 288 Average loss: 30.1718\n",
      "Train Epoch: 289 [0/8000 (0%)]\tLoss: 30.320250\n",
      "Train Epoch: 289 [1280/8000 (16%)]\tLoss: 30.139814\n",
      "Train Epoch: 289 [2560/8000 (32%)]\tLoss: 30.504267\n",
      "Loss tensor(1951.3595, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1093.4440, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 289 [3840/8000 (48%)]\tLoss: 30.489992\n",
      "Train Epoch: 289 [5120/8000 (64%)]\tLoss: 29.937401\n",
      "Train Epoch: 289 [6400/8000 (80%)]\tLoss: 29.974051\n",
      "Train Epoch: 289 [7680/8000 (96%)]\tLoss: 30.228361\n",
      "====> Epoch: 289 Average loss: 30.1819\n",
      "Train Epoch: 290 [0/8000 (0%)]\tLoss: 30.185650\n",
      "Loss tensor(1904.8723, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1100.2734, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 290 [1280/8000 (16%)]\tLoss: 30.319731\n",
      "Train Epoch: 290 [2560/8000 (32%)]\tLoss: 30.300421\n",
      "Train Epoch: 290 [3840/8000 (48%)]\tLoss: 29.788303\n",
      "Train Epoch: 290 [5120/8000 (64%)]\tLoss: 30.074085\n",
      "Train Epoch: 290 [6400/8000 (80%)]\tLoss: 30.024305\n",
      "Train Epoch: 290 [7680/8000 (96%)]\tLoss: 30.313547\n",
      "====> Epoch: 290 Average loss: 30.1659\n",
      "Train Epoch: 291 [0/8000 (0%)]\tLoss: 30.264633\n",
      "Train Epoch: 291 [1280/8000 (16%)]\tLoss: 30.196875\n",
      "Loss tensor(1917.2794, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1121.8035, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 291 [2560/8000 (32%)]\tLoss: 30.313206\n",
      "Train Epoch: 291 [3840/8000 (48%)]\tLoss: 30.202589\n",
      "Loss tensor(1942.7301, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1130.9999, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 291 [5120/8000 (64%)]\tLoss: 30.252464\n",
      "Train Epoch: 291 [6400/8000 (80%)]\tLoss: 29.881763\n",
      "Train Epoch: 291 [7680/8000 (96%)]\tLoss: 29.947319\n",
      "Loss tensor(1924.7737, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1127.2864, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 291 Average loss: 30.1545\n",
      "Train Epoch: 292 [0/8000 (0%)]\tLoss: 30.314407\n",
      "Train Epoch: 292 [1280/8000 (16%)]\tLoss: 30.278305\n",
      "Train Epoch: 292 [2560/8000 (32%)]\tLoss: 29.836905\n",
      "Train Epoch: 292 [3840/8000 (48%)]\tLoss: 30.292994\n",
      "Train Epoch: 292 [5120/8000 (64%)]\tLoss: 30.100273\n",
      "Train Epoch: 292 [6400/8000 (80%)]\tLoss: 30.304747\n",
      "Train Epoch: 292 [7680/8000 (96%)]\tLoss: 30.012753\n",
      "====> Epoch: 292 Average loss: 30.1475\n",
      "Train Epoch: 293 [0/8000 (0%)]\tLoss: 30.114916\n",
      "Train Epoch: 293 [1280/8000 (16%)]\tLoss: 30.443325\n",
      "Train Epoch: 293 [2560/8000 (32%)]\tLoss: 30.009523\n",
      "Train Epoch: 293 [3840/8000 (48%)]\tLoss: 30.105598\n",
      "Train Epoch: 293 [5120/8000 (64%)]\tLoss: 30.091713\n",
      "Loss tensor(1928.1746, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1131.1819, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 293 [6400/8000 (80%)]\tLoss: 30.372925\n",
      "Loss tensor(1953.5876, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1138.6846, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 293 [7680/8000 (96%)]\tLoss: 30.244816\n",
      "====> Epoch: 293 Average loss: 30.1592\n",
      "Train Epoch: 294 [0/8000 (0%)]\tLoss: 30.320982\n",
      "Train Epoch: 294 [1280/8000 (16%)]\tLoss: 30.028811\n",
      "Loss tensor(1942.9518, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1134.1587, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 294 [2560/8000 (32%)]\tLoss: 30.148308\n",
      "Train Epoch: 294 [3840/8000 (48%)]\tLoss: 30.052780\n",
      "Train Epoch: 294 [5120/8000 (64%)]\tLoss: 30.129360\n",
      "Train Epoch: 294 [6400/8000 (80%)]\tLoss: 30.334377\n",
      "Train Epoch: 294 [7680/8000 (96%)]\tLoss: 30.025719\n",
      "====> Epoch: 294 Average loss: 30.1906\n",
      "Train Epoch: 295 [0/8000 (0%)]\tLoss: 29.898485\n",
      "Train Epoch: 295 [1280/8000 (16%)]\tLoss: 30.317554\n",
      "Train Epoch: 295 [2560/8000 (32%)]\tLoss: 30.188995\n",
      "Train Epoch: 295 [3840/8000 (48%)]\tLoss: 30.291540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 295 [5120/8000 (64%)]\tLoss: 30.063490\n",
      "Train Epoch: 295 [6400/8000 (80%)]\tLoss: 30.357029\n",
      "Train Epoch: 295 [7680/8000 (96%)]\tLoss: 29.834789\n",
      "====> Epoch: 295 Average loss: 30.1681\n",
      "Train Epoch: 296 [0/8000 (0%)]\tLoss: 30.301929\n",
      "Train Epoch: 296 [1280/8000 (16%)]\tLoss: 30.054169\n",
      "Train Epoch: 296 [2560/8000 (32%)]\tLoss: 30.163149\n",
      "Train Epoch: 296 [3840/8000 (48%)]\tLoss: 30.458941\n",
      "Train Epoch: 296 [5120/8000 (64%)]\tLoss: 30.034525\n",
      "Train Epoch: 296 [6400/8000 (80%)]\tLoss: 30.278130\n",
      "Train Epoch: 296 [7680/8000 (96%)]\tLoss: 30.128193\n",
      "====> Epoch: 296 Average loss: 30.1541\n",
      "Train Epoch: 297 [0/8000 (0%)]\tLoss: 30.348122\n",
      "Loss tensor(1942.7917, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1207.6609, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1927.1407, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1210.8983, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 297 [1280/8000 (16%)]\tLoss: 30.283661\n",
      "Loss tensor(1941.9163, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1212.6458, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 297 [2560/8000 (32%)]\tLoss: 30.861811\n",
      "Train Epoch: 297 [3840/8000 (48%)]\tLoss: 30.142023\n",
      "Train Epoch: 297 [5120/8000 (64%)]\tLoss: 30.218555\n",
      "Train Epoch: 297 [6400/8000 (80%)]\tLoss: 29.854507\n",
      "Train Epoch: 297 [7680/8000 (96%)]\tLoss: 30.149696\n",
      "====> Epoch: 297 Average loss: 30.1773\n",
      "Train Epoch: 298 [0/8000 (0%)]\tLoss: 30.048368\n",
      "Train Epoch: 298 [1280/8000 (16%)]\tLoss: 30.017527\n",
      "Loss tensor(1916.5748, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1223.9525, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1913.0330, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1223.4061, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 298 [2560/8000 (32%)]\tLoss: 30.250378\n",
      "Train Epoch: 298 [3840/8000 (48%)]\tLoss: 30.245861\n",
      "Train Epoch: 298 [5120/8000 (64%)]\tLoss: 29.963173\n",
      "Loss tensor(1916.0454, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1217.6268, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 298 [6400/8000 (80%)]\tLoss: 30.080805\n",
      "Train Epoch: 298 [7680/8000 (96%)]\tLoss: 30.781672\n",
      "====> Epoch: 298 Average loss: 30.1617\n",
      "Train Epoch: 299 [0/8000 (0%)]\tLoss: 29.855484\n",
      "Train Epoch: 299 [1280/8000 (16%)]\tLoss: 30.066826\n",
      "Loss tensor(1946.7375, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1227.3296, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 299 [2560/8000 (32%)]\tLoss: 30.058228\n",
      "Train Epoch: 299 [3840/8000 (48%)]\tLoss: 29.857792\n",
      "Train Epoch: 299 [5120/8000 (64%)]\tLoss: 30.136204\n",
      "Train Epoch: 299 [6400/8000 (80%)]\tLoss: 30.295200\n",
      "Train Epoch: 299 [7680/8000 (96%)]\tLoss: 29.898836\n",
      "====> Epoch: 299 Average loss: 30.1597\n",
      "Train Epoch: 300 [0/8000 (0%)]\tLoss: 29.905209\n",
      "Train Epoch: 300 [1280/8000 (16%)]\tLoss: 29.882559\n",
      "Train Epoch: 300 [2560/8000 (32%)]\tLoss: 30.223150\n",
      "Train Epoch: 300 [3840/8000 (48%)]\tLoss: 30.107605\n",
      "Train Epoch: 300 [5120/8000 (64%)]\tLoss: 30.077759\n",
      "Train Epoch: 300 [6400/8000 (80%)]\tLoss: 29.931499\n",
      "Train Epoch: 300 [7680/8000 (96%)]\tLoss: 30.020826\n",
      "====> Epoch: 300 Average loss: 30.1444\n",
      "Train Epoch: 301 [0/8000 (0%)]\tLoss: 30.114815\n",
      "Loss tensor(1908.3696, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1227.5139, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 301 [1280/8000 (16%)]\tLoss: 30.154087\n",
      "Train Epoch: 301 [2560/8000 (32%)]\tLoss: 30.094074\n",
      "Train Epoch: 301 [3840/8000 (48%)]\tLoss: 30.469040\n",
      "Train Epoch: 301 [5120/8000 (64%)]\tLoss: 30.153963\n",
      "Train Epoch: 301 [6400/8000 (80%)]\tLoss: 30.213362\n",
      "Train Epoch: 301 [7680/8000 (96%)]\tLoss: 30.254711\n",
      "====> Epoch: 301 Average loss: 30.1568\n",
      "Loss tensor(1962.6486, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1223.3179, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 302 [0/8000 (0%)]\tLoss: 30.666384\n",
      "Loss tensor(1952.3159, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1222.0751, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 302 [1280/8000 (16%)]\tLoss: 29.678055\n",
      "Loss tensor(1935.7878, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1214.1002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 302 [2560/8000 (32%)]\tLoss: 30.242159\n",
      "Train Epoch: 302 [3840/8000 (48%)]\tLoss: 30.136246\n",
      "Train Epoch: 302 [5120/8000 (64%)]\tLoss: 30.242485\n",
      "Train Epoch: 302 [6400/8000 (80%)]\tLoss: 30.008718\n",
      "Train Epoch: 302 [7680/8000 (96%)]\tLoss: 30.042562\n",
      "====> Epoch: 302 Average loss: 30.1609\n",
      "Train Epoch: 303 [0/8000 (0%)]\tLoss: 29.854864\n",
      "Loss tensor(1935.1364, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1217.5157, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 303 [1280/8000 (16%)]\tLoss: 30.329576\n",
      "Train Epoch: 303 [2560/8000 (32%)]\tLoss: 30.269491\n",
      "Train Epoch: 303 [3840/8000 (48%)]\tLoss: 30.201748\n",
      "Loss tensor(1921.5281, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1209.0383, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1925.9648, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1208.3517, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 303 [5120/8000 (64%)]\tLoss: 30.300016\n",
      "Train Epoch: 303 [6400/8000 (80%)]\tLoss: 30.233675\n",
      "Train Epoch: 303 [7680/8000 (96%)]\tLoss: 30.313810\n",
      "====> Epoch: 303 Average loss: 30.1882\n",
      "Train Epoch: 304 [0/8000 (0%)]\tLoss: 30.049833\n",
      "Train Epoch: 304 [1280/8000 (16%)]\tLoss: 30.225868\n",
      "Train Epoch: 304 [2560/8000 (32%)]\tLoss: 30.197575\n",
      "Train Epoch: 304 [3840/8000 (48%)]\tLoss: 30.502611\n",
      "Train Epoch: 304 [5120/8000 (64%)]\tLoss: 29.759981\n",
      "Loss tensor(1929.6864, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1213.5593, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 304 [6400/8000 (80%)]\tLoss: 30.340696\n",
      "Train Epoch: 304 [7680/8000 (96%)]\tLoss: 30.324566\n",
      "====> Epoch: 304 Average loss: 30.1706\n",
      "Train Epoch: 305 [0/8000 (0%)]\tLoss: 30.649426\n",
      "Train Epoch: 305 [1280/8000 (16%)]\tLoss: 30.115875\n",
      "Loss tensor(1920.6974, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1224.6379, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 305 [2560/8000 (32%)]\tLoss: 29.936197\n",
      "Loss tensor(1910.8204, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1221.5950, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 305 [3840/8000 (48%)]\tLoss: 29.871649\n",
      "Train Epoch: 305 [5120/8000 (64%)]\tLoss: 29.964417\n",
      "Train Epoch: 305 [6400/8000 (80%)]\tLoss: 30.315037\n",
      "Train Epoch: 305 [7680/8000 (96%)]\tLoss: 30.381447\n",
      "====> Epoch: 305 Average loss: 30.1515\n",
      "Train Epoch: 306 [0/8000 (0%)]\tLoss: 30.130007\n",
      "Train Epoch: 306 [1280/8000 (16%)]\tLoss: 30.092064\n",
      "Loss tensor(1937.6804, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1230.3352, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 306 [2560/8000 (32%)]\tLoss: 30.376780\n",
      "Train Epoch: 306 [3840/8000 (48%)]\tLoss: 29.999517\n",
      "Train Epoch: 306 [5120/8000 (64%)]\tLoss: 30.128328\n",
      "Loss tensor(1934.1985, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1224.5092, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 306 [6400/8000 (80%)]\tLoss: 29.986353\n",
      "Train Epoch: 306 [7680/8000 (96%)]\tLoss: 30.026894\n",
      "====> Epoch: 306 Average loss: 30.1644\n",
      "Train Epoch: 307 [0/8000 (0%)]\tLoss: 30.552126\n",
      "Train Epoch: 307 [1280/8000 (16%)]\tLoss: 30.306805\n",
      "Train Epoch: 307 [2560/8000 (32%)]\tLoss: 30.398666\n",
      "Train Epoch: 307 [3840/8000 (48%)]\tLoss: 29.868776\n",
      "Train Epoch: 307 [5120/8000 (64%)]\tLoss: 30.057346\n",
      "Train Epoch: 307 [6400/8000 (80%)]\tLoss: 30.465561\n",
      "Train Epoch: 307 [7680/8000 (96%)]\tLoss: 30.143209\n",
      "====> Epoch: 307 Average loss: 30.1453\n",
      "Train Epoch: 308 [0/8000 (0%)]\tLoss: 30.128416\n",
      "Train Epoch: 308 [1280/8000 (16%)]\tLoss: 30.498169\n",
      "Train Epoch: 308 [2560/8000 (32%)]\tLoss: 30.204916\n",
      "Train Epoch: 308 [3840/8000 (48%)]\tLoss: 29.970610\n",
      "Train Epoch: 308 [5120/8000 (64%)]\tLoss: 30.253521\n",
      "Train Epoch: 308 [6400/8000 (80%)]\tLoss: 29.833353\n",
      "Train Epoch: 308 [7680/8000 (96%)]\tLoss: 30.101158\n",
      "====> Epoch: 308 Average loss: 30.1578\n",
      "Train Epoch: 309 [0/8000 (0%)]\tLoss: 30.234802\n",
      "Train Epoch: 309 [1280/8000 (16%)]\tLoss: 29.937561\n",
      "Train Epoch: 309 [2560/8000 (32%)]\tLoss: 30.087429\n",
      "Train Epoch: 309 [3840/8000 (48%)]\tLoss: 29.880619\n",
      "Train Epoch: 309 [5120/8000 (64%)]\tLoss: 30.310764\n",
      "Loss tensor(1948.5031, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1242.6965, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 309 [6400/8000 (80%)]\tLoss: 29.836746\n",
      "Train Epoch: 309 [7680/8000 (96%)]\tLoss: 29.973015\n",
      "====> Epoch: 309 Average loss: 30.1560\n",
      "Train Epoch: 310 [0/8000 (0%)]\tLoss: 30.114807\n",
      "Train Epoch: 310 [1280/8000 (16%)]\tLoss: 29.830408\n",
      "Loss tensor(1933.0562, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1262.7321, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 310 [2560/8000 (32%)]\tLoss: 30.004347\n",
      "Train Epoch: 310 [3840/8000 (48%)]\tLoss: 30.120556\n",
      "Train Epoch: 310 [5120/8000 (64%)]\tLoss: 30.090136\n",
      "Train Epoch: 310 [6400/8000 (80%)]\tLoss: 30.157829\n",
      "Train Epoch: 310 [7680/8000 (96%)]\tLoss: 30.281628\n",
      "====> Epoch: 310 Average loss: 30.1460\n",
      "Train Epoch: 311 [0/8000 (0%)]\tLoss: 30.134666\n",
      "Train Epoch: 311 [1280/8000 (16%)]\tLoss: 30.174507\n",
      "Train Epoch: 311 [2560/8000 (32%)]\tLoss: 29.852432\n",
      "Train Epoch: 311 [3840/8000 (48%)]\tLoss: 30.030617\n",
      "Train Epoch: 311 [5120/8000 (64%)]\tLoss: 30.175270\n",
      "Train Epoch: 311 [6400/8000 (80%)]\tLoss: 30.242323\n",
      "Train Epoch: 311 [7680/8000 (96%)]\tLoss: 29.982973\n",
      "====> Epoch: 311 Average loss: 30.1435\n",
      "Train Epoch: 312 [0/8000 (0%)]\tLoss: 30.037249\n",
      "Train Epoch: 312 [1280/8000 (16%)]\tLoss: 30.413445\n",
      "Loss tensor(1929.4835, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1270.6727, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 312 [2560/8000 (32%)]\tLoss: 29.999744\n",
      "Loss tensor(1942.3977, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1271.2717, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1953.7483, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1270.2294, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 312 [3840/8000 (48%)]\tLoss: 30.221058\n",
      "Train Epoch: 312 [5120/8000 (64%)]\tLoss: 30.301426\n",
      "Loss tensor(1928.9767, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1273.4578, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 312 [6400/8000 (80%)]\tLoss: 29.827269\n",
      "Loss tensor(1909.6499, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1274.7417, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1922.1908, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1268.2332, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 312 [7680/8000 (96%)]\tLoss: 30.085449\n",
      "====> Epoch: 312 Average loss: 30.1252\n",
      "Train Epoch: 313 [0/8000 (0%)]\tLoss: 30.082172\n",
      "Loss tensor(1926.6461, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1267.9807, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1926.9336, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1267.3848, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 313 [1280/8000 (16%)]\tLoss: 30.189735\n",
      "Train Epoch: 313 [2560/8000 (32%)]\tLoss: 30.094273\n",
      "Train Epoch: 313 [3840/8000 (48%)]\tLoss: 30.261003\n",
      "Loss tensor(1963.1332, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1282.4988, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 313 [5120/8000 (64%)]\tLoss: 30.174175\n",
      "Loss tensor(1921.8954, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1277.3063, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 313 [6400/8000 (80%)]\tLoss: 30.110077\n",
      "Train Epoch: 313 [7680/8000 (96%)]\tLoss: 29.940273\n",
      "====> Epoch: 313 Average loss: 30.1296\n",
      "Train Epoch: 314 [0/8000 (0%)]\tLoss: 30.331709\n",
      "Train Epoch: 314 [1280/8000 (16%)]\tLoss: 30.344681\n",
      "Train Epoch: 314 [2560/8000 (32%)]\tLoss: 30.291443\n",
      "Loss tensor(1910.6390, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1277.5552, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 314 [3840/8000 (48%)]\tLoss: 29.936026\n",
      "Loss tensor(1909.4554, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1291.5157, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 314 [5120/8000 (64%)]\tLoss: 30.023693\n",
      "Train Epoch: 314 [6400/8000 (80%)]\tLoss: 30.027719\n",
      "Loss tensor(1926.5536, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1325.0908, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 314 [7680/8000 (96%)]\tLoss: 29.948238\n",
      "====> Epoch: 314 Average loss: 30.1318\n",
      "Train Epoch: 315 [0/8000 (0%)]\tLoss: 30.390842\n",
      "Train Epoch: 315 [1280/8000 (16%)]\tLoss: 29.715736\n",
      "Train Epoch: 315 [2560/8000 (32%)]\tLoss: 29.974073\n",
      "Train Epoch: 315 [3840/8000 (48%)]\tLoss: 30.557247\n",
      "Train Epoch: 315 [5120/8000 (64%)]\tLoss: 30.208881\n",
      "Train Epoch: 315 [6400/8000 (80%)]\tLoss: 30.124788\n",
      "Train Epoch: 315 [7680/8000 (96%)]\tLoss: 30.087488\n",
      "====> Epoch: 315 Average loss: 30.1213\n",
      "Train Epoch: 316 [0/8000 (0%)]\tLoss: 30.156506\n",
      "Train Epoch: 316 [1280/8000 (16%)]\tLoss: 30.074928\n",
      "Train Epoch: 316 [2560/8000 (32%)]\tLoss: 30.359648\n",
      "Loss tensor(1914.0703, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1335.1722, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 316 [3840/8000 (48%)]\tLoss: 29.907349\n",
      "Train Epoch: 316 [5120/8000 (64%)]\tLoss: 30.139336\n",
      "Train Epoch: 316 [6400/8000 (80%)]\tLoss: 29.922085\n",
      "Train Epoch: 316 [7680/8000 (96%)]\tLoss: 29.769972\n",
      "====> Epoch: 316 Average loss: 30.1518\n",
      "Train Epoch: 317 [0/8000 (0%)]\tLoss: 30.613077\n",
      "Loss tensor(1913.8002, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1311.1768, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 317 [1280/8000 (16%)]\tLoss: 30.004196\n",
      "Loss tensor(1900.2443, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1301.6685, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1920.0103, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1304.1074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 317 [2560/8000 (32%)]\tLoss: 30.216972\n",
      "Train Epoch: 317 [3840/8000 (48%)]\tLoss: 30.310865\n",
      "Train Epoch: 317 [5120/8000 (64%)]\tLoss: 30.312553\n",
      "Train Epoch: 317 [6400/8000 (80%)]\tLoss: 30.139635\n",
      "Train Epoch: 317 [7680/8000 (96%)]\tLoss: 30.196772\n",
      "Loss tensor(1923.5729, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1311.7517, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 317 Average loss: 30.1271\n",
      "Train Epoch: 318 [0/8000 (0%)]\tLoss: 29.917751\n",
      "Loss tensor(1931.9768, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1316.4834, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 318 [1280/8000 (16%)]\tLoss: 30.249212\n",
      "Loss tensor(1940.5216, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1319.2563, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 318 [2560/8000 (32%)]\tLoss: 30.332979\n",
      "Train Epoch: 318 [3840/8000 (48%)]\tLoss: 29.830057\n",
      "Train Epoch: 318 [5120/8000 (64%)]\tLoss: 30.360989\n",
      "Train Epoch: 318 [6400/8000 (80%)]\tLoss: 30.263262\n",
      "Loss tensor(1906.6644, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1323.1843, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 318 [7680/8000 (96%)]\tLoss: 30.063765\n",
      "====> Epoch: 318 Average loss: 30.1422\n",
      "Train Epoch: 319 [0/8000 (0%)]\tLoss: 30.187176\n",
      "Train Epoch: 319 [1280/8000 (16%)]\tLoss: 30.456999\n",
      "Train Epoch: 319 [2560/8000 (32%)]\tLoss: 29.915110\n",
      "Train Epoch: 319 [3840/8000 (48%)]\tLoss: 29.699890\n",
      "Train Epoch: 319 [5120/8000 (64%)]\tLoss: 30.454044\n",
      "Loss tensor(1911.7745, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1351.6499, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 319 [6400/8000 (80%)]\tLoss: 30.001003\n",
      "Train Epoch: 319 [7680/8000 (96%)]\tLoss: 30.059912\n",
      "====> Epoch: 319 Average loss: 30.1225\n",
      "Train Epoch: 320 [0/8000 (0%)]\tLoss: 30.369421\n",
      "Train Epoch: 320 [1280/8000 (16%)]\tLoss: 30.030354\n",
      "Train Epoch: 320 [2560/8000 (32%)]\tLoss: 30.121035\n",
      "Loss tensor(1921.6511, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1366.4418, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 320 [3840/8000 (48%)]\tLoss: 30.178957\n",
      "Train Epoch: 320 [5120/8000 (64%)]\tLoss: 29.843594\n",
      "Train Epoch: 320 [6400/8000 (80%)]\tLoss: 30.257242\n",
      "Loss tensor(1913.0359, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1362.8674, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 320 [7680/8000 (96%)]\tLoss: 29.994884\n",
      "====> Epoch: 320 Average loss: 30.1231\n",
      "Train Epoch: 321 [0/8000 (0%)]\tLoss: 30.142460\n",
      "Train Epoch: 321 [1280/8000 (16%)]\tLoss: 30.032862\n",
      "Train Epoch: 321 [2560/8000 (32%)]\tLoss: 29.933399\n",
      "Train Epoch: 321 [3840/8000 (48%)]\tLoss: 29.888618\n",
      "Loss tensor(1934.7017, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1381.2506, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 321 [5120/8000 (64%)]\tLoss: 30.388941\n",
      "Loss tensor(1928.8901, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1383.2083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 321 [6400/8000 (80%)]\tLoss: 30.107054\n",
      "Train Epoch: 321 [7680/8000 (96%)]\tLoss: 30.163315\n",
      "====> Epoch: 321 Average loss: 30.1087\n",
      "Train Epoch: 322 [0/8000 (0%)]\tLoss: 29.978451\n",
      "Train Epoch: 322 [1280/8000 (16%)]\tLoss: 29.908657\n",
      "Train Epoch: 322 [2560/8000 (32%)]\tLoss: 29.851349\n",
      "Train Epoch: 322 [3840/8000 (48%)]\tLoss: 29.843904\n",
      "Train Epoch: 322 [5120/8000 (64%)]\tLoss: 30.028360\n",
      "Train Epoch: 322 [6400/8000 (80%)]\tLoss: 30.355761\n",
      "Train Epoch: 322 [7680/8000 (96%)]\tLoss: 30.078915\n",
      "====> Epoch: 322 Average loss: 30.1353\n",
      "Train Epoch: 323 [0/8000 (0%)]\tLoss: 30.175564\n",
      "Train Epoch: 323 [1280/8000 (16%)]\tLoss: 29.824921\n",
      "Train Epoch: 323 [2560/8000 (32%)]\tLoss: 30.164488\n",
      "Train Epoch: 323 [3840/8000 (48%)]\tLoss: 30.134319\n",
      "Train Epoch: 323 [5120/8000 (64%)]\tLoss: 30.107796\n",
      "Train Epoch: 323 [6400/8000 (80%)]\tLoss: 30.046791\n",
      "Train Epoch: 323 [7680/8000 (96%)]\tLoss: 30.081152\n",
      "====> Epoch: 323 Average loss: 30.1271\n",
      "Train Epoch: 324 [0/8000 (0%)]\tLoss: 29.880514\n",
      "Loss tensor(1922.7715, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1384.1643, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1943.5919, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1385.7585, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 324 [1280/8000 (16%)]\tLoss: 30.441189\n",
      "Train Epoch: 324 [2560/8000 (32%)]\tLoss: 30.020414\n",
      "Train Epoch: 324 [3840/8000 (48%)]\tLoss: 30.204094\n",
      "Train Epoch: 324 [5120/8000 (64%)]\tLoss: 29.971792\n",
      "Train Epoch: 324 [6400/8000 (80%)]\tLoss: 29.846060\n",
      "Train Epoch: 324 [7680/8000 (96%)]\tLoss: 29.817867\n",
      "====> Epoch: 324 Average loss: 30.1207\n",
      "Train Epoch: 325 [0/8000 (0%)]\tLoss: 30.272867\n",
      "Train Epoch: 325 [1280/8000 (16%)]\tLoss: 30.077652\n",
      "Train Epoch: 325 [2560/8000 (32%)]\tLoss: 29.696362\n",
      "Loss tensor(1919.6514, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1378.3894, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 325 [3840/8000 (48%)]\tLoss: 30.052757\n",
      "Loss tensor(1938.9352, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1379.6611, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 325 [5120/8000 (64%)]\tLoss: 30.167444\n",
      "Loss tensor(1922.6732, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1387.2278, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 325 [6400/8000 (80%)]\tLoss: 30.454466\n",
      "Loss tensor(1903.9624, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1384.7528, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 325 [7680/8000 (96%)]\tLoss: 30.046358\n",
      "====> Epoch: 325 Average loss: 30.1308\n",
      "Train Epoch: 326 [0/8000 (0%)]\tLoss: 30.099091\n",
      "Train Epoch: 326 [1280/8000 (16%)]\tLoss: 30.418142\n",
      "Train Epoch: 326 [2560/8000 (32%)]\tLoss: 30.124542\n",
      "Train Epoch: 326 [3840/8000 (48%)]\tLoss: 30.059160\n",
      "Train Epoch: 326 [5120/8000 (64%)]\tLoss: 30.444557\n",
      "Train Epoch: 326 [6400/8000 (80%)]\tLoss: 30.115057\n",
      "Train Epoch: 326 [7680/8000 (96%)]\tLoss: 30.227924\n",
      "Loss tensor(1933.4548, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1406.8794, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 326 Average loss: 30.1403\n",
      "Train Epoch: 327 [0/8000 (0%)]\tLoss: 29.638103\n",
      "Train Epoch: 327 [1280/8000 (16%)]\tLoss: 30.029711\n",
      "Train Epoch: 327 [2560/8000 (32%)]\tLoss: 30.078857\n",
      "Loss tensor(1941.1213, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1403.4504, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 327 [3840/8000 (48%)]\tLoss: 29.986151\n",
      "Train Epoch: 327 [5120/8000 (64%)]\tLoss: 30.269371\n",
      "Train Epoch: 327 [6400/8000 (80%)]\tLoss: 29.911942\n",
      "Train Epoch: 327 [7680/8000 (96%)]\tLoss: 30.284370\n",
      "====> Epoch: 327 Average loss: 30.1263\n",
      "Train Epoch: 328 [0/8000 (0%)]\tLoss: 30.188667\n",
      "Loss tensor(1936.0961, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1407.7236, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 328 [1280/8000 (16%)]\tLoss: 30.227667\n",
      "Loss tensor(1918.9148, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1415.2129, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 328 [2560/8000 (32%)]\tLoss: 29.908464\n",
      "Train Epoch: 328 [3840/8000 (48%)]\tLoss: 30.089912\n",
      "Train Epoch: 328 [5120/8000 (64%)]\tLoss: 30.310581\n",
      "Loss tensor(1918.2177, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1451.0281, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1909.0061, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1441.6294, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 328 [6400/8000 (80%)]\tLoss: 30.335363\n",
      "Train Epoch: 328 [7680/8000 (96%)]\tLoss: 30.096008\n",
      "====> Epoch: 328 Average loss: 30.1460\n",
      "Train Epoch: 329 [0/8000 (0%)]\tLoss: 30.093508\n",
      "Train Epoch: 329 [1280/8000 (16%)]\tLoss: 30.023697\n",
      "Train Epoch: 329 [2560/8000 (32%)]\tLoss: 30.394247\n",
      "Train Epoch: 329 [3840/8000 (48%)]\tLoss: 29.946985\n",
      "Train Epoch: 329 [5120/8000 (64%)]\tLoss: 30.172867\n",
      "Loss tensor(1930.4459, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1454.3481, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 329 [6400/8000 (80%)]\tLoss: 30.087614\n",
      "Train Epoch: 329 [7680/8000 (96%)]\tLoss: 30.068819\n",
      "====> Epoch: 329 Average loss: 30.1328\n",
      "Train Epoch: 330 [0/8000 (0%)]\tLoss: 29.991936\n",
      "Train Epoch: 330 [1280/8000 (16%)]\tLoss: 30.207005\n",
      "Train Epoch: 330 [2560/8000 (32%)]\tLoss: 30.469526\n",
      "Train Epoch: 330 [3840/8000 (48%)]\tLoss: 29.974030\n",
      "Train Epoch: 330 [5120/8000 (64%)]\tLoss: 30.235144\n",
      "Train Epoch: 330 [6400/8000 (80%)]\tLoss: 30.338949\n",
      "Train Epoch: 330 [7680/8000 (96%)]\tLoss: 30.323833\n",
      "====> Epoch: 330 Average loss: 30.1288\n",
      "Train Epoch: 331 [0/8000 (0%)]\tLoss: 30.133522\n",
      "Train Epoch: 331 [1280/8000 (16%)]\tLoss: 30.026436\n",
      "Loss tensor(1925.1320, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1456.8621, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 331 [2560/8000 (32%)]\tLoss: 30.327446\n",
      "Train Epoch: 331 [3840/8000 (48%)]\tLoss: 30.292879\n",
      "Loss tensor(1913.6863, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1450.7406, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 331 [5120/8000 (64%)]\tLoss: 30.030388\n",
      "Train Epoch: 331 [6400/8000 (80%)]\tLoss: 29.995613\n",
      "Train Epoch: 331 [7680/8000 (96%)]\tLoss: 29.568275\n",
      "====> Epoch: 331 Average loss: 30.1363\n",
      "Train Epoch: 332 [0/8000 (0%)]\tLoss: 30.267612\n",
      "Train Epoch: 332 [1280/8000 (16%)]\tLoss: 30.116535\n",
      "Train Epoch: 332 [2560/8000 (32%)]\tLoss: 30.106791\n",
      "Train Epoch: 332 [3840/8000 (48%)]\tLoss: 30.347998\n",
      "Train Epoch: 332 [5120/8000 (64%)]\tLoss: 30.309338\n",
      "Loss tensor(1925.7726, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1466.6538, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 332 [6400/8000 (80%)]\tLoss: 29.966469\n",
      "Train Epoch: 332 [7680/8000 (96%)]\tLoss: 30.075415\n",
      "====> Epoch: 332 Average loss: 30.1187\n",
      "Train Epoch: 333 [0/8000 (0%)]\tLoss: 29.832352\n",
      "Loss tensor(1934.5850, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1458.7332, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 333 [1280/8000 (16%)]\tLoss: 30.083954\n",
      "Train Epoch: 333 [2560/8000 (32%)]\tLoss: 30.050255\n",
      "Train Epoch: 333 [3840/8000 (48%)]\tLoss: 29.989010\n",
      "Train Epoch: 333 [5120/8000 (64%)]\tLoss: 30.251019\n",
      "Train Epoch: 333 [6400/8000 (80%)]\tLoss: 29.968330\n",
      "Train Epoch: 333 [7680/8000 (96%)]\tLoss: 30.013208\n",
      "====> Epoch: 333 Average loss: 30.1007\n",
      "Train Epoch: 334 [0/8000 (0%)]\tLoss: 30.163897\n",
      "Loss tensor(1936.6539, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1474.2092, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 334 [1280/8000 (16%)]\tLoss: 30.144997\n",
      "Train Epoch: 334 [2560/8000 (32%)]\tLoss: 30.120258\n",
      "Train Epoch: 334 [3840/8000 (48%)]\tLoss: 30.141724\n",
      "Train Epoch: 334 [5120/8000 (64%)]\tLoss: 30.128731\n",
      "Train Epoch: 334 [6400/8000 (80%)]\tLoss: 29.968962\n",
      "Train Epoch: 334 [7680/8000 (96%)]\tLoss: 30.261669\n",
      "====> Epoch: 334 Average loss: 30.1168\n",
      "Train Epoch: 335 [0/8000 (0%)]\tLoss: 30.204473\n",
      "Train Epoch: 335 [1280/8000 (16%)]\tLoss: 30.233009\n",
      "Train Epoch: 335 [2560/8000 (32%)]\tLoss: 30.510666\n",
      "Train Epoch: 335 [3840/8000 (48%)]\tLoss: 30.045748\n",
      "Loss tensor(1908.8518, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1467.5948, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 335 [5120/8000 (64%)]\tLoss: 30.368584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 335 [6400/8000 (80%)]\tLoss: 29.937222\n",
      "Loss tensor(1928.4050, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1475.5339, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 335 [7680/8000 (96%)]\tLoss: 30.131329\n",
      "====> Epoch: 335 Average loss: 30.1111\n",
      "Train Epoch: 336 [0/8000 (0%)]\tLoss: 30.292023\n",
      "Train Epoch: 336 [1280/8000 (16%)]\tLoss: 29.852264\n",
      "Train Epoch: 336 [2560/8000 (32%)]\tLoss: 29.827929\n",
      "Train Epoch: 336 [3840/8000 (48%)]\tLoss: 29.765379\n",
      "Train Epoch: 336 [5120/8000 (64%)]\tLoss: 30.225393\n",
      "Loss tensor(1917.3633, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1471.8630, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 336 [6400/8000 (80%)]\tLoss: 30.516186\n",
      "Train Epoch: 336 [7680/8000 (96%)]\tLoss: 29.959908\n",
      "====> Epoch: 336 Average loss: 30.1138\n",
      "Train Epoch: 337 [0/8000 (0%)]\tLoss: 30.169031\n",
      "Train Epoch: 337 [1280/8000 (16%)]\tLoss: 29.975405\n",
      "Train Epoch: 337 [2560/8000 (32%)]\tLoss: 29.859886\n",
      "Train Epoch: 337 [3840/8000 (48%)]\tLoss: 30.104511\n",
      "Train Epoch: 337 [5120/8000 (64%)]\tLoss: 30.338194\n",
      "Loss tensor(1930.1069, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1473.2871, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 337 [6400/8000 (80%)]\tLoss: 29.990957\n",
      "Train Epoch: 337 [7680/8000 (96%)]\tLoss: 29.751598\n",
      "====> Epoch: 337 Average loss: 30.1201\n",
      "Train Epoch: 338 [0/8000 (0%)]\tLoss: 29.892147\n",
      "Train Epoch: 338 [1280/8000 (16%)]\tLoss: 30.216389\n",
      "Loss tensor(1934.9213, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1454.0574, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 338 [2560/8000 (32%)]\tLoss: 30.136566\n",
      "Train Epoch: 338 [3840/8000 (48%)]\tLoss: 30.264250\n",
      "Train Epoch: 338 [5120/8000 (64%)]\tLoss: 30.000181\n",
      "Train Epoch: 338 [6400/8000 (80%)]\tLoss: 30.285496\n",
      "Loss tensor(1925.8840, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1447.5052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 338 [7680/8000 (96%)]\tLoss: 30.054333\n",
      "====> Epoch: 338 Average loss: 30.1349\n",
      "Train Epoch: 339 [0/8000 (0%)]\tLoss: 30.384705\n",
      "Train Epoch: 339 [1280/8000 (16%)]\tLoss: 29.934032\n",
      "Train Epoch: 339 [2560/8000 (32%)]\tLoss: 29.998255\n",
      "Train Epoch: 339 [3840/8000 (48%)]\tLoss: 30.092087\n",
      "Train Epoch: 339 [5120/8000 (64%)]\tLoss: 30.015825\n",
      "Train Epoch: 339 [6400/8000 (80%)]\tLoss: 30.235857\n",
      "Train Epoch: 339 [7680/8000 (96%)]\tLoss: 30.080402\n",
      "====> Epoch: 339 Average loss: 30.1039\n",
      "Train Epoch: 340 [0/8000 (0%)]\tLoss: 30.106409\n",
      "Train Epoch: 340 [1280/8000 (16%)]\tLoss: 30.086636\n",
      "Loss tensor(1947.7045, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1449.4548, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 340 [2560/8000 (32%)]\tLoss: 30.365976\n",
      "Train Epoch: 340 [3840/8000 (48%)]\tLoss: 30.031940\n",
      "Train Epoch: 340 [5120/8000 (64%)]\tLoss: 30.327095\n",
      "Train Epoch: 340 [6400/8000 (80%)]\tLoss: 29.822382\n",
      "Train Epoch: 340 [7680/8000 (96%)]\tLoss: 30.183691\n",
      "====> Epoch: 340 Average loss: 30.1042\n",
      "Train Epoch: 341 [0/8000 (0%)]\tLoss: 30.412790\n",
      "Train Epoch: 341 [1280/8000 (16%)]\tLoss: 30.064939\n",
      "Train Epoch: 341 [2560/8000 (32%)]\tLoss: 29.995380\n",
      "Loss tensor(1925.3116, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1448.5057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 341 [3840/8000 (48%)]\tLoss: 30.565113\n",
      "Train Epoch: 341 [5120/8000 (64%)]\tLoss: 30.121782\n",
      "Train Epoch: 341 [6400/8000 (80%)]\tLoss: 30.273117\n",
      "Loss tensor(1919.9824, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1453.0951, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 341 [7680/8000 (96%)]\tLoss: 29.858007\n",
      "====> Epoch: 341 Average loss: 30.1168\n",
      "Train Epoch: 342 [0/8000 (0%)]\tLoss: 30.122156\n",
      "Train Epoch: 342 [1280/8000 (16%)]\tLoss: 30.005547\n",
      "Train Epoch: 342 [2560/8000 (32%)]\tLoss: 30.047890\n",
      "Loss tensor(1928.1467, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1452.2839, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 342 [3840/8000 (48%)]\tLoss: 30.021759\n",
      "Train Epoch: 342 [5120/8000 (64%)]\tLoss: 30.318237\n",
      "Loss tensor(1924.5110, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1449.9579, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 342 [6400/8000 (80%)]\tLoss: 29.994886\n",
      "Train Epoch: 342 [7680/8000 (96%)]\tLoss: 30.270687\n",
      "====> Epoch: 342 Average loss: 30.0962\n",
      "Train Epoch: 343 [0/8000 (0%)]\tLoss: 29.992531\n",
      "Train Epoch: 343 [1280/8000 (16%)]\tLoss: 30.243708\n",
      "Train Epoch: 343 [2560/8000 (32%)]\tLoss: 29.911283\n",
      "Train Epoch: 343 [3840/8000 (48%)]\tLoss: 30.107769\n",
      "Train Epoch: 343 [5120/8000 (64%)]\tLoss: 30.250471\n",
      "Train Epoch: 343 [6400/8000 (80%)]\tLoss: 30.071281\n",
      "Train Epoch: 343 [7680/8000 (96%)]\tLoss: 30.351326\n",
      "====> Epoch: 343 Average loss: 30.1088\n",
      "Train Epoch: 344 [0/8000 (0%)]\tLoss: 30.033958\n",
      "Train Epoch: 344 [1280/8000 (16%)]\tLoss: 30.038038\n",
      "Train Epoch: 344 [2560/8000 (32%)]\tLoss: 30.295033\n",
      "Train Epoch: 344 [3840/8000 (48%)]\tLoss: 30.291164\n",
      "Train Epoch: 344 [5120/8000 (64%)]\tLoss: 30.040129\n",
      "Loss tensor(1931.9036, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1441.5813, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 344 [6400/8000 (80%)]\tLoss: 30.297249\n",
      "Train Epoch: 344 [7680/8000 (96%)]\tLoss: 29.853016\n",
      "====> Epoch: 344 Average loss: 30.1014\n",
      "Train Epoch: 345 [0/8000 (0%)]\tLoss: 29.675035\n",
      "Train Epoch: 345 [1280/8000 (16%)]\tLoss: 29.976641\n",
      "Train Epoch: 345 [2560/8000 (32%)]\tLoss: 29.989162\n",
      "Train Epoch: 345 [3840/8000 (48%)]\tLoss: 29.800440\n",
      "Train Epoch: 345 [5120/8000 (64%)]\tLoss: 29.953333\n",
      "Loss tensor(1958.4437, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1456.8125, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1928.2289, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1454.9071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 345 [6400/8000 (80%)]\tLoss: 29.948978\n",
      "Train Epoch: 345 [7680/8000 (96%)]\tLoss: 30.108002\n",
      "====> Epoch: 345 Average loss: 30.1108\n",
      "Train Epoch: 346 [0/8000 (0%)]\tLoss: 30.143047\n",
      "Train Epoch: 346 [1280/8000 (16%)]\tLoss: 30.419485\n",
      "Train Epoch: 346 [2560/8000 (32%)]\tLoss: 29.893318\n",
      "Train Epoch: 346 [3840/8000 (48%)]\tLoss: 30.284422\n",
      "Train Epoch: 346 [5120/8000 (64%)]\tLoss: 30.622765\n",
      "Train Epoch: 346 [6400/8000 (80%)]\tLoss: 29.827927\n",
      "Train Epoch: 346 [7680/8000 (96%)]\tLoss: 30.068407\n",
      "====> Epoch: 346 Average loss: 30.0965\n",
      "Train Epoch: 347 [0/8000 (0%)]\tLoss: 30.142424\n",
      "Train Epoch: 347 [1280/8000 (16%)]\tLoss: 30.128757\n",
      "Train Epoch: 347 [2560/8000 (32%)]\tLoss: 30.029789\n",
      "Train Epoch: 347 [3840/8000 (48%)]\tLoss: 30.350090\n",
      "Train Epoch: 347 [5120/8000 (64%)]\tLoss: 30.156389\n",
      "Train Epoch: 347 [6400/8000 (80%)]\tLoss: 30.117245\n",
      "Train Epoch: 347 [7680/8000 (96%)]\tLoss: 29.880524\n",
      "====> Epoch: 347 Average loss: 30.0925\n",
      "Train Epoch: 348 [0/8000 (0%)]\tLoss: 30.133261\n",
      "Train Epoch: 348 [1280/8000 (16%)]\tLoss: 30.184532\n",
      "Train Epoch: 348 [2560/8000 (32%)]\tLoss: 29.936550\n",
      "Train Epoch: 348 [3840/8000 (48%)]\tLoss: 29.988178\n",
      "Train Epoch: 348 [5120/8000 (64%)]\tLoss: 30.444559\n",
      "Loss tensor(1929.2820, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1455.6075, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 348 [6400/8000 (80%)]\tLoss: 29.988682\n",
      "Train Epoch: 348 [7680/8000 (96%)]\tLoss: 29.915146\n",
      "====> Epoch: 348 Average loss: 30.1010\n",
      "Train Epoch: 349 [0/8000 (0%)]\tLoss: 30.112988\n",
      "Train Epoch: 349 [1280/8000 (16%)]\tLoss: 29.801834\n",
      "Train Epoch: 349 [2560/8000 (32%)]\tLoss: 30.003399\n",
      "Train Epoch: 349 [3840/8000 (48%)]\tLoss: 30.195536\n",
      "Train Epoch: 349 [5120/8000 (64%)]\tLoss: 29.908098\n",
      "Train Epoch: 349 [6400/8000 (80%)]\tLoss: 29.758324\n",
      "Train Epoch: 349 [7680/8000 (96%)]\tLoss: 30.048197\n",
      "Loss tensor(1952.1689, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1456.2727, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 349 Average loss: 30.0920\n",
      "Train Epoch: 350 [0/8000 (0%)]\tLoss: 29.946905\n",
      "Train Epoch: 350 [1280/8000 (16%)]\tLoss: 30.409159\n",
      "Loss tensor(1933.2767, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1456.9963, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 350 [2560/8000 (32%)]\tLoss: 29.802948\n",
      "Loss tensor(1928.5005, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1445.4839, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 350 [3840/8000 (48%)]\tLoss: 30.186853\n",
      "Train Epoch: 350 [5120/8000 (64%)]\tLoss: 29.722069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 350 [6400/8000 (80%)]\tLoss: 30.108925\n",
      "Train Epoch: 350 [7680/8000 (96%)]\tLoss: 30.258886\n",
      "====> Epoch: 350 Average loss: 30.0949\n",
      "Train Epoch: 351 [0/8000 (0%)]\tLoss: 30.807604\n",
      "Train Epoch: 351 [1280/8000 (16%)]\tLoss: 30.007023\n",
      "Train Epoch: 351 [2560/8000 (32%)]\tLoss: 29.996511\n",
      "Train Epoch: 351 [3840/8000 (48%)]\tLoss: 30.276722\n",
      "Loss tensor(1921.8145, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1454.3862, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 351 [5120/8000 (64%)]\tLoss: 30.198034\n",
      "Train Epoch: 351 [6400/8000 (80%)]\tLoss: 30.352083\n",
      "Train Epoch: 351 [7680/8000 (96%)]\tLoss: 29.829096\n",
      "====> Epoch: 351 Average loss: 30.1079\n",
      "Train Epoch: 352 [0/8000 (0%)]\tLoss: 29.802042\n",
      "Train Epoch: 352 [1280/8000 (16%)]\tLoss: 29.957630\n",
      "Loss tensor(1930.9742, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1474.2903, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 352 [2560/8000 (32%)]\tLoss: 30.051308\n",
      "Loss tensor(1925.3994, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1472.5874, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 352 [3840/8000 (48%)]\tLoss: 30.279385\n",
      "Train Epoch: 352 [5120/8000 (64%)]\tLoss: 29.848528\n",
      "Train Epoch: 352 [6400/8000 (80%)]\tLoss: 30.045300\n",
      "Loss tensor(1931.2633, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1480.1453, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1950.3451, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1480.5049, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 352 [7680/8000 (96%)]\tLoss: 30.474142\n",
      "====> Epoch: 352 Average loss: 30.0965\n",
      "Train Epoch: 353 [0/8000 (0%)]\tLoss: 30.236340\n",
      "Train Epoch: 353 [1280/8000 (16%)]\tLoss: 30.184870\n",
      "Train Epoch: 353 [2560/8000 (32%)]\tLoss: 29.716478\n",
      "Train Epoch: 353 [3840/8000 (48%)]\tLoss: 30.102770\n",
      "Train Epoch: 353 [5120/8000 (64%)]\tLoss: 30.007690\n",
      "Train Epoch: 353 [6400/8000 (80%)]\tLoss: 30.094934\n",
      "Train Epoch: 353 [7680/8000 (96%)]\tLoss: 30.137257\n",
      "====> Epoch: 353 Average loss: 30.0978\n",
      "Train Epoch: 354 [0/8000 (0%)]\tLoss: 30.006666\n",
      "Train Epoch: 354 [1280/8000 (16%)]\tLoss: 29.909893\n",
      "Train Epoch: 354 [2560/8000 (32%)]\tLoss: 29.873102\n",
      "Loss tensor(1908.1621, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1496.5145, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1918.8286, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1490.9226, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 354 [3840/8000 (48%)]\tLoss: 29.588865\n",
      "Train Epoch: 354 [5120/8000 (64%)]\tLoss: 30.013586\n",
      "Loss tensor(1915.2527, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1485.5906, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 354 [6400/8000 (80%)]\tLoss: 30.023560\n",
      "Train Epoch: 354 [7680/8000 (96%)]\tLoss: 30.216307\n",
      "Loss tensor(1927.1852, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1493.3408, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 354 Average loss: 30.0908\n",
      "Train Epoch: 355 [0/8000 (0%)]\tLoss: 30.024672\n",
      "Train Epoch: 355 [1280/8000 (16%)]\tLoss: 30.020477\n",
      "Loss tensor(1916.3182, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1487.9885, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 355 [2560/8000 (32%)]\tLoss: 29.942472\n",
      "Train Epoch: 355 [3840/8000 (48%)]\tLoss: 30.143108\n",
      "Train Epoch: 355 [5120/8000 (64%)]\tLoss: 30.270987\n",
      "Loss tensor(1917.7686, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1469.5178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 355 [6400/8000 (80%)]\tLoss: 30.334930\n",
      "Train Epoch: 355 [7680/8000 (96%)]\tLoss: 30.021803\n",
      "====> Epoch: 355 Average loss: 30.1125\n",
      "Train Epoch: 356 [0/8000 (0%)]\tLoss: 30.067419\n",
      "Train Epoch: 356 [1280/8000 (16%)]\tLoss: 30.148701\n",
      "Loss tensor(1923.8790, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1444.0643, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1936.3995, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1442.6680, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 356 [2560/8000 (32%)]\tLoss: 30.096979\n",
      "Train Epoch: 356 [3840/8000 (48%)]\tLoss: 30.116028\n",
      "Loss tensor(1938.3035, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1436.4346, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 356 [5120/8000 (64%)]\tLoss: 30.285992\n",
      "Train Epoch: 356 [6400/8000 (80%)]\tLoss: 30.032949\n",
      "Train Epoch: 356 [7680/8000 (96%)]\tLoss: 29.799858\n",
      "====> Epoch: 356 Average loss: 30.0876\n",
      "Train Epoch: 357 [0/8000 (0%)]\tLoss: 30.143784\n",
      "Train Epoch: 357 [1280/8000 (16%)]\tLoss: 30.039948\n",
      "Train Epoch: 357 [2560/8000 (32%)]\tLoss: 30.486290\n",
      "Loss tensor(1915.9951, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1478.5068, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 357 [3840/8000 (48%)]\tLoss: 30.125057\n",
      "Train Epoch: 357 [5120/8000 (64%)]\tLoss: 30.038723\n",
      "Train Epoch: 357 [6400/8000 (80%)]\tLoss: 30.277273\n",
      "Loss tensor(1926.8900, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1487.4431, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 357 [7680/8000 (96%)]\tLoss: 29.980793\n",
      "====> Epoch: 357 Average loss: 30.0825\n",
      "Train Epoch: 358 [0/8000 (0%)]\tLoss: 29.992090\n",
      "Loss tensor(1926.2502, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1480.6797, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 358 [1280/8000 (16%)]\tLoss: 30.389692\n",
      "Train Epoch: 358 [2560/8000 (32%)]\tLoss: 30.080784\n",
      "Train Epoch: 358 [3840/8000 (48%)]\tLoss: 29.628376\n",
      "Loss tensor(1922.3467, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1484.4436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 358 [5120/8000 (64%)]\tLoss: 30.036667\n",
      "Train Epoch: 358 [6400/8000 (80%)]\tLoss: 29.974199\n",
      "Train Epoch: 358 [7680/8000 (96%)]\tLoss: 30.255119\n",
      "====> Epoch: 358 Average loss: 30.0783\n",
      "Train Epoch: 359 [0/8000 (0%)]\tLoss: 30.015884\n",
      "Train Epoch: 359 [1280/8000 (16%)]\tLoss: 29.923645\n",
      "Train Epoch: 359 [2560/8000 (32%)]\tLoss: 30.098045\n",
      "Train Epoch: 359 [3840/8000 (48%)]\tLoss: 29.934212\n",
      "Train Epoch: 359 [5120/8000 (64%)]\tLoss: 29.902180\n",
      "Train Epoch: 359 [6400/8000 (80%)]\tLoss: 30.375246\n",
      "Train Epoch: 359 [7680/8000 (96%)]\tLoss: 29.783209\n",
      "====> Epoch: 359 Average loss: 30.0878\n",
      "Train Epoch: 360 [0/8000 (0%)]\tLoss: 29.960840\n",
      "Train Epoch: 360 [1280/8000 (16%)]\tLoss: 30.255718\n",
      "Loss tensor(1927.5907, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1472.7317, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 360 [2560/8000 (32%)]\tLoss: 30.164913\n",
      "Train Epoch: 360 [3840/8000 (48%)]\tLoss: 29.896713\n",
      "Train Epoch: 360 [5120/8000 (64%)]\tLoss: 30.234894\n",
      "Train Epoch: 360 [6400/8000 (80%)]\tLoss: 30.206774\n",
      "Train Epoch: 360 [7680/8000 (96%)]\tLoss: 30.205950\n",
      "====> Epoch: 360 Average loss: 30.0895\n",
      "Train Epoch: 361 [0/8000 (0%)]\tLoss: 30.071461\n",
      "Train Epoch: 361 [1280/8000 (16%)]\tLoss: 30.096605\n",
      "Train Epoch: 361 [2560/8000 (32%)]\tLoss: 29.821363\n",
      "Train Epoch: 361 [3840/8000 (48%)]\tLoss: 29.995201\n",
      "Train Epoch: 361 [5120/8000 (64%)]\tLoss: 30.008175\n",
      "Train Epoch: 361 [6400/8000 (80%)]\tLoss: 30.190361\n",
      "Loss tensor(1933.5430, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1457.4797, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 361 [7680/8000 (96%)]\tLoss: 30.070683\n",
      "====> Epoch: 361 Average loss: 30.0860\n",
      "Train Epoch: 362 [0/8000 (0%)]\tLoss: 30.058413\n",
      "Loss tensor(1911.2504, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1457.0576, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 362 [1280/8000 (16%)]\tLoss: 30.246229\n",
      "Train Epoch: 362 [2560/8000 (32%)]\tLoss: 30.088638\n",
      "Train Epoch: 362 [3840/8000 (48%)]\tLoss: 30.236338\n",
      "Train Epoch: 362 [5120/8000 (64%)]\tLoss: 29.874281\n",
      "Train Epoch: 362 [6400/8000 (80%)]\tLoss: 29.974060\n",
      "Loss tensor(1920.0027, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1440.1986, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 362 [7680/8000 (96%)]\tLoss: 30.620722\n",
      "====> Epoch: 362 Average loss: 30.0839\n",
      "Train Epoch: 363 [0/8000 (0%)]\tLoss: 30.199459\n",
      "Train Epoch: 363 [1280/8000 (16%)]\tLoss: 30.368401\n",
      "Loss tensor(1902.6307, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1444.1927, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 363 [2560/8000 (32%)]\tLoss: 29.902922\n",
      "Train Epoch: 363 [3840/8000 (48%)]\tLoss: 30.155188\n",
      "Loss tensor(1938.4612, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1436.3057, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1927.0688, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1437.9586, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 363 [5120/8000 (64%)]\tLoss: 30.336346\n",
      "Train Epoch: 363 [6400/8000 (80%)]\tLoss: 30.266354\n",
      "Train Epoch: 363 [7680/8000 (96%)]\tLoss: 30.016228\n",
      "====> Epoch: 363 Average loss: 30.0792\n",
      "Train Epoch: 364 [0/8000 (0%)]\tLoss: 30.156174\n",
      "Train Epoch: 364 [1280/8000 (16%)]\tLoss: 30.042982\n",
      "Train Epoch: 364 [2560/8000 (32%)]\tLoss: 29.948626\n",
      "Train Epoch: 364 [3840/8000 (48%)]\tLoss: 30.504490\n",
      "Train Epoch: 364 [5120/8000 (64%)]\tLoss: 29.978651\n",
      "Loss tensor(1932.8716, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1436.0111, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 364 [6400/8000 (80%)]\tLoss: 29.982264\n",
      "Train Epoch: 364 [7680/8000 (96%)]\tLoss: 30.372408\n",
      "====> Epoch: 364 Average loss: 30.0701\n",
      "Train Epoch: 365 [0/8000 (0%)]\tLoss: 30.435780\n",
      "Loss tensor(1921.7590, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1445.5000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 365 [1280/8000 (16%)]\tLoss: 29.866568\n",
      "Train Epoch: 365 [2560/8000 (32%)]\tLoss: 29.952196\n",
      "Loss tensor(1930.9186, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1448.3317, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 365 [3840/8000 (48%)]\tLoss: 30.145224\n",
      "Loss tensor(1911.8892, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1453.5730, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1928.0183, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1444.9259, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 365 [5120/8000 (64%)]\tLoss: 30.140709\n",
      "Train Epoch: 365 [6400/8000 (80%)]\tLoss: 30.450838\n",
      "Train Epoch: 365 [7680/8000 (96%)]\tLoss: 29.430046\n",
      "====> Epoch: 365 Average loss: 30.0705\n",
      "Train Epoch: 366 [0/8000 (0%)]\tLoss: 29.943291\n",
      "Train Epoch: 366 [1280/8000 (16%)]\tLoss: 30.317297\n",
      "Loss tensor(1913.0980, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1443.3638, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 366 [2560/8000 (32%)]\tLoss: 29.853432\n",
      "Train Epoch: 366 [3840/8000 (48%)]\tLoss: 29.989416\n",
      "Train Epoch: 366 [5120/8000 (64%)]\tLoss: 29.867617\n",
      "Train Epoch: 366 [6400/8000 (80%)]\tLoss: 30.011332\n",
      "Loss tensor(1904.3717, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1445.6324, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 366 [7680/8000 (96%)]\tLoss: 30.167614\n",
      "====> Epoch: 366 Average loss: 30.0718\n",
      "Train Epoch: 367 [0/8000 (0%)]\tLoss: 29.882877\n",
      "Train Epoch: 367 [1280/8000 (16%)]\tLoss: 30.009298\n",
      "Loss tensor(1939.4679, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1445.2762, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1911.1967, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1439.5554, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 367 [2560/8000 (32%)]\tLoss: 29.974216\n",
      "Train Epoch: 367 [3840/8000 (48%)]\tLoss: 30.078341\n",
      "Loss tensor(1917.9976, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1433.1169, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 367 [5120/8000 (64%)]\tLoss: 30.181940\n",
      "Train Epoch: 367 [6400/8000 (80%)]\tLoss: 30.112034\n",
      "Loss tensor(1900.4946, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1435.1047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 367 [7680/8000 (96%)]\tLoss: 29.981615\n",
      "Loss tensor(1910.4329, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1437.3328, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 367 Average loss: 30.0748\n",
      "Train Epoch: 368 [0/8000 (0%)]\tLoss: 30.273212\n",
      "Train Epoch: 368 [1280/8000 (16%)]\tLoss: 29.939318\n",
      "Train Epoch: 368 [2560/8000 (32%)]\tLoss: 29.852053\n",
      "Train Epoch: 368 [3840/8000 (48%)]\tLoss: 30.300476\n",
      "Train Epoch: 368 [5120/8000 (64%)]\tLoss: 30.110760\n",
      "Train Epoch: 368 [6400/8000 (80%)]\tLoss: 30.216999\n",
      "Loss tensor(1937.9216, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1417.8253, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 368 [7680/8000 (96%)]\tLoss: 30.154510\n",
      "Loss tensor(1898.3438, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1416.6846, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 368 Average loss: 30.0821\n",
      "Train Epoch: 369 [0/8000 (0%)]\tLoss: 29.955036\n",
      "Train Epoch: 369 [1280/8000 (16%)]\tLoss: 30.026814\n",
      "Train Epoch: 369 [2560/8000 (32%)]\tLoss: 30.092491\n",
      "Train Epoch: 369 [3840/8000 (48%)]\tLoss: 29.852072\n",
      "Train Epoch: 369 [5120/8000 (64%)]\tLoss: 30.125990\n",
      "Train Epoch: 369 [6400/8000 (80%)]\tLoss: 30.248182\n",
      "Train Epoch: 369 [7680/8000 (96%)]\tLoss: 30.065090\n",
      "====> Epoch: 369 Average loss: 30.0640\n",
      "Train Epoch: 370 [0/8000 (0%)]\tLoss: 29.769644\n",
      "Train Epoch: 370 [1280/8000 (16%)]\tLoss: 29.759697\n",
      "Loss tensor(1943.2930, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1427.4749, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 370 [2560/8000 (32%)]\tLoss: 30.207708\n",
      "Train Epoch: 370 [3840/8000 (48%)]\tLoss: 29.697943\n",
      "Train Epoch: 370 [5120/8000 (64%)]\tLoss: 30.545406\n",
      "Train Epoch: 370 [6400/8000 (80%)]\tLoss: 30.010241\n",
      "Train Epoch: 370 [7680/8000 (96%)]\tLoss: 30.241568\n",
      "====> Epoch: 370 Average loss: 30.0576\n",
      "Train Epoch: 371 [0/8000 (0%)]\tLoss: 30.058907\n",
      "Train Epoch: 371 [1280/8000 (16%)]\tLoss: 29.921412\n",
      "Train Epoch: 371 [2560/8000 (32%)]\tLoss: 29.842587\n",
      "Train Epoch: 371 [3840/8000 (48%)]\tLoss: 29.906750\n",
      "Loss tensor(1910.0190, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1417.4923, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 371 [5120/8000 (64%)]\tLoss: 30.099844\n",
      "Train Epoch: 371 [6400/8000 (80%)]\tLoss: 30.287470\n",
      "Train Epoch: 371 [7680/8000 (96%)]\tLoss: 30.083090\n",
      "====> Epoch: 371 Average loss: 30.0614\n",
      "Train Epoch: 372 [0/8000 (0%)]\tLoss: 30.005939\n",
      "Train Epoch: 372 [1280/8000 (16%)]\tLoss: 30.105938\n",
      "Train Epoch: 372 [2560/8000 (32%)]\tLoss: 29.973942\n",
      "Train Epoch: 372 [3840/8000 (48%)]\tLoss: 30.080061\n",
      "Loss tensor(1916.4939, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1416.0076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1924.9517, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1408.4148, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 372 [5120/8000 (64%)]\tLoss: 30.065971\n",
      "Train Epoch: 372 [6400/8000 (80%)]\tLoss: 29.763678\n",
      "Loss tensor(1923.2053, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1417.8452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1916.4344, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1416.4849, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 372 [7680/8000 (96%)]\tLoss: 30.681118\n",
      "====> Epoch: 372 Average loss: 30.0660\n",
      "Train Epoch: 373 [0/8000 (0%)]\tLoss: 30.080757\n",
      "Train Epoch: 373 [1280/8000 (16%)]\tLoss: 30.195127\n",
      "Train Epoch: 373 [2560/8000 (32%)]\tLoss: 30.079760\n",
      "Train Epoch: 373 [3840/8000 (48%)]\tLoss: 30.334248\n",
      "Train Epoch: 373 [5120/8000 (64%)]\tLoss: 30.053476\n",
      "Train Epoch: 373 [6400/8000 (80%)]\tLoss: 30.232563\n",
      "Train Epoch: 373 [7680/8000 (96%)]\tLoss: 29.935345\n",
      "====> Epoch: 373 Average loss: 30.0642\n",
      "Train Epoch: 374 [0/8000 (0%)]\tLoss: 30.276442\n",
      "Train Epoch: 374 [1280/8000 (16%)]\tLoss: 30.145628\n",
      "Train Epoch: 374 [2560/8000 (32%)]\tLoss: 29.905355\n",
      "Train Epoch: 374 [3840/8000 (48%)]\tLoss: 30.080759\n",
      "Train Epoch: 374 [5120/8000 (64%)]\tLoss: 29.885616\n",
      "Train Epoch: 374 [6400/8000 (80%)]\tLoss: 30.084633\n",
      "Train Epoch: 374 [7680/8000 (96%)]\tLoss: 29.771671\n",
      "====> Epoch: 374 Average loss: 30.0584\n",
      "Train Epoch: 375 [0/8000 (0%)]\tLoss: 30.076321\n",
      "Train Epoch: 375 [1280/8000 (16%)]\tLoss: 29.793678\n",
      "Train Epoch: 375 [2560/8000 (32%)]\tLoss: 30.062988\n",
      "Train Epoch: 375 [3840/8000 (48%)]\tLoss: 30.133041\n",
      "Loss tensor(1914.1995, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1397.6895, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 375 [5120/8000 (64%)]\tLoss: 30.032303\n",
      "Loss tensor(1918.4662, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1404.0884, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 375 [6400/8000 (80%)]\tLoss: 30.011969\n",
      "Train Epoch: 375 [7680/8000 (96%)]\tLoss: 29.953243\n",
      "====> Epoch: 375 Average loss: 30.0663\n",
      "Train Epoch: 376 [0/8000 (0%)]\tLoss: 30.274866\n",
      "Train Epoch: 376 [1280/8000 (16%)]\tLoss: 30.114799\n",
      "Train Epoch: 376 [2560/8000 (32%)]\tLoss: 30.267717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 376 [3840/8000 (48%)]\tLoss: 29.856121\n",
      "Train Epoch: 376 [5120/8000 (64%)]\tLoss: 30.390253\n",
      "Train Epoch: 376 [6400/8000 (80%)]\tLoss: 30.141178\n",
      "Train Epoch: 376 [7680/8000 (96%)]\tLoss: 29.957743\n",
      "====> Epoch: 376 Average loss: 30.0648\n",
      "Train Epoch: 377 [0/8000 (0%)]\tLoss: 29.827236\n",
      "Train Epoch: 377 [1280/8000 (16%)]\tLoss: 29.973572\n",
      "Train Epoch: 377 [2560/8000 (32%)]\tLoss: 29.954578\n",
      "Train Epoch: 377 [3840/8000 (48%)]\tLoss: 30.229126\n",
      "Train Epoch: 377 [5120/8000 (64%)]\tLoss: 29.565790\n",
      "Train Epoch: 377 [6400/8000 (80%)]\tLoss: 30.226675\n",
      "Loss tensor(1941.2590, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1451.2068, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 377 [7680/8000 (96%)]\tLoss: 30.089273\n",
      "====> Epoch: 377 Average loss: 30.0597\n",
      "Train Epoch: 378 [0/8000 (0%)]\tLoss: 30.230671\n",
      "Train Epoch: 378 [1280/8000 (16%)]\tLoss: 29.753273\n",
      "Train Epoch: 378 [2560/8000 (32%)]\tLoss: 30.230064\n",
      "Train Epoch: 378 [3840/8000 (48%)]\tLoss: 30.470480\n",
      "Train Epoch: 378 [5120/8000 (64%)]\tLoss: 29.963974\n",
      "Train Epoch: 378 [6400/8000 (80%)]\tLoss: 30.081825\n",
      "Loss tensor(1908.0607, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1443.1385, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 378 [7680/8000 (96%)]\tLoss: 30.096378\n",
      "Loss tensor(1938.2334, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1440.2391, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 378 Average loss: 30.0571\n",
      "Train Epoch: 379 [0/8000 (0%)]\tLoss: 30.213800\n",
      "Train Epoch: 379 [1280/8000 (16%)]\tLoss: 30.380316\n",
      "Train Epoch: 379 [2560/8000 (32%)]\tLoss: 29.645344\n",
      "Train Epoch: 379 [3840/8000 (48%)]\tLoss: 30.147980\n",
      "Train Epoch: 379 [5120/8000 (64%)]\tLoss: 29.869556\n",
      "Train Epoch: 379 [6400/8000 (80%)]\tLoss: 30.098824\n",
      "Train Epoch: 379 [7680/8000 (96%)]\tLoss: 29.911783\n",
      "====> Epoch: 379 Average loss: 30.0552\n",
      "Train Epoch: 380 [0/8000 (0%)]\tLoss: 29.917839\n",
      "Loss tensor(1954.3101, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1499.5984, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 380 [1280/8000 (16%)]\tLoss: 30.140684\n",
      "Train Epoch: 380 [2560/8000 (32%)]\tLoss: 29.948221\n",
      "Train Epoch: 380 [3840/8000 (48%)]\tLoss: 30.268316\n",
      "Train Epoch: 380 [5120/8000 (64%)]\tLoss: 30.177189\n",
      "Loss tensor(1923.0447, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1504.8452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 380 [6400/8000 (80%)]\tLoss: 29.813530\n",
      "Train Epoch: 380 [7680/8000 (96%)]\tLoss: 30.095819\n",
      "====> Epoch: 380 Average loss: 30.0640\n",
      "Train Epoch: 381 [0/8000 (0%)]\tLoss: 29.868521\n",
      "Train Epoch: 381 [1280/8000 (16%)]\tLoss: 30.302870\n",
      "Train Epoch: 381 [2560/8000 (32%)]\tLoss: 30.331964\n",
      "Train Epoch: 381 [3840/8000 (48%)]\tLoss: 30.092939\n",
      "Train Epoch: 381 [5120/8000 (64%)]\tLoss: 29.872730\n",
      "Train Epoch: 381 [6400/8000 (80%)]\tLoss: 29.636120\n",
      "Loss tensor(1926.0909, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1514.4922, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1918.3445, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1509.4351, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 381 [7680/8000 (96%)]\tLoss: 30.285427\n",
      "====> Epoch: 381 Average loss: 30.0608\n",
      "Train Epoch: 382 [0/8000 (0%)]\tLoss: 30.091959\n",
      "Train Epoch: 382 [1280/8000 (16%)]\tLoss: 29.834229\n",
      "Train Epoch: 382 [2560/8000 (32%)]\tLoss: 29.809992\n",
      "Train Epoch: 382 [3840/8000 (48%)]\tLoss: 29.971786\n",
      "Train Epoch: 382 [5120/8000 (64%)]\tLoss: 29.554285\n",
      "Train Epoch: 382 [6400/8000 (80%)]\tLoss: 30.241550\n",
      "Train Epoch: 382 [7680/8000 (96%)]\tLoss: 30.065969\n",
      "====> Epoch: 382 Average loss: 30.0471\n",
      "Train Epoch: 383 [0/8000 (0%)]\tLoss: 29.627230\n",
      "Train Epoch: 383 [1280/8000 (16%)]\tLoss: 29.833206\n",
      "Train Epoch: 383 [2560/8000 (32%)]\tLoss: 29.922604\n",
      "Train Epoch: 383 [3840/8000 (48%)]\tLoss: 29.777021\n",
      "Train Epoch: 383 [5120/8000 (64%)]\tLoss: 30.103548\n",
      "Train Epoch: 383 [6400/8000 (80%)]\tLoss: 29.940153\n",
      "Train Epoch: 383 [7680/8000 (96%)]\tLoss: 30.157356\n",
      "Loss tensor(1909.5470, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1520.2855, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 383 Average loss: 30.0496\n",
      "Train Epoch: 384 [0/8000 (0%)]\tLoss: 30.028051\n",
      "Train Epoch: 384 [1280/8000 (16%)]\tLoss: 29.985592\n",
      "Train Epoch: 384 [2560/8000 (32%)]\tLoss: 30.039415\n",
      "Train Epoch: 384 [3840/8000 (48%)]\tLoss: 29.950508\n",
      "Train Epoch: 384 [5120/8000 (64%)]\tLoss: 29.929783\n",
      "Train Epoch: 384 [6400/8000 (80%)]\tLoss: 30.422955\n",
      "Loss tensor(1924.8073, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1505.4303, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 384 [7680/8000 (96%)]\tLoss: 29.998634\n",
      "====> Epoch: 384 Average loss: 30.0657\n",
      "Train Epoch: 385 [0/8000 (0%)]\tLoss: 30.344501\n",
      "Loss tensor(1936.7164, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1496.3450, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1932.0701, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1500.7549, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 385 [1280/8000 (16%)]\tLoss: 29.906471\n",
      "Train Epoch: 385 [2560/8000 (32%)]\tLoss: 30.111958\n",
      "Loss tensor(1899.8892, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1492.1853, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 385 [3840/8000 (48%)]\tLoss: 30.168615\n",
      "Train Epoch: 385 [5120/8000 (64%)]\tLoss: 29.821430\n",
      "Train Epoch: 385 [6400/8000 (80%)]\tLoss: 30.061886\n",
      "Train Epoch: 385 [7680/8000 (96%)]\tLoss: 29.851738\n",
      "====> Epoch: 385 Average loss: 30.0497\n",
      "Train Epoch: 386 [0/8000 (0%)]\tLoss: 30.110607\n",
      "Train Epoch: 386 [1280/8000 (16%)]\tLoss: 29.935112\n",
      "Loss tensor(1911.8530, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1504.0211, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 386 [2560/8000 (32%)]\tLoss: 29.927128\n",
      "Train Epoch: 386 [3840/8000 (48%)]\tLoss: 30.073318\n",
      "Train Epoch: 386 [5120/8000 (64%)]\tLoss: 30.152176\n",
      "Train Epoch: 386 [6400/8000 (80%)]\tLoss: 30.194260\n",
      "Train Epoch: 386 [7680/8000 (96%)]\tLoss: 29.952169\n",
      "====> Epoch: 386 Average loss: 30.0477\n",
      "Train Epoch: 387 [0/8000 (0%)]\tLoss: 30.115236\n",
      "Loss tensor(1921.0798, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1495.9402, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1932.9519, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1497.7548, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 387 [1280/8000 (16%)]\tLoss: 30.511917\n",
      "Train Epoch: 387 [2560/8000 (32%)]\tLoss: 30.041565\n",
      "Train Epoch: 387 [3840/8000 (48%)]\tLoss: 30.096169\n",
      "Train Epoch: 387 [5120/8000 (64%)]\tLoss: 29.887877\n",
      "Loss tensor(1910.6813, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1505.1224, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 387 [6400/8000 (80%)]\tLoss: 29.741631\n",
      "Train Epoch: 387 [7680/8000 (96%)]\tLoss: 30.357731\n",
      "====> Epoch: 387 Average loss: 30.0492\n",
      "Train Epoch: 388 [0/8000 (0%)]\tLoss: 30.030542\n",
      "Train Epoch: 388 [1280/8000 (16%)]\tLoss: 30.316542\n",
      "Train Epoch: 388 [2560/8000 (32%)]\tLoss: 29.897055\n",
      "Train Epoch: 388 [3840/8000 (48%)]\tLoss: 30.131701\n",
      "Train Epoch: 388 [5120/8000 (64%)]\tLoss: 29.864836\n",
      "Train Epoch: 388 [6400/8000 (80%)]\tLoss: 30.276369\n",
      "Train Epoch: 388 [7680/8000 (96%)]\tLoss: 30.129910\n",
      "====> Epoch: 388 Average loss: 30.0554\n",
      "Train Epoch: 389 [0/8000 (0%)]\tLoss: 30.180182\n",
      "Train Epoch: 389 [1280/8000 (16%)]\tLoss: 29.836035\n",
      "Loss tensor(1916.1471, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1521.3710, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 389 [2560/8000 (32%)]\tLoss: 29.922302\n",
      "Train Epoch: 389 [3840/8000 (48%)]\tLoss: 30.020178\n",
      "Train Epoch: 389 [5120/8000 (64%)]\tLoss: 30.178963\n",
      "Train Epoch: 389 [6400/8000 (80%)]\tLoss: 30.055571\n",
      "Train Epoch: 389 [7680/8000 (96%)]\tLoss: 29.793581\n",
      "====> Epoch: 389 Average loss: 30.0271\n",
      "Train Epoch: 390 [0/8000 (0%)]\tLoss: 30.004709\n",
      "Loss tensor(1923.7671, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1516.0332, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1917.6698, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1516.9150, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 390 [1280/8000 (16%)]\tLoss: 30.002090\n",
      "Loss tensor(1905.2014, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1523.4113, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 390 [2560/8000 (32%)]\tLoss: 29.909893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 390 [3840/8000 (48%)]\tLoss: 30.246290\n",
      "Train Epoch: 390 [5120/8000 (64%)]\tLoss: 30.149406\n",
      "Train Epoch: 390 [6400/8000 (80%)]\tLoss: 30.088943\n",
      "Train Epoch: 390 [7680/8000 (96%)]\tLoss: 30.186525\n",
      "====> Epoch: 390 Average loss: 30.0545\n",
      "Train Epoch: 391 [0/8000 (0%)]\tLoss: 30.031940\n",
      "Loss tensor(1918.6907, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1538.4744, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 391 [1280/8000 (16%)]\tLoss: 30.083807\n",
      "Train Epoch: 391 [2560/8000 (32%)]\tLoss: 29.904039\n",
      "Loss tensor(1922.9723, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1559.2587, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 391 [3840/8000 (48%)]\tLoss: 29.659920\n",
      "Train Epoch: 391 [5120/8000 (64%)]\tLoss: 30.145716\n",
      "Train Epoch: 391 [6400/8000 (80%)]\tLoss: 29.899118\n",
      "Train Epoch: 391 [7680/8000 (96%)]\tLoss: 30.005611\n",
      "====> Epoch: 391 Average loss: 30.0447\n",
      "Train Epoch: 392 [0/8000 (0%)]\tLoss: 29.823132\n",
      "Train Epoch: 392 [1280/8000 (16%)]\tLoss: 29.855333\n",
      "Train Epoch: 392 [2560/8000 (32%)]\tLoss: 29.957113\n",
      "Train Epoch: 392 [3840/8000 (48%)]\tLoss: 29.913921\n",
      "Loss tensor(1918.5477, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1541.7972, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1917.1649, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1540.5066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 392 [5120/8000 (64%)]\tLoss: 30.137884\n",
      "Train Epoch: 392 [6400/8000 (80%)]\tLoss: 29.927347\n",
      "Loss tensor(1945.4987, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1537.9260, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 392 [7680/8000 (96%)]\tLoss: 30.278383\n",
      "====> Epoch: 392 Average loss: 30.0480\n",
      "Train Epoch: 393 [0/8000 (0%)]\tLoss: 30.092936\n",
      "Train Epoch: 393 [1280/8000 (16%)]\tLoss: 30.052532\n",
      "Train Epoch: 393 [2560/8000 (32%)]\tLoss: 30.325247\n",
      "Train Epoch: 393 [3840/8000 (48%)]\tLoss: 30.241013\n",
      "Train Epoch: 393 [5120/8000 (64%)]\tLoss: 29.956409\n",
      "Train Epoch: 393 [6400/8000 (80%)]\tLoss: 30.004322\n",
      "Train Epoch: 393 [7680/8000 (96%)]\tLoss: 30.049971\n",
      "====> Epoch: 393 Average loss: 30.0243\n",
      "Train Epoch: 394 [0/8000 (0%)]\tLoss: 29.742195\n",
      "Loss tensor(1936.6067, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1547.8129, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 394 [1280/8000 (16%)]\tLoss: 30.230473\n",
      "Train Epoch: 394 [2560/8000 (32%)]\tLoss: 29.704382\n",
      "Loss tensor(1907.1044, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1547.8282, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 394 [3840/8000 (48%)]\tLoss: 30.470970\n",
      "Loss tensor(1929.7802, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1555.5809, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 394 [5120/8000 (64%)]\tLoss: 29.938929\n",
      "Train Epoch: 394 [6400/8000 (80%)]\tLoss: 29.792040\n",
      "Train Epoch: 394 [7680/8000 (96%)]\tLoss: 29.983961\n",
      "====> Epoch: 394 Average loss: 30.0434\n",
      "Train Epoch: 395 [0/8000 (0%)]\tLoss: 30.360865\n",
      "Train Epoch: 395 [1280/8000 (16%)]\tLoss: 29.827930\n",
      "Train Epoch: 395 [2560/8000 (32%)]\tLoss: 29.999731\n",
      "Loss tensor(1932.3002, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1558.0085, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 395 [3840/8000 (48%)]\tLoss: 29.980036\n",
      "Loss tensor(1928.3278, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1557.6270, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 395 [5120/8000 (64%)]\tLoss: 30.032907\n",
      "Train Epoch: 395 [6400/8000 (80%)]\tLoss: 29.979401\n",
      "Loss tensor(1929.3586, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1555.0466, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 395 [7680/8000 (96%)]\tLoss: 30.176132\n",
      "====> Epoch: 395 Average loss: 30.0342\n",
      "Train Epoch: 396 [0/8000 (0%)]\tLoss: 29.913372\n",
      "Train Epoch: 396 [1280/8000 (16%)]\tLoss: 30.067373\n",
      "Train Epoch: 396 [2560/8000 (32%)]\tLoss: 30.000767\n",
      "Loss tensor(1899.7345, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1544.8079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 396 [3840/8000 (48%)]\tLoss: 29.693291\n",
      "Train Epoch: 396 [5120/8000 (64%)]\tLoss: 29.747126\n",
      "Train Epoch: 396 [6400/8000 (80%)]\tLoss: 30.132578\n",
      "Loss tensor(1953.0676, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1553.9437, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 396 [7680/8000 (96%)]\tLoss: 29.955418\n",
      "====> Epoch: 396 Average loss: 30.0327\n",
      "Train Epoch: 397 [0/8000 (0%)]\tLoss: 29.793512\n",
      "Train Epoch: 397 [1280/8000 (16%)]\tLoss: 29.974596\n",
      "Loss tensor(1914.8802, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1539.1541, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 397 [2560/8000 (32%)]\tLoss: 29.781811\n",
      "Train Epoch: 397 [3840/8000 (48%)]\tLoss: 29.842564\n",
      "Train Epoch: 397 [5120/8000 (64%)]\tLoss: 30.048819\n",
      "Train Epoch: 397 [6400/8000 (80%)]\tLoss: 30.209877\n",
      "Train Epoch: 397 [7680/8000 (96%)]\tLoss: 29.933599\n",
      "====> Epoch: 397 Average loss: 30.0291\n",
      "Train Epoch: 398 [0/8000 (0%)]\tLoss: 30.088667\n",
      "Train Epoch: 398 [1280/8000 (16%)]\tLoss: 30.164669\n",
      "Train Epoch: 398 [2560/8000 (32%)]\tLoss: 30.137100\n",
      "Train Epoch: 398 [3840/8000 (48%)]\tLoss: 30.155966\n",
      "Train Epoch: 398 [5120/8000 (64%)]\tLoss: 29.991413\n",
      "Train Epoch: 398 [6400/8000 (80%)]\tLoss: 29.907028\n",
      "Train Epoch: 398 [7680/8000 (96%)]\tLoss: 30.299894\n",
      "Loss tensor(1909.2069, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1552.0177, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 398 Average loss: 30.0433\n",
      "Train Epoch: 399 [0/8000 (0%)]\tLoss: 30.004902\n",
      "Train Epoch: 399 [1280/8000 (16%)]\tLoss: 30.333435\n",
      "Train Epoch: 399 [2560/8000 (32%)]\tLoss: 30.166405\n",
      "Train Epoch: 399 [3840/8000 (48%)]\tLoss: 29.778513\n",
      "Train Epoch: 399 [5120/8000 (64%)]\tLoss: 29.710106\n",
      "Loss tensor(1902.6320, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1545.7781, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 399 [6400/8000 (80%)]\tLoss: 29.979658\n",
      "Train Epoch: 399 [7680/8000 (96%)]\tLoss: 30.103598\n",
      "====> Epoch: 399 Average loss: 30.0262\n",
      "Train Epoch: 400 [0/8000 (0%)]\tLoss: 29.951483\n",
      "Train Epoch: 400 [1280/8000 (16%)]\tLoss: 29.882160\n",
      "Loss tensor(1929.0989, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1556.1541, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1934.9575, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1557.6719, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 400 [2560/8000 (32%)]\tLoss: 30.314569\n",
      "Train Epoch: 400 [3840/8000 (48%)]\tLoss: 29.903708\n",
      "Train Epoch: 400 [5120/8000 (64%)]\tLoss: 30.061678\n",
      "Train Epoch: 400 [6400/8000 (80%)]\tLoss: 30.283648\n",
      "Train Epoch: 400 [7680/8000 (96%)]\tLoss: 30.277987\n",
      "====> Epoch: 400 Average loss: 30.0271\n",
      "Train Epoch: 401 [0/8000 (0%)]\tLoss: 30.250694\n",
      "Loss tensor(1949.7952, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1552.3838, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1919.9813, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1550.5930, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 401 [1280/8000 (16%)]\tLoss: 29.868538\n",
      "Train Epoch: 401 [2560/8000 (32%)]\tLoss: 29.838356\n",
      "Loss tensor(1918.7198, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1553.9563, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 401 [3840/8000 (48%)]\tLoss: 30.177084\n",
      "Train Epoch: 401 [5120/8000 (64%)]\tLoss: 30.229424\n",
      "Train Epoch: 401 [6400/8000 (80%)]\tLoss: 29.912039\n",
      "Loss tensor(1928.7435, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1564.0912, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 401 [7680/8000 (96%)]\tLoss: 30.162975\n",
      "====> Epoch: 401 Average loss: 30.0190\n",
      "Train Epoch: 402 [0/8000 (0%)]\tLoss: 30.226372\n",
      "Loss tensor(1910.2440, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1558.2007, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 402 [1280/8000 (16%)]\tLoss: 29.935753\n",
      "Train Epoch: 402 [2560/8000 (32%)]\tLoss: 30.193014\n",
      "Train Epoch: 402 [3840/8000 (48%)]\tLoss: 29.749128\n",
      "Train Epoch: 402 [5120/8000 (64%)]\tLoss: 29.824503\n",
      "Train Epoch: 402 [6400/8000 (80%)]\tLoss: 29.858110\n",
      "Train Epoch: 402 [7680/8000 (96%)]\tLoss: 29.990629\n",
      "====> Epoch: 402 Average loss: 30.0490\n",
      "Train Epoch: 403 [0/8000 (0%)]\tLoss: 29.683178\n",
      "Train Epoch: 403 [1280/8000 (16%)]\tLoss: 30.069914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 403 [2560/8000 (32%)]\tLoss: 30.072390\n",
      "Train Epoch: 403 [3840/8000 (48%)]\tLoss: 30.124945\n",
      "Train Epoch: 403 [5120/8000 (64%)]\tLoss: 29.812243\n",
      "Train Epoch: 403 [6400/8000 (80%)]\tLoss: 29.783028\n",
      "Train Epoch: 403 [7680/8000 (96%)]\tLoss: 29.943613\n",
      "====> Epoch: 403 Average loss: 30.0265\n",
      "Train Epoch: 404 [0/8000 (0%)]\tLoss: 30.103134\n",
      "Train Epoch: 404 [1280/8000 (16%)]\tLoss: 29.712732\n",
      "Train Epoch: 404 [2560/8000 (32%)]\tLoss: 30.223459\n",
      "Loss tensor(1911.3713, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1556.9385, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 404 [3840/8000 (48%)]\tLoss: 30.087978\n",
      "Loss tensor(1934.6918, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1558.4597, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 404 [5120/8000 (64%)]\tLoss: 30.102516\n",
      "Train Epoch: 404 [6400/8000 (80%)]\tLoss: 29.684732\n",
      "Loss tensor(1926.3444, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1554.7036, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 404 [7680/8000 (96%)]\tLoss: 30.121565\n",
      "====> Epoch: 404 Average loss: 30.0282\n",
      "Train Epoch: 405 [0/8000 (0%)]\tLoss: 29.695745\n",
      "Train Epoch: 405 [1280/8000 (16%)]\tLoss: 29.927574\n",
      "Loss tensor(1930.5305, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1566.3933, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 405 [2560/8000 (32%)]\tLoss: 29.843019\n",
      "Train Epoch: 405 [3840/8000 (48%)]\tLoss: 29.789824\n",
      "Train Epoch: 405 [5120/8000 (64%)]\tLoss: 29.768223\n",
      "Train Epoch: 405 [6400/8000 (80%)]\tLoss: 30.225863\n",
      "Train Epoch: 405 [7680/8000 (96%)]\tLoss: 29.907198\n",
      "====> Epoch: 405 Average loss: 30.0351\n",
      "Train Epoch: 406 [0/8000 (0%)]\tLoss: 30.006443\n",
      "Loss tensor(1929.1617, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1562.2617, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1905.3699, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1562.5366, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 406 [1280/8000 (16%)]\tLoss: 30.004366\n",
      "Train Epoch: 406 [2560/8000 (32%)]\tLoss: 30.024651\n",
      "Train Epoch: 406 [3840/8000 (48%)]\tLoss: 30.035345\n",
      "Train Epoch: 406 [5120/8000 (64%)]\tLoss: 30.125673\n",
      "Train Epoch: 406 [6400/8000 (80%)]\tLoss: 29.873125\n",
      "Train Epoch: 406 [7680/8000 (96%)]\tLoss: 30.161587\n",
      "====> Epoch: 406 Average loss: 30.0108\n",
      "Train Epoch: 407 [0/8000 (0%)]\tLoss: 29.791483\n",
      "Train Epoch: 407 [1280/8000 (16%)]\tLoss: 29.591673\n",
      "Train Epoch: 407 [2560/8000 (32%)]\tLoss: 29.754732\n",
      "Train Epoch: 407 [3840/8000 (48%)]\tLoss: 30.045677\n",
      "Train Epoch: 407 [5120/8000 (64%)]\tLoss: 29.755735\n",
      "Train Epoch: 407 [6400/8000 (80%)]\tLoss: 30.006903\n",
      "Train Epoch: 407 [7680/8000 (96%)]\tLoss: 30.013800\n",
      "====> Epoch: 407 Average loss: 30.0237\n",
      "Train Epoch: 408 [0/8000 (0%)]\tLoss: 29.910151\n",
      "Train Epoch: 408 [1280/8000 (16%)]\tLoss: 29.923252\n",
      "Train Epoch: 408 [2560/8000 (32%)]\tLoss: 29.891857\n",
      "Loss tensor(1934.5551, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1582.4066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 408 [3840/8000 (48%)]\tLoss: 30.146095\n",
      "Loss tensor(1927.7228, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1582.7876, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 408 [5120/8000 (64%)]\tLoss: 30.056944\n",
      "Train Epoch: 408 [6400/8000 (80%)]\tLoss: 29.776905\n",
      "Train Epoch: 408 [7680/8000 (96%)]\tLoss: 29.801779\n",
      "====> Epoch: 408 Average loss: 30.0190\n",
      "Train Epoch: 409 [0/8000 (0%)]\tLoss: 30.256680\n",
      "Train Epoch: 409 [1280/8000 (16%)]\tLoss: 29.800055\n",
      "Train Epoch: 409 [2560/8000 (32%)]\tLoss: 30.080410\n",
      "Train Epoch: 409 [3840/8000 (48%)]\tLoss: 29.812885\n",
      "Train Epoch: 409 [5120/8000 (64%)]\tLoss: 29.751278\n",
      "Loss tensor(1923.6648, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1580.5164, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 409 [6400/8000 (80%)]\tLoss: 30.149214\n",
      "Train Epoch: 409 [7680/8000 (96%)]\tLoss: 30.094452\n",
      "====> Epoch: 409 Average loss: 30.0207\n",
      "Train Epoch: 410 [0/8000 (0%)]\tLoss: 29.889168\n",
      "Train Epoch: 410 [1280/8000 (16%)]\tLoss: 29.951286\n",
      "Loss tensor(1932.7346, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1650.1102, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 410 [2560/8000 (32%)]\tLoss: 30.068596\n",
      "Train Epoch: 410 [3840/8000 (48%)]\tLoss: 30.139458\n",
      "Train Epoch: 410 [5120/8000 (64%)]\tLoss: 30.089960\n",
      "Train Epoch: 410 [6400/8000 (80%)]\tLoss: 30.626156\n",
      "Train Epoch: 410 [7680/8000 (96%)]\tLoss: 29.964310\n",
      "====> Epoch: 410 Average loss: 30.0169\n",
      "Train Epoch: 411 [0/8000 (0%)]\tLoss: 29.781603\n",
      "Train Epoch: 411 [1280/8000 (16%)]\tLoss: 30.036430\n",
      "Train Epoch: 411 [2560/8000 (32%)]\tLoss: 29.634251\n",
      "Loss tensor(1900.0864, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1654.7604, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 411 [3840/8000 (48%)]\tLoss: 30.199524\n",
      "Loss tensor(1922.1821, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1643.0876, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 411 [5120/8000 (64%)]\tLoss: 30.070591\n",
      "Train Epoch: 411 [6400/8000 (80%)]\tLoss: 30.117136\n",
      "Train Epoch: 411 [7680/8000 (96%)]\tLoss: 29.752619\n",
      "====> Epoch: 411 Average loss: 30.0204\n",
      "Train Epoch: 412 [0/8000 (0%)]\tLoss: 30.405630\n",
      "Loss tensor(1925.1193, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1654.8755, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 412 [1280/8000 (16%)]\tLoss: 30.056786\n",
      "Train Epoch: 412 [2560/8000 (32%)]\tLoss: 30.026758\n",
      "Loss tensor(1912.1901, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1645.0099, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 412 [3840/8000 (48%)]\tLoss: 30.181274\n",
      "Train Epoch: 412 [5120/8000 (64%)]\tLoss: 30.002350\n",
      "Train Epoch: 412 [6400/8000 (80%)]\tLoss: 29.755079\n",
      "Train Epoch: 412 [7680/8000 (96%)]\tLoss: 29.781372\n",
      "====> Epoch: 412 Average loss: 30.0140\n",
      "Train Epoch: 413 [0/8000 (0%)]\tLoss: 30.184315\n",
      "Train Epoch: 413 [1280/8000 (16%)]\tLoss: 30.033617\n",
      "Train Epoch: 413 [2560/8000 (32%)]\tLoss: 29.652973\n",
      "Loss tensor(1924.6644, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1657.3081, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 413 [3840/8000 (48%)]\tLoss: 29.869795\n",
      "Loss tensor(1927.4757, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1644.7692, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 413 [5120/8000 (64%)]\tLoss: 30.186693\n",
      "Train Epoch: 413 [6400/8000 (80%)]\tLoss: 30.185301\n",
      "Train Epoch: 413 [7680/8000 (96%)]\tLoss: 30.140411\n",
      "====> Epoch: 413 Average loss: 30.0383\n",
      "Train Epoch: 414 [0/8000 (0%)]\tLoss: 30.061506\n",
      "Train Epoch: 414 [1280/8000 (16%)]\tLoss: 30.001043\n",
      "Train Epoch: 414 [2560/8000 (32%)]\tLoss: 30.210432\n",
      "Train Epoch: 414 [3840/8000 (48%)]\tLoss: 30.137617\n",
      "Train Epoch: 414 [5120/8000 (64%)]\tLoss: 30.068901\n",
      "Loss tensor(1964.8192, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1640.7642, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 414 [6400/8000 (80%)]\tLoss: 30.116699\n",
      "Loss tensor(1923.0344, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1641.0718, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1908.0249, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1644.6687, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 414 [7680/8000 (96%)]\tLoss: 29.812889\n",
      "====> Epoch: 414 Average loss: 30.0290\n",
      "Train Epoch: 415 [0/8000 (0%)]\tLoss: 29.953892\n",
      "Loss tensor(1913.6871, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1651.8392, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 415 [1280/8000 (16%)]\tLoss: 30.059269\n",
      "Train Epoch: 415 [2560/8000 (32%)]\tLoss: 30.253603\n",
      "Train Epoch: 415 [3840/8000 (48%)]\tLoss: 30.076265\n",
      "Train Epoch: 415 [5120/8000 (64%)]\tLoss: 29.915295\n",
      "Loss tensor(1914.7540, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1644.3452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 415 [6400/8000 (80%)]\tLoss: 30.288136\n",
      "Train Epoch: 415 [7680/8000 (96%)]\tLoss: 29.972500\n",
      "====> Epoch: 415 Average loss: 30.0089\n",
      "Train Epoch: 416 [0/8000 (0%)]\tLoss: 29.741518\n",
      "Train Epoch: 416 [1280/8000 (16%)]\tLoss: 30.001524\n",
      "Loss tensor(1916.9919, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1650.0914, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1935.4998, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1653.6814, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1928.0397, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1659.8674, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 416 [2560/8000 (32%)]\tLoss: 29.911106\n",
      "Train Epoch: 416 [3840/8000 (48%)]\tLoss: 30.151022\n",
      "Train Epoch: 416 [5120/8000 (64%)]\tLoss: 30.173594\n",
      "Train Epoch: 416 [6400/8000 (80%)]\tLoss: 30.034613\n",
      "Train Epoch: 416 [7680/8000 (96%)]\tLoss: 29.875360\n",
      "====> Epoch: 416 Average loss: 30.0141\n",
      "Train Epoch: 417 [0/8000 (0%)]\tLoss: 30.241653\n",
      "Train Epoch: 417 [1280/8000 (16%)]\tLoss: 30.276260\n",
      "Train Epoch: 417 [2560/8000 (32%)]\tLoss: 29.957531\n",
      "Loss tensor(1928.8379, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1647.2056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 417 [3840/8000 (48%)]\tLoss: 30.043751\n",
      "Train Epoch: 417 [5120/8000 (64%)]\tLoss: 30.027353\n",
      "Train Epoch: 417 [6400/8000 (80%)]\tLoss: 29.703417\n",
      "Loss tensor(1928.6111, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1641.5720, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 417 [7680/8000 (96%)]\tLoss: 29.987883\n",
      "====> Epoch: 417 Average loss: 30.0275\n",
      "Train Epoch: 418 [0/8000 (0%)]\tLoss: 30.234665\n",
      "Train Epoch: 418 [1280/8000 (16%)]\tLoss: 29.693794\n",
      "Train Epoch: 418 [2560/8000 (32%)]\tLoss: 30.010563\n",
      "Train Epoch: 418 [3840/8000 (48%)]\tLoss: 30.292606\n",
      "Train Epoch: 418 [5120/8000 (64%)]\tLoss: 29.588560\n",
      "Train Epoch: 418 [6400/8000 (80%)]\tLoss: 30.009228\n",
      "Train Epoch: 418 [7680/8000 (96%)]\tLoss: 29.854019\n",
      "====> Epoch: 418 Average loss: 30.0128\n",
      "Train Epoch: 419 [0/8000 (0%)]\tLoss: 30.085009\n",
      "Train Epoch: 419 [1280/8000 (16%)]\tLoss: 29.956865\n",
      "Train Epoch: 419 [2560/8000 (32%)]\tLoss: 30.218716\n",
      "Train Epoch: 419 [3840/8000 (48%)]\tLoss: 30.033928\n",
      "Train Epoch: 419 [5120/8000 (64%)]\tLoss: 29.752352\n",
      "Train Epoch: 419 [6400/8000 (80%)]\tLoss: 30.183083\n",
      "Loss tensor(1914.4766, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1655.3245, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 419 [7680/8000 (96%)]\tLoss: 29.919920\n",
      "====> Epoch: 419 Average loss: 30.0206\n",
      "Train Epoch: 420 [0/8000 (0%)]\tLoss: 30.007565\n",
      "Train Epoch: 420 [1280/8000 (16%)]\tLoss: 29.901855\n",
      "Train Epoch: 420 [2560/8000 (32%)]\tLoss: 29.831617\n",
      "Train Epoch: 420 [3840/8000 (48%)]\tLoss: 30.179749\n",
      "Train Epoch: 420 [5120/8000 (64%)]\tLoss: 29.769171\n",
      "Train Epoch: 420 [6400/8000 (80%)]\tLoss: 30.117985\n",
      "Train Epoch: 420 [7680/8000 (96%)]\tLoss: 29.778406\n",
      "====> Epoch: 420 Average loss: 30.0277\n",
      "Train Epoch: 421 [0/8000 (0%)]\tLoss: 29.453506\n",
      "Train Epoch: 421 [1280/8000 (16%)]\tLoss: 29.964426\n",
      "Train Epoch: 421 [2560/8000 (32%)]\tLoss: 30.409157\n",
      "Train Epoch: 421 [3840/8000 (48%)]\tLoss: 29.859644\n",
      "Loss tensor(1931.8680, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1678.1821, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 421 [5120/8000 (64%)]\tLoss: 29.867901\n",
      "Train Epoch: 421 [6400/8000 (80%)]\tLoss: 29.986130\n",
      "Train Epoch: 421 [7680/8000 (96%)]\tLoss: 30.042845\n",
      "====> Epoch: 421 Average loss: 30.0081\n",
      "Train Epoch: 422 [0/8000 (0%)]\tLoss: 30.153666\n",
      "Loss tensor(1945.2024, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1670.6400, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 422 [1280/8000 (16%)]\tLoss: 30.139790\n",
      "Loss tensor(1921.4156, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1677.2303, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 422 [2560/8000 (32%)]\tLoss: 29.846361\n",
      "Train Epoch: 422 [3840/8000 (48%)]\tLoss: 30.048326\n",
      "Loss tensor(1922.5409, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1665.1519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 422 [5120/8000 (64%)]\tLoss: 30.230236\n",
      "Loss tensor(1933.5345, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1672.5958, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 422 [6400/8000 (80%)]\tLoss: 30.089230\n",
      "Train Epoch: 422 [7680/8000 (96%)]\tLoss: 29.929375\n",
      "====> Epoch: 422 Average loss: 30.0183\n",
      "Train Epoch: 423 [0/8000 (0%)]\tLoss: 30.019974\n",
      "Train Epoch: 423 [1280/8000 (16%)]\tLoss: 29.602964\n",
      "Train Epoch: 423 [2560/8000 (32%)]\tLoss: 30.120564\n",
      "Train Epoch: 423 [3840/8000 (48%)]\tLoss: 29.948368\n",
      "Loss tensor(1911.8499, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1678.6165, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1933.6232, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1674.2880, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1906.1085, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1674.2896, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 423 [5120/8000 (64%)]\tLoss: 29.850018\n",
      "Train Epoch: 423 [6400/8000 (80%)]\tLoss: 29.894167\n",
      "Train Epoch: 423 [7680/8000 (96%)]\tLoss: 29.799257\n",
      "====> Epoch: 423 Average loss: 29.9856\n",
      "Train Epoch: 424 [0/8000 (0%)]\tLoss: 29.912237\n",
      "Train Epoch: 424 [1280/8000 (16%)]\tLoss: 29.896324\n",
      "Loss tensor(1932.9884, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1689.3903, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 424 [2560/8000 (32%)]\tLoss: 29.894939\n",
      "Train Epoch: 424 [3840/8000 (48%)]\tLoss: 29.609320\n",
      "Train Epoch: 424 [5120/8000 (64%)]\tLoss: 30.001451\n",
      "Train Epoch: 424 [6400/8000 (80%)]\tLoss: 30.032452\n",
      "Loss tensor(1916.4346, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1682.9436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 424 [7680/8000 (96%)]\tLoss: 29.858265\n",
      "====> Epoch: 424 Average loss: 29.9995\n",
      "Train Epoch: 425 [0/8000 (0%)]\tLoss: 29.779091\n",
      "Train Epoch: 425 [1280/8000 (16%)]\tLoss: 29.815708\n",
      "Train Epoch: 425 [2560/8000 (32%)]\tLoss: 30.216675\n",
      "Train Epoch: 425 [3840/8000 (48%)]\tLoss: 29.839926\n",
      "Train Epoch: 425 [5120/8000 (64%)]\tLoss: 30.187719\n",
      "Train Epoch: 425 [6400/8000 (80%)]\tLoss: 29.958109\n",
      "Train Epoch: 425 [7680/8000 (96%)]\tLoss: 29.849981\n",
      "====> Epoch: 425 Average loss: 30.0195\n",
      "Train Epoch: 426 [0/8000 (0%)]\tLoss: 29.776510\n",
      "Train Epoch: 426 [1280/8000 (16%)]\tLoss: 29.965429\n",
      "Train Epoch: 426 [2560/8000 (32%)]\tLoss: 29.841135\n",
      "Train Epoch: 426 [3840/8000 (48%)]\tLoss: 29.794525\n",
      "Train Epoch: 426 [5120/8000 (64%)]\tLoss: 30.209633\n",
      "Train Epoch: 426 [6400/8000 (80%)]\tLoss: 29.758621\n",
      "Train Epoch: 426 [7680/8000 (96%)]\tLoss: 29.866871\n",
      "====> Epoch: 426 Average loss: 29.9913\n",
      "Train Epoch: 427 [0/8000 (0%)]\tLoss: 30.033110\n",
      "Loss tensor(1912.4783, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1682.8344, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 427 [1280/8000 (16%)]\tLoss: 30.360462\n",
      "Train Epoch: 427 [2560/8000 (32%)]\tLoss: 30.182589\n",
      "Loss tensor(1928.8534, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1700.7504, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 427 [3840/8000 (48%)]\tLoss: 29.892464\n",
      "Train Epoch: 427 [5120/8000 (64%)]\tLoss: 29.935612\n",
      "Train Epoch: 427 [6400/8000 (80%)]\tLoss: 29.924486\n",
      "Train Epoch: 427 [7680/8000 (96%)]\tLoss: 29.973475\n",
      "====> Epoch: 427 Average loss: 29.9825\n",
      "Train Epoch: 428 [0/8000 (0%)]\tLoss: 29.931086\n",
      "Loss tensor(1938.9519, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1703.2751, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 428 [1280/8000 (16%)]\tLoss: 29.752975\n",
      "Train Epoch: 428 [2560/8000 (32%)]\tLoss: 29.709795\n",
      "Train Epoch: 428 [3840/8000 (48%)]\tLoss: 29.866865\n",
      "Train Epoch: 428 [5120/8000 (64%)]\tLoss: 30.117165\n",
      "Loss tensor(1911.1954, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1683.2222, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1924.1422, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1686.1213, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 428 [6400/8000 (80%)]\tLoss: 29.995096\n",
      "Train Epoch: 428 [7680/8000 (96%)]\tLoss: 29.986492\n",
      "====> Epoch: 428 Average loss: 29.9956\n",
      "Train Epoch: 429 [0/8000 (0%)]\tLoss: 29.936214\n",
      "Train Epoch: 429 [1280/8000 (16%)]\tLoss: 30.123917\n",
      "Train Epoch: 429 [2560/8000 (32%)]\tLoss: 30.059816\n",
      "Train Epoch: 429 [3840/8000 (48%)]\tLoss: 29.957790\n",
      "Train Epoch: 429 [5120/8000 (64%)]\tLoss: 29.954468\n",
      "Train Epoch: 429 [6400/8000 (80%)]\tLoss: 30.012327\n",
      "Train Epoch: 429 [7680/8000 (96%)]\tLoss: 30.019554\n",
      "====> Epoch: 429 Average loss: 29.9923\n",
      "Train Epoch: 430 [0/8000 (0%)]\tLoss: 30.358494\n",
      "Train Epoch: 430 [1280/8000 (16%)]\tLoss: 30.119362\n",
      "Loss tensor(1903.0107, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1678.3010, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 430 [2560/8000 (32%)]\tLoss: 29.738729\n",
      "Train Epoch: 430 [3840/8000 (48%)]\tLoss: 29.428495\n",
      "Train Epoch: 430 [5120/8000 (64%)]\tLoss: 30.016895\n",
      "Train Epoch: 430 [6400/8000 (80%)]\tLoss: 30.322601\n",
      "Train Epoch: 430 [7680/8000 (96%)]\tLoss: 30.243361\n",
      "====> Epoch: 430 Average loss: 29.9806\n",
      "Train Epoch: 431 [0/8000 (0%)]\tLoss: 30.095381\n",
      "Train Epoch: 431 [1280/8000 (16%)]\tLoss: 29.858501\n",
      "Train Epoch: 431 [2560/8000 (32%)]\tLoss: 30.551456\n",
      "Train Epoch: 431 [3840/8000 (48%)]\tLoss: 30.058287\n",
      "Train Epoch: 431 [5120/8000 (64%)]\tLoss: 29.629873\n",
      "Loss tensor(1918.5139, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1663.6924, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 431 [6400/8000 (80%)]\tLoss: 29.974167\n",
      "Train Epoch: 431 [7680/8000 (96%)]\tLoss: 29.990101\n",
      "====> Epoch: 431 Average loss: 30.0059\n",
      "Train Epoch: 432 [0/8000 (0%)]\tLoss: 30.057709\n",
      "Loss tensor(1928.0227, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1678.8406, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 432 [1280/8000 (16%)]\tLoss: 30.135136\n",
      "Train Epoch: 432 [2560/8000 (32%)]\tLoss: 30.331833\n",
      "Loss tensor(1915.9615, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1667.9867, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 432 [3840/8000 (48%)]\tLoss: 29.936899\n",
      "Train Epoch: 432 [5120/8000 (64%)]\tLoss: 30.127642\n",
      "Train Epoch: 432 [6400/8000 (80%)]\tLoss: 29.951466\n",
      "Train Epoch: 432 [7680/8000 (96%)]\tLoss: 30.036148\n",
      "====> Epoch: 432 Average loss: 29.9861\n",
      "Train Epoch: 433 [0/8000 (0%)]\tLoss: 29.795572\n",
      "Train Epoch: 433 [1280/8000 (16%)]\tLoss: 30.130959\n",
      "Train Epoch: 433 [2560/8000 (32%)]\tLoss: 30.202503\n",
      "Train Epoch: 433 [3840/8000 (48%)]\tLoss: 29.862345\n",
      "Train Epoch: 433 [5120/8000 (64%)]\tLoss: 29.962275\n",
      "Train Epoch: 433 [6400/8000 (80%)]\tLoss: 30.357704\n",
      "Train Epoch: 433 [7680/8000 (96%)]\tLoss: 30.000765\n",
      "====> Epoch: 433 Average loss: 29.9827\n",
      "Train Epoch: 434 [0/8000 (0%)]\tLoss: 30.155279\n",
      "Train Epoch: 434 [1280/8000 (16%)]\tLoss: 30.015003\n",
      "Loss tensor(1926.7192, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1682.1523, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 434 [2560/8000 (32%)]\tLoss: 29.875786\n",
      "Train Epoch: 434 [3840/8000 (48%)]\tLoss: 29.846514\n",
      "Train Epoch: 434 [5120/8000 (64%)]\tLoss: 29.776796\n",
      "Train Epoch: 434 [6400/8000 (80%)]\tLoss: 30.017244\n",
      "Train Epoch: 434 [7680/8000 (96%)]\tLoss: 29.910166\n",
      "====> Epoch: 434 Average loss: 29.9660\n",
      "Train Epoch: 435 [0/8000 (0%)]\tLoss: 29.892567\n",
      "Train Epoch: 435 [1280/8000 (16%)]\tLoss: 30.333067\n",
      "Train Epoch: 435 [2560/8000 (32%)]\tLoss: 30.450323\n",
      "Loss tensor(1925.6727, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1687.8961, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 435 [3840/8000 (48%)]\tLoss: 30.001003\n",
      "Train Epoch: 435 [5120/8000 (64%)]\tLoss: 29.683010\n",
      "Train Epoch: 435 [6400/8000 (80%)]\tLoss: 30.007414\n",
      "Train Epoch: 435 [7680/8000 (96%)]\tLoss: 30.379538\n",
      "====> Epoch: 435 Average loss: 29.9939\n",
      "Train Epoch: 436 [0/8000 (0%)]\tLoss: 30.122679\n",
      "Train Epoch: 436 [1280/8000 (16%)]\tLoss: 30.093523\n",
      "Train Epoch: 436 [2560/8000 (32%)]\tLoss: 29.977251\n",
      "Train Epoch: 436 [3840/8000 (48%)]\tLoss: 30.061779\n",
      "Train Epoch: 436 [5120/8000 (64%)]\tLoss: 29.991728\n",
      "Loss tensor(1913.8948, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1683.2836, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 436 [6400/8000 (80%)]\tLoss: 29.826494\n",
      "Train Epoch: 436 [7680/8000 (96%)]\tLoss: 29.853706\n",
      "====> Epoch: 436 Average loss: 29.9843\n",
      "Train Epoch: 437 [0/8000 (0%)]\tLoss: 30.251005\n",
      "Train Epoch: 437 [1280/8000 (16%)]\tLoss: 29.790464\n",
      "Train Epoch: 437 [2560/8000 (32%)]\tLoss: 29.895575\n",
      "Loss tensor(1910.4178, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1695.1693, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 437 [3840/8000 (48%)]\tLoss: 30.207298\n",
      "Loss tensor(1930.4039, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1683.5231, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 437 [5120/8000 (64%)]\tLoss: 30.102621\n",
      "Train Epoch: 437 [6400/8000 (80%)]\tLoss: 29.951733\n",
      "Loss tensor(1924.2568, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1687.3507, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 437 [7680/8000 (96%)]\tLoss: 29.777828\n",
      "====> Epoch: 437 Average loss: 29.9638\n",
      "Train Epoch: 438 [0/8000 (0%)]\tLoss: 29.856016\n",
      "Loss tensor(1910.1144, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1677.3442, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 438 [1280/8000 (16%)]\tLoss: 29.992317\n",
      "Train Epoch: 438 [2560/8000 (32%)]\tLoss: 30.120600\n",
      "Train Epoch: 438 [3840/8000 (48%)]\tLoss: 29.924665\n",
      "Loss tensor(1915.8586, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1684.1658, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 438 [5120/8000 (64%)]\tLoss: 30.046436\n",
      "Loss tensor(1926.9822, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1688.8782, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 438 [6400/8000 (80%)]\tLoss: 30.363466\n",
      "Loss tensor(1914.2222, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1681.3345, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 438 [7680/8000 (96%)]\tLoss: 29.683540\n",
      "====> Epoch: 438 Average loss: 29.9892\n",
      "Train Epoch: 439 [0/8000 (0%)]\tLoss: 30.196304\n",
      "Train Epoch: 439 [1280/8000 (16%)]\tLoss: 30.805325\n",
      "Train Epoch: 439 [2560/8000 (32%)]\tLoss: 29.826159\n",
      "Train Epoch: 439 [3840/8000 (48%)]\tLoss: 29.921444\n",
      "Loss tensor(1930.3862, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1666.5240, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 439 [5120/8000 (64%)]\tLoss: 30.082352\n",
      "Train Epoch: 439 [6400/8000 (80%)]\tLoss: 29.992714\n",
      "Loss tensor(1908.9083, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1650.2260, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1955.5479, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1660.6189, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 439 [7680/8000 (96%)]\tLoss: 29.909954\n",
      "Loss tensor(1920.4146, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1660.5922, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 439 Average loss: 29.9762\n",
      "Train Epoch: 440 [0/8000 (0%)]\tLoss: 29.836103\n",
      "Train Epoch: 440 [1280/8000 (16%)]\tLoss: 29.939960\n",
      "Train Epoch: 440 [2560/8000 (32%)]\tLoss: 30.135096\n",
      "Train Epoch: 440 [3840/8000 (48%)]\tLoss: 30.058428\n",
      "Train Epoch: 440 [5120/8000 (64%)]\tLoss: 29.920601\n",
      "Train Epoch: 440 [6400/8000 (80%)]\tLoss: 29.660112\n",
      "Train Epoch: 440 [7680/8000 (96%)]\tLoss: 29.864243\n",
      "====> Epoch: 440 Average loss: 29.9716\n",
      "Train Epoch: 441 [0/8000 (0%)]\tLoss: 29.990608\n",
      "Loss tensor(1927.4038, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1670.9015, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 441 [1280/8000 (16%)]\tLoss: 30.125832\n",
      "Train Epoch: 441 [2560/8000 (32%)]\tLoss: 30.047636\n",
      "Train Epoch: 441 [3840/8000 (48%)]\tLoss: 29.848200\n",
      "Train Epoch: 441 [5120/8000 (64%)]\tLoss: 29.900999\n",
      "Train Epoch: 441 [6400/8000 (80%)]\tLoss: 29.782003\n",
      "Loss tensor(1915.3286, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1674.4045, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 441 [7680/8000 (96%)]\tLoss: 30.187216\n",
      "====> Epoch: 441 Average loss: 29.9817\n",
      "Train Epoch: 442 [0/8000 (0%)]\tLoss: 30.029112\n",
      "Train Epoch: 442 [1280/8000 (16%)]\tLoss: 29.829071\n",
      "Loss tensor(1913.5314, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1688.9385, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 442 [2560/8000 (32%)]\tLoss: 30.109989\n",
      "Train Epoch: 442 [3840/8000 (48%)]\tLoss: 29.844131\n",
      "Train Epoch: 442 [5120/8000 (64%)]\tLoss: 30.557129\n",
      "Train Epoch: 442 [6400/8000 (80%)]\tLoss: 29.850327\n",
      "Train Epoch: 442 [7680/8000 (96%)]\tLoss: 29.834988\n",
      "====> Epoch: 442 Average loss: 29.9841\n",
      "Train Epoch: 443 [0/8000 (0%)]\tLoss: 29.908314\n",
      "Train Epoch: 443 [1280/8000 (16%)]\tLoss: 29.834486\n",
      "Loss tensor(1928.6797, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1675.3837, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 443 [2560/8000 (32%)]\tLoss: 29.915611\n",
      "Train Epoch: 443 [3840/8000 (48%)]\tLoss: 30.093904\n",
      "Train Epoch: 443 [5120/8000 (64%)]\tLoss: 30.038515\n",
      "Train Epoch: 443 [6400/8000 (80%)]\tLoss: 30.028515\n",
      "Train Epoch: 443 [7680/8000 (96%)]\tLoss: 30.196533\n",
      "====> Epoch: 443 Average loss: 29.9649\n",
      "Train Epoch: 444 [0/8000 (0%)]\tLoss: 30.042227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 444 [1280/8000 (16%)]\tLoss: 30.259964\n",
      "Train Epoch: 444 [2560/8000 (32%)]\tLoss: 30.025505\n",
      "Train Epoch: 444 [3840/8000 (48%)]\tLoss: 30.039732\n",
      "Loss tensor(1930.0923, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1685.6833, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 444 [5120/8000 (64%)]\tLoss: 29.818766\n",
      "Train Epoch: 444 [6400/8000 (80%)]\tLoss: 29.738861\n",
      "Train Epoch: 444 [7680/8000 (96%)]\tLoss: 29.826227\n",
      "====> Epoch: 444 Average loss: 29.9751\n",
      "Train Epoch: 445 [0/8000 (0%)]\tLoss: 29.710222\n",
      "Train Epoch: 445 [1280/8000 (16%)]\tLoss: 30.034163\n",
      "Train Epoch: 445 [2560/8000 (32%)]\tLoss: 30.233360\n",
      "Train Epoch: 445 [3840/8000 (48%)]\tLoss: 29.941051\n",
      "Train Epoch: 445 [5120/8000 (64%)]\tLoss: 30.013866\n",
      "Train Epoch: 445 [6400/8000 (80%)]\tLoss: 30.019630\n",
      "Train Epoch: 445 [7680/8000 (96%)]\tLoss: 29.767769\n",
      "====> Epoch: 445 Average loss: 29.9706\n",
      "Train Epoch: 446 [0/8000 (0%)]\tLoss: 30.065567\n",
      "Train Epoch: 446 [1280/8000 (16%)]\tLoss: 30.140141\n",
      "Train Epoch: 446 [2560/8000 (32%)]\tLoss: 29.976768\n",
      "Train Epoch: 446 [3840/8000 (48%)]\tLoss: 30.189850\n",
      "Train Epoch: 446 [5120/8000 (64%)]\tLoss: 30.001032\n",
      "Train Epoch: 446 [6400/8000 (80%)]\tLoss: 29.958897\n",
      "Train Epoch: 446 [7680/8000 (96%)]\tLoss: 30.046801\n",
      "====> Epoch: 446 Average loss: 29.9548\n",
      "Train Epoch: 447 [0/8000 (0%)]\tLoss: 29.848368\n",
      "Loss tensor(1911.1117, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1691.3608, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 447 [1280/8000 (16%)]\tLoss: 29.680695\n",
      "Train Epoch: 447 [2560/8000 (32%)]\tLoss: 30.206642\n",
      "Train Epoch: 447 [3840/8000 (48%)]\tLoss: 30.106148\n",
      "Train Epoch: 447 [5120/8000 (64%)]\tLoss: 29.683073\n",
      "Train Epoch: 447 [6400/8000 (80%)]\tLoss: 30.047327\n",
      "Train Epoch: 447 [7680/8000 (96%)]\tLoss: 30.137426\n",
      "====> Epoch: 447 Average loss: 29.9736\n",
      "Train Epoch: 448 [0/8000 (0%)]\tLoss: 30.061285\n",
      "Loss tensor(1901.4575, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1691.2983, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 448 [1280/8000 (16%)]\tLoss: 30.083963\n",
      "Train Epoch: 448 [2560/8000 (32%)]\tLoss: 29.885946\n",
      "Train Epoch: 448 [3840/8000 (48%)]\tLoss: 29.458521\n",
      "Train Epoch: 448 [5120/8000 (64%)]\tLoss: 30.074516\n",
      "Train Epoch: 448 [6400/8000 (80%)]\tLoss: 30.272154\n",
      "Train Epoch: 448 [7680/8000 (96%)]\tLoss: 30.058197\n",
      "====> Epoch: 448 Average loss: 29.9588\n",
      "Train Epoch: 449 [0/8000 (0%)]\tLoss: 29.950384\n",
      "Train Epoch: 449 [1280/8000 (16%)]\tLoss: 29.894480\n",
      "Train Epoch: 449 [2560/8000 (32%)]\tLoss: 29.905987\n",
      "Train Epoch: 449 [3840/8000 (48%)]\tLoss: 30.239372\n",
      "Train Epoch: 449 [5120/8000 (64%)]\tLoss: 29.923946\n",
      "Train Epoch: 449 [6400/8000 (80%)]\tLoss: 30.309240\n",
      "Train Epoch: 449 [7680/8000 (96%)]\tLoss: 29.953896\n",
      "====> Epoch: 449 Average loss: 29.9573\n",
      "Train Epoch: 450 [0/8000 (0%)]\tLoss: 30.002167\n",
      "Loss tensor(1921.6831, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1687.4812, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 450 [1280/8000 (16%)]\tLoss: 29.842182\n",
      "Loss tensor(1930.8228, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1683.8600, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 450 [2560/8000 (32%)]\tLoss: 29.998846\n",
      "Loss tensor(1905.3182, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1678.1228, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 450 [3840/8000 (48%)]\tLoss: 30.209755\n",
      "Train Epoch: 450 [5120/8000 (64%)]\tLoss: 29.933163\n",
      "Train Epoch: 450 [6400/8000 (80%)]\tLoss: 30.109594\n",
      "Train Epoch: 450 [7680/8000 (96%)]\tLoss: 30.055904\n",
      "====> Epoch: 450 Average loss: 29.9488\n",
      "Train Epoch: 451 [0/8000 (0%)]\tLoss: 29.885368\n",
      "Train Epoch: 451 [1280/8000 (16%)]\tLoss: 29.889185\n",
      "Train Epoch: 451 [2560/8000 (32%)]\tLoss: 29.865904\n",
      "Train Epoch: 451 [3840/8000 (48%)]\tLoss: 29.749094\n",
      "Train Epoch: 451 [5120/8000 (64%)]\tLoss: 30.102758\n",
      "Train Epoch: 451 [6400/8000 (80%)]\tLoss: 29.943413\n",
      "Train Epoch: 451 [7680/8000 (96%)]\tLoss: 29.956314\n",
      "====> Epoch: 451 Average loss: 29.9543\n",
      "Train Epoch: 452 [0/8000 (0%)]\tLoss: 29.939705\n",
      "Train Epoch: 452 [1280/8000 (16%)]\tLoss: 29.853456\n",
      "Train Epoch: 452 [2560/8000 (32%)]\tLoss: 29.929665\n",
      "Train Epoch: 452 [3840/8000 (48%)]\tLoss: 30.069366\n",
      "Train Epoch: 452 [5120/8000 (64%)]\tLoss: 29.766127\n",
      "Loss tensor(1932.0447, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1693.5288, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 452 [6400/8000 (80%)]\tLoss: 29.890100\n",
      "Loss tensor(1905.5085, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1692.4293, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 452 [7680/8000 (96%)]\tLoss: 30.044472\n",
      "====> Epoch: 452 Average loss: 29.9432\n",
      "Train Epoch: 453 [0/8000 (0%)]\tLoss: 30.486950\n",
      "Loss tensor(1904.3203, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1690.4626, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 453 [1280/8000 (16%)]\tLoss: 29.964909\n",
      "Train Epoch: 453 [2560/8000 (32%)]\tLoss: 29.940086\n",
      "Train Epoch: 453 [3840/8000 (48%)]\tLoss: 30.008886\n",
      "Train Epoch: 453 [5120/8000 (64%)]\tLoss: 29.979174\n",
      "Train Epoch: 453 [6400/8000 (80%)]\tLoss: 30.276838\n",
      "Loss tensor(1931.7971, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1684.9618, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 453 [7680/8000 (96%)]\tLoss: 30.047953\n",
      "====> Epoch: 453 Average loss: 29.9755\n",
      "Train Epoch: 454 [0/8000 (0%)]\tLoss: 30.162273\n",
      "Train Epoch: 454 [1280/8000 (16%)]\tLoss: 29.899300\n",
      "Train Epoch: 454 [2560/8000 (32%)]\tLoss: 29.875010\n",
      "Train Epoch: 454 [3840/8000 (48%)]\tLoss: 29.945002\n",
      "Loss tensor(1919.2731, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1677.0989, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 454 [5120/8000 (64%)]\tLoss: 30.213894\n",
      "Train Epoch: 454 [6400/8000 (80%)]\tLoss: 30.101448\n",
      "Train Epoch: 454 [7680/8000 (96%)]\tLoss: 29.894848\n",
      "====> Epoch: 454 Average loss: 29.9599\n",
      "Train Epoch: 455 [0/8000 (0%)]\tLoss: 29.978296\n",
      "Train Epoch: 455 [1280/8000 (16%)]\tLoss: 29.860962\n",
      "Train Epoch: 455 [2560/8000 (32%)]\tLoss: 30.190376\n",
      "Train Epoch: 455 [3840/8000 (48%)]\tLoss: 30.165989\n",
      "Train Epoch: 455 [5120/8000 (64%)]\tLoss: 29.767046\n",
      "Train Epoch: 455 [6400/8000 (80%)]\tLoss: 29.786322\n",
      "Train Epoch: 455 [7680/8000 (96%)]\tLoss: 30.009888\n",
      "====> Epoch: 455 Average loss: 29.9472\n",
      "Train Epoch: 456 [0/8000 (0%)]\tLoss: 30.010529\n",
      "Train Epoch: 456 [1280/8000 (16%)]\tLoss: 29.931334\n",
      "Loss tensor(1912.2561, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1684.0615, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 456 [2560/8000 (32%)]\tLoss: 29.759777\n",
      "Loss tensor(1910.0471, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1695.2677, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 456 [3840/8000 (48%)]\tLoss: 30.014685\n",
      "Train Epoch: 456 [5120/8000 (64%)]\tLoss: 29.998718\n",
      "Train Epoch: 456 [6400/8000 (80%)]\tLoss: 30.012922\n",
      "Train Epoch: 456 [7680/8000 (96%)]\tLoss: 30.177605\n",
      "====> Epoch: 456 Average loss: 29.9524\n",
      "Train Epoch: 457 [0/8000 (0%)]\tLoss: 29.965776\n",
      "Loss tensor(1917.9543, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1698.0852, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 457 [1280/8000 (16%)]\tLoss: 29.959944\n",
      "Train Epoch: 457 [2560/8000 (32%)]\tLoss: 29.881926\n",
      "Train Epoch: 457 [3840/8000 (48%)]\tLoss: 29.881592\n",
      "Train Epoch: 457 [5120/8000 (64%)]\tLoss: 29.898018\n",
      "Train Epoch: 457 [6400/8000 (80%)]\tLoss: 30.051016\n",
      "Loss tensor(1892.3396, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1744.1045, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 457 [7680/8000 (96%)]\tLoss: 29.958229\n",
      "====> Epoch: 457 Average loss: 29.9505\n",
      "Train Epoch: 458 [0/8000 (0%)]\tLoss: 29.937185\n",
      "Train Epoch: 458 [1280/8000 (16%)]\tLoss: 30.086891\n",
      "Loss tensor(1927.6011, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1746.0193, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 458 [2560/8000 (32%)]\tLoss: 29.714897\n",
      "Train Epoch: 458 [3840/8000 (48%)]\tLoss: 30.147627\n",
      "Train Epoch: 458 [5120/8000 (64%)]\tLoss: 30.087046\n",
      "Train Epoch: 458 [6400/8000 (80%)]\tLoss: 29.970940\n",
      "Train Epoch: 458 [7680/8000 (96%)]\tLoss: 30.085392\n",
      "====> Epoch: 458 Average loss: 29.9395\n",
      "Train Epoch: 459 [0/8000 (0%)]\tLoss: 30.146627\n",
      "Train Epoch: 459 [1280/8000 (16%)]\tLoss: 29.861691\n",
      "Train Epoch: 459 [2560/8000 (32%)]\tLoss: 29.985727\n",
      "Loss tensor(1920.1370, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1734.4875, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 459 [3840/8000 (48%)]\tLoss: 29.911745\n",
      "Train Epoch: 459 [5120/8000 (64%)]\tLoss: 29.898811\n",
      "Train Epoch: 459 [6400/8000 (80%)]\tLoss: 30.037510\n",
      "Train Epoch: 459 [7680/8000 (96%)]\tLoss: 30.124342\n",
      "====> Epoch: 459 Average loss: 29.9529\n",
      "Train Epoch: 460 [0/8000 (0%)]\tLoss: 30.011002\n",
      "Train Epoch: 460 [1280/8000 (16%)]\tLoss: 29.860279\n",
      "Loss tensor(1911.6289, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1711.0360, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 460 [2560/8000 (32%)]\tLoss: 29.779577\n",
      "Train Epoch: 460 [3840/8000 (48%)]\tLoss: 29.707998\n",
      "Train Epoch: 460 [5120/8000 (64%)]\tLoss: 29.851313\n",
      "Loss tensor(1917.8058, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1721.6252, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 460 [6400/8000 (80%)]\tLoss: 29.965715\n",
      "Train Epoch: 460 [7680/8000 (96%)]\tLoss: 29.582836\n",
      "====> Epoch: 460 Average loss: 29.9400\n",
      "Train Epoch: 461 [0/8000 (0%)]\tLoss: 29.922255\n",
      "Train Epoch: 461 [1280/8000 (16%)]\tLoss: 29.790005\n",
      "Train Epoch: 461 [2560/8000 (32%)]\tLoss: 30.335182\n",
      "Train Epoch: 461 [3840/8000 (48%)]\tLoss: 29.778776\n",
      "Train Epoch: 461 [5120/8000 (64%)]\tLoss: 29.968821\n",
      "Train Epoch: 461 [6400/8000 (80%)]\tLoss: 30.123461\n",
      "Train Epoch: 461 [7680/8000 (96%)]\tLoss: 29.893961\n",
      "====> Epoch: 461 Average loss: 29.9356\n",
      "Train Epoch: 462 [0/8000 (0%)]\tLoss: 30.175003\n",
      "Train Epoch: 462 [1280/8000 (16%)]\tLoss: 29.822948\n",
      "Train Epoch: 462 [2560/8000 (32%)]\tLoss: 29.941406\n",
      "Train Epoch: 462 [3840/8000 (48%)]\tLoss: 29.767189\n",
      "Train Epoch: 462 [5120/8000 (64%)]\tLoss: 29.957970\n",
      "Train Epoch: 462 [6400/8000 (80%)]\tLoss: 29.638020\n",
      "Loss tensor(1916.6002, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1724.3597, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1905.5438, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1723.5105, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 462 [7680/8000 (96%)]\tLoss: 29.895647\n",
      "====> Epoch: 462 Average loss: 29.9357\n",
      "Train Epoch: 463 [0/8000 (0%)]\tLoss: 29.847494\n",
      "Train Epoch: 463 [1280/8000 (16%)]\tLoss: 29.937101\n",
      "Train Epoch: 463 [2560/8000 (32%)]\tLoss: 29.668678\n",
      "Train Epoch: 463 [3840/8000 (48%)]\tLoss: 30.094927\n",
      "Train Epoch: 463 [5120/8000 (64%)]\tLoss: 29.968086\n",
      "Train Epoch: 463 [6400/8000 (80%)]\tLoss: 30.329975\n",
      "Train Epoch: 463 [7680/8000 (96%)]\tLoss: 29.987692\n",
      "====> Epoch: 463 Average loss: 29.9382\n",
      "Train Epoch: 464 [0/8000 (0%)]\tLoss: 30.019716\n",
      "Train Epoch: 464 [1280/8000 (16%)]\tLoss: 29.654398\n",
      "Train Epoch: 464 [2560/8000 (32%)]\tLoss: 30.212891\n",
      "Train Epoch: 464 [3840/8000 (48%)]\tLoss: 29.899342\n",
      "Train Epoch: 464 [5120/8000 (64%)]\tLoss: 30.219894\n",
      "Train Epoch: 464 [6400/8000 (80%)]\tLoss: 29.963928\n",
      "Train Epoch: 464 [7680/8000 (96%)]\tLoss: 29.948303\n",
      "====> Epoch: 464 Average loss: 29.9309\n",
      "Train Epoch: 465 [0/8000 (0%)]\tLoss: 29.775618\n",
      "Train Epoch: 465 [1280/8000 (16%)]\tLoss: 29.847862\n",
      "Train Epoch: 465 [2560/8000 (32%)]\tLoss: 29.935316\n",
      "Train Epoch: 465 [3840/8000 (48%)]\tLoss: 30.053825\n",
      "Train Epoch: 465 [5120/8000 (64%)]\tLoss: 29.559370\n",
      "Loss tensor(1900.7882, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1721.4894, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 465 [6400/8000 (80%)]\tLoss: 29.849443\n",
      "Train Epoch: 465 [7680/8000 (96%)]\tLoss: 29.908031\n",
      "====> Epoch: 465 Average loss: 29.9218\n",
      "Train Epoch: 466 [0/8000 (0%)]\tLoss: 29.890566\n",
      "Train Epoch: 466 [1280/8000 (16%)]\tLoss: 29.688278\n",
      "Loss tensor(1936.3762, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1709.0005, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 466 [2560/8000 (32%)]\tLoss: 29.758492\n",
      "Train Epoch: 466 [3840/8000 (48%)]\tLoss: 30.166470\n",
      "Train Epoch: 466 [5120/8000 (64%)]\tLoss: 29.783258\n",
      "Train Epoch: 466 [6400/8000 (80%)]\tLoss: 30.015699\n",
      "Train Epoch: 466 [7680/8000 (96%)]\tLoss: 29.883673\n",
      "====> Epoch: 466 Average loss: 29.9377\n",
      "Train Epoch: 467 [0/8000 (0%)]\tLoss: 29.631199\n",
      "Train Epoch: 467 [1280/8000 (16%)]\tLoss: 29.997484\n",
      "Loss tensor(1929.8251, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1719.5378, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1923.9900, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1725.9349, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 467 [2560/8000 (32%)]\tLoss: 29.587231\n",
      "Train Epoch: 467 [3840/8000 (48%)]\tLoss: 30.099831\n",
      "Train Epoch: 467 [5120/8000 (64%)]\tLoss: 30.251898\n",
      "Train Epoch: 467 [6400/8000 (80%)]\tLoss: 29.741869\n",
      "Train Epoch: 467 [7680/8000 (96%)]\tLoss: 29.630772\n",
      "====> Epoch: 467 Average loss: 29.9327\n",
      "Train Epoch: 468 [0/8000 (0%)]\tLoss: 29.727110\n",
      "Train Epoch: 468 [1280/8000 (16%)]\tLoss: 29.658768\n",
      "Train Epoch: 468 [2560/8000 (32%)]\tLoss: 29.856064\n",
      "Train Epoch: 468 [3840/8000 (48%)]\tLoss: 29.731094\n",
      "Train Epoch: 468 [5120/8000 (64%)]\tLoss: 30.071037\n",
      "Train Epoch: 468 [6400/8000 (80%)]\tLoss: 29.730646\n",
      "Train Epoch: 468 [7680/8000 (96%)]\tLoss: 30.064535\n",
      "====> Epoch: 468 Average loss: 29.9395\n",
      "Train Epoch: 469 [0/8000 (0%)]\tLoss: 29.818682\n",
      "Train Epoch: 469 [1280/8000 (16%)]\tLoss: 30.012089\n",
      "Train Epoch: 469 [2560/8000 (32%)]\tLoss: 30.098333\n",
      "Train Epoch: 469 [3840/8000 (48%)]\tLoss: 29.972778\n",
      "Train Epoch: 469 [5120/8000 (64%)]\tLoss: 29.934866\n",
      "Train Epoch: 469 [6400/8000 (80%)]\tLoss: 29.848349\n",
      "Loss tensor(1933.4413, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1704.6705, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 469 [7680/8000 (96%)]\tLoss: 29.818575\n",
      "====> Epoch: 469 Average loss: 29.9474\n",
      "Train Epoch: 470 [0/8000 (0%)]\tLoss: 29.784529\n",
      "Train Epoch: 470 [1280/8000 (16%)]\tLoss: 29.600487\n",
      "Loss tensor(1910.2448, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1715.9298, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 470 [2560/8000 (32%)]\tLoss: 29.947298\n",
      "Loss tensor(1899.9891, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1718.7292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1928.9167, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1716.4143, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 470 [3840/8000 (48%)]\tLoss: 29.541645\n",
      "Train Epoch: 470 [5120/8000 (64%)]\tLoss: 30.163269\n",
      "Train Epoch: 470 [6400/8000 (80%)]\tLoss: 29.953028\n",
      "Train Epoch: 470 [7680/8000 (96%)]\tLoss: 30.112173\n",
      "====> Epoch: 470 Average loss: 29.9326\n",
      "Train Epoch: 471 [0/8000 (0%)]\tLoss: 29.969330\n",
      "Train Epoch: 471 [1280/8000 (16%)]\tLoss: 29.869514\n",
      "Train Epoch: 471 [2560/8000 (32%)]\tLoss: 29.826462\n",
      "Loss tensor(1943.3535, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1692.6407, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 471 [3840/8000 (48%)]\tLoss: 30.107958\n",
      "Loss tensor(1929.1532, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1697.3749, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 471 [5120/8000 (64%)]\tLoss: 29.749397\n",
      "Loss tensor(1907.8933, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1694.2853, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 471 [6400/8000 (80%)]\tLoss: 29.608513\n",
      "Loss tensor(1929.6917, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1700.3948, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1921.9885, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1712.8958, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 471 [7680/8000 (96%)]\tLoss: 29.923843\n",
      "====> Epoch: 471 Average loss: 29.9443\n",
      "Train Epoch: 472 [0/8000 (0%)]\tLoss: 29.750772\n",
      "Train Epoch: 472 [1280/8000 (16%)]\tLoss: 29.710794\n",
      "Train Epoch: 472 [2560/8000 (32%)]\tLoss: 30.007429\n",
      "Train Epoch: 472 [3840/8000 (48%)]\tLoss: 29.845518\n",
      "Train Epoch: 472 [5120/8000 (64%)]\tLoss: 29.971590\n",
      "Train Epoch: 472 [6400/8000 (80%)]\tLoss: 29.918272\n",
      "Train Epoch: 472 [7680/8000 (96%)]\tLoss: 29.855347\n",
      "====> Epoch: 472 Average loss: 29.9231\n",
      "Train Epoch: 473 [0/8000 (0%)]\tLoss: 29.908691\n",
      "Train Epoch: 473 [1280/8000 (16%)]\tLoss: 29.681366\n",
      "Loss tensor(1909.2217, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1720.4979, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 473 [2560/8000 (32%)]\tLoss: 29.749590\n",
      "Train Epoch: 473 [3840/8000 (48%)]\tLoss: 30.077480\n",
      "Train Epoch: 473 [5120/8000 (64%)]\tLoss: 29.561283\n",
      "Train Epoch: 473 [6400/8000 (80%)]\tLoss: 30.184534\n",
      "Train Epoch: 473 [7680/8000 (96%)]\tLoss: 30.033491\n",
      "====> Epoch: 473 Average loss: 29.9395\n",
      "Train Epoch: 474 [0/8000 (0%)]\tLoss: 29.861637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 474 [1280/8000 (16%)]\tLoss: 30.013443\n",
      "Train Epoch: 474 [2560/8000 (32%)]\tLoss: 30.040936\n",
      "Loss tensor(1918.5786, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1712.7635, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 474 [3840/8000 (48%)]\tLoss: 29.831930\n",
      "Train Epoch: 474 [5120/8000 (64%)]\tLoss: 30.353001\n",
      "Train Epoch: 474 [6400/8000 (80%)]\tLoss: 30.025524\n",
      "Train Epoch: 474 [7680/8000 (96%)]\tLoss: 29.684610\n",
      "Loss tensor(1910.0750, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1714.2855, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 474 Average loss: 29.9298\n",
      "Train Epoch: 475 [0/8000 (0%)]\tLoss: 30.012859\n",
      "Train Epoch: 475 [1280/8000 (16%)]\tLoss: 29.885508\n",
      "Train Epoch: 475 [2560/8000 (32%)]\tLoss: 29.798548\n",
      "Train Epoch: 475 [3840/8000 (48%)]\tLoss: 30.055824\n",
      "Train Epoch: 475 [5120/8000 (64%)]\tLoss: 30.175537\n",
      "Loss tensor(1916.8287, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1716.3883, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 475 [6400/8000 (80%)]\tLoss: 30.231649\n",
      "Train Epoch: 475 [7680/8000 (96%)]\tLoss: 29.765244\n",
      "====> Epoch: 475 Average loss: 29.9246\n",
      "Train Epoch: 476 [0/8000 (0%)]\tLoss: 29.676203\n",
      "Train Epoch: 476 [1280/8000 (16%)]\tLoss: 30.027714\n",
      "Train Epoch: 476 [2560/8000 (32%)]\tLoss: 29.752644\n",
      "Train Epoch: 476 [3840/8000 (48%)]\tLoss: 29.974150\n",
      "Train Epoch: 476 [5120/8000 (64%)]\tLoss: 29.830187\n",
      "Train Epoch: 476 [6400/8000 (80%)]\tLoss: 30.043163\n",
      "Train Epoch: 476 [7680/8000 (96%)]\tLoss: 29.687706\n",
      "====> Epoch: 476 Average loss: 29.9179\n",
      "Train Epoch: 477 [0/8000 (0%)]\tLoss: 29.928110\n",
      "Train Epoch: 477 [1280/8000 (16%)]\tLoss: 29.833984\n",
      "Train Epoch: 477 [2560/8000 (32%)]\tLoss: 29.760000\n",
      "Loss tensor(1915.7366, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1708.4113, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 477 [3840/8000 (48%)]\tLoss: 30.124660\n",
      "Train Epoch: 477 [5120/8000 (64%)]\tLoss: 29.984476\n",
      "Train Epoch: 477 [6400/8000 (80%)]\tLoss: 30.443190\n",
      "Train Epoch: 477 [7680/8000 (96%)]\tLoss: 30.115191\n",
      "====> Epoch: 477 Average loss: 29.9263\n",
      "Train Epoch: 478 [0/8000 (0%)]\tLoss: 29.632278\n",
      "Train Epoch: 478 [1280/8000 (16%)]\tLoss: 30.027388\n",
      "Train Epoch: 478 [2560/8000 (32%)]\tLoss: 29.810928\n",
      "Loss tensor(1913.4575, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1719.2487, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 478 [3840/8000 (48%)]\tLoss: 30.298456\n",
      "Train Epoch: 478 [5120/8000 (64%)]\tLoss: 30.040792\n",
      "Train Epoch: 478 [6400/8000 (80%)]\tLoss: 29.847897\n",
      "Train Epoch: 478 [7680/8000 (96%)]\tLoss: 29.825216\n",
      "====> Epoch: 478 Average loss: 29.9178\n",
      "Train Epoch: 479 [0/8000 (0%)]\tLoss: 30.014790\n",
      "Train Epoch: 479 [1280/8000 (16%)]\tLoss: 29.995886\n",
      "Train Epoch: 479 [2560/8000 (32%)]\tLoss: 29.999296\n",
      "Train Epoch: 479 [3840/8000 (48%)]\tLoss: 29.819645\n",
      "Train Epoch: 479 [5120/8000 (64%)]\tLoss: 29.963140\n",
      "Train Epoch: 479 [6400/8000 (80%)]\tLoss: 29.599463\n",
      "Loss tensor(1915.0980, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1716.9250, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 479 [7680/8000 (96%)]\tLoss: 29.733337\n",
      "Loss tensor(1901.9603, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1722.6309, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "====> Epoch: 479 Average loss: 29.9170\n",
      "Train Epoch: 480 [0/8000 (0%)]\tLoss: 29.684088\n",
      "Loss tensor(1909.3188, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1714.2451, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1919.8252, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1719.0505, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 480 [1280/8000 (16%)]\tLoss: 29.968557\n",
      "Train Epoch: 480 [2560/8000 (32%)]\tLoss: 30.285212\n",
      "Train Epoch: 480 [3840/8000 (48%)]\tLoss: 30.114206\n",
      "Loss tensor(1908.4244, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1723.3910, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Loss tensor(1903.1945, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss tensor(1722.0735, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Train Epoch: 480 [5120/8000 (64%)]\tLoss: 29.730091\n",
      "Train Epoch: 480 [6400/8000 (80%)]\tLoss: 29.767406\n",
      "Train Epoch: 480 [7680/8000 (96%)]\tLoss: 29.943336\n",
      "====> Epoch: 480 Average loss: 29.9160\n",
      "BURN IN DEBUG\n",
      "Going post burn in\n",
      "Train Epoch: 481 [0/8000 (0%)]\tLoss: 29.836689\n",
      "Train Epoch: 481 [1280/8000 (16%)]\tLoss: 29.698139\n",
      "Train Epoch: 481 [2560/8000 (32%)]\tLoss: 30.144836\n",
      "Train Epoch: 481 [3840/8000 (48%)]\tLoss: 29.765034\n",
      "Train Epoch: 481 [5120/8000 (64%)]\tLoss: 30.345858\n",
      "Train Epoch: 481 [6400/8000 (80%)]\tLoss: 29.976202\n",
      "Train Epoch: 481 [7680/8000 (96%)]\tLoss: 30.015226\n",
      "====> Epoch: 481 Average loss: 29.9123\n",
      "Train Epoch: 482 [0/8000 (0%)]\tLoss: 30.107174\n",
      "Train Epoch: 482 [1280/8000 (16%)]\tLoss: 29.746933\n",
      "Train Epoch: 482 [2560/8000 (32%)]\tLoss: 29.801693\n",
      "Train Epoch: 482 [3840/8000 (48%)]\tLoss: 29.863398\n",
      "Train Epoch: 482 [5120/8000 (64%)]\tLoss: 30.004278\n",
      "Train Epoch: 482 [6400/8000 (80%)]\tLoss: 29.909824\n",
      "Loss tensor(1937.4089, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 482 [7680/8000 (96%)]\tLoss: 29.909443\n",
      "====> Epoch: 482 Average loss: 29.9006\n",
      "Train Epoch: 483 [0/8000 (0%)]\tLoss: 29.694868\n",
      "Train Epoch: 483 [1280/8000 (16%)]\tLoss: 29.918238\n",
      "Train Epoch: 483 [2560/8000 (32%)]\tLoss: 29.730021\n",
      "Train Epoch: 483 [3840/8000 (48%)]\tLoss: 29.912649\n",
      "Train Epoch: 483 [5120/8000 (64%)]\tLoss: 29.822947\n",
      "Train Epoch: 483 [6400/8000 (80%)]\tLoss: 29.849907\n",
      "Train Epoch: 483 [7680/8000 (96%)]\tLoss: 29.804853\n",
      "====> Epoch: 483 Average loss: 29.9079\n",
      "Train Epoch: 484 [0/8000 (0%)]\tLoss: 29.880333\n",
      "Train Epoch: 484 [1280/8000 (16%)]\tLoss: 30.047262\n",
      "Train Epoch: 484 [2560/8000 (32%)]\tLoss: 29.800549\n",
      "Train Epoch: 484 [3840/8000 (48%)]\tLoss: 29.889948\n",
      "Train Epoch: 484 [5120/8000 (64%)]\tLoss: 29.952085\n",
      "Train Epoch: 484 [6400/8000 (80%)]\tLoss: 29.808002\n",
      "Train Epoch: 484 [7680/8000 (96%)]\tLoss: 29.697792\n",
      "====> Epoch: 484 Average loss: 29.9166\n",
      "Train Epoch: 485 [0/8000 (0%)]\tLoss: 29.652399\n",
      "Train Epoch: 485 [1280/8000 (16%)]\tLoss: 30.035666\n",
      "Train Epoch: 485 [2560/8000 (32%)]\tLoss: 29.722921\n",
      "Train Epoch: 485 [3840/8000 (48%)]\tLoss: 30.195478\n",
      "Train Epoch: 485 [5120/8000 (64%)]\tLoss: 29.902800\n",
      "Loss tensor(1902.2024, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 485 [6400/8000 (80%)]\tLoss: 29.916891\n",
      "Train Epoch: 485 [7680/8000 (96%)]\tLoss: 30.079971\n",
      "====> Epoch: 485 Average loss: 29.9095\n",
      "Train Epoch: 486 [0/8000 (0%)]\tLoss: 29.511435\n",
      "Train Epoch: 486 [1280/8000 (16%)]\tLoss: 29.720631\n",
      "Train Epoch: 486 [2560/8000 (32%)]\tLoss: 29.736021\n",
      "Train Epoch: 486 [3840/8000 (48%)]\tLoss: 30.099844\n",
      "Train Epoch: 486 [5120/8000 (64%)]\tLoss: 30.106653\n",
      "Train Epoch: 486 [6400/8000 (80%)]\tLoss: 30.061920\n",
      "Loss tensor(1921.8605, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 486 [7680/8000 (96%)]\tLoss: 29.852533\n",
      "====> Epoch: 486 Average loss: 29.8990\n",
      "Train Epoch: 487 [0/8000 (0%)]\tLoss: 29.853861\n",
      "Train Epoch: 487 [1280/8000 (16%)]\tLoss: 30.025286\n",
      "Train Epoch: 487 [2560/8000 (32%)]\tLoss: 29.878885\n",
      "Loss tensor(1931.2573, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1915.7507, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 487 [3840/8000 (48%)]\tLoss: 29.785532\n",
      "Train Epoch: 487 [5120/8000 (64%)]\tLoss: 29.806477\n",
      "Train Epoch: 487 [6400/8000 (80%)]\tLoss: 30.267775\n",
      "Train Epoch: 487 [7680/8000 (96%)]\tLoss: 29.796280\n",
      "====> Epoch: 487 Average loss: 29.9063\n",
      "Train Epoch: 488 [0/8000 (0%)]\tLoss: 30.044697\n",
      "Train Epoch: 488 [1280/8000 (16%)]\tLoss: 29.803520\n",
      "Train Epoch: 488 [2560/8000 (32%)]\tLoss: 30.153927\n",
      "Train Epoch: 488 [3840/8000 (48%)]\tLoss: 29.954439\n",
      "Train Epoch: 488 [5120/8000 (64%)]\tLoss: 29.697081\n",
      "Train Epoch: 488 [6400/8000 (80%)]\tLoss: 30.091845\n",
      "Train Epoch: 488 [7680/8000 (96%)]\tLoss: 29.594122\n",
      "====> Epoch: 488 Average loss: 29.8946\n",
      "Train Epoch: 489 [0/8000 (0%)]\tLoss: 29.997976\n",
      "Loss tensor(1915., device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 489 [1280/8000 (16%)]\tLoss: 30.097284\n",
      "Train Epoch: 489 [2560/8000 (32%)]\tLoss: 29.742258\n",
      "Train Epoch: 489 [3840/8000 (48%)]\tLoss: 29.839661\n",
      "Train Epoch: 489 [5120/8000 (64%)]\tLoss: 29.865143\n",
      "Train Epoch: 489 [6400/8000 (80%)]\tLoss: 29.914696\n",
      "Train Epoch: 489 [7680/8000 (96%)]\tLoss: 30.152582\n",
      "====> Epoch: 489 Average loss: 29.8998\n",
      "Train Epoch: 490 [0/8000 (0%)]\tLoss: 29.898020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 490 [1280/8000 (16%)]\tLoss: 30.120070\n",
      "Train Epoch: 490 [2560/8000 (32%)]\tLoss: 29.986668\n",
      "Train Epoch: 490 [3840/8000 (48%)]\tLoss: 29.544451\n",
      "Train Epoch: 490 [5120/8000 (64%)]\tLoss: 29.614172\n",
      "Train Epoch: 490 [6400/8000 (80%)]\tLoss: 30.020945\n",
      "Train Epoch: 490 [7680/8000 (96%)]\tLoss: 30.005817\n",
      "====> Epoch: 490 Average loss: 29.8951\n",
      "Train Epoch: 491 [0/8000 (0%)]\tLoss: 29.939331\n",
      "Train Epoch: 491 [1280/8000 (16%)]\tLoss: 29.596500\n",
      "Train Epoch: 491 [2560/8000 (32%)]\tLoss: 29.894768\n",
      "Train Epoch: 491 [3840/8000 (48%)]\tLoss: 29.826548\n",
      "Train Epoch: 491 [5120/8000 (64%)]\tLoss: 29.851933\n",
      "Train Epoch: 491 [6400/8000 (80%)]\tLoss: 29.854401\n",
      "Train Epoch: 491 [7680/8000 (96%)]\tLoss: 29.944401\n",
      "====> Epoch: 491 Average loss: 29.8981\n",
      "Train Epoch: 492 [0/8000 (0%)]\tLoss: 29.870298\n",
      "Train Epoch: 492 [1280/8000 (16%)]\tLoss: 29.720686\n",
      "Train Epoch: 492 [2560/8000 (32%)]\tLoss: 29.657915\n",
      "Loss tensor(1923.7301, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 492 [3840/8000 (48%)]\tLoss: 30.058283\n",
      "Train Epoch: 492 [5120/8000 (64%)]\tLoss: 29.778582\n",
      "Train Epoch: 492 [6400/8000 (80%)]\tLoss: 29.966549\n",
      "Train Epoch: 492 [7680/8000 (96%)]\tLoss: 29.817526\n",
      "====> Epoch: 492 Average loss: 29.8928\n",
      "Train Epoch: 493 [0/8000 (0%)]\tLoss: 30.094744\n",
      "Loss tensor(1906.0297, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 493 [1280/8000 (16%)]\tLoss: 29.812223\n",
      "Train Epoch: 493 [2560/8000 (32%)]\tLoss: 30.200701\n",
      "Train Epoch: 493 [3840/8000 (48%)]\tLoss: 29.919071\n",
      "Train Epoch: 493 [5120/8000 (64%)]\tLoss: 30.003551\n",
      "Loss tensor(1908.4287, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 493 [6400/8000 (80%)]\tLoss: 29.733025\n",
      "Train Epoch: 493 [7680/8000 (96%)]\tLoss: 29.913723\n",
      "====> Epoch: 493 Average loss: 29.8971\n",
      "Train Epoch: 494 [0/8000 (0%)]\tLoss: 30.164085\n",
      "Train Epoch: 494 [1280/8000 (16%)]\tLoss: 29.869495\n",
      "Loss tensor(1894.7190, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 494 [2560/8000 (32%)]\tLoss: 29.715693\n",
      "Train Epoch: 494 [3840/8000 (48%)]\tLoss: 30.188643\n",
      "Train Epoch: 494 [5120/8000 (64%)]\tLoss: 30.125303\n",
      "Train Epoch: 494 [6400/8000 (80%)]\tLoss: 30.061401\n",
      "Loss tensor(1907.2203, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 494 [7680/8000 (96%)]\tLoss: 29.943350\n",
      "====> Epoch: 494 Average loss: 29.8910\n",
      "Train Epoch: 495 [0/8000 (0%)]\tLoss: 30.164574\n",
      "Loss tensor(1926.5225, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 495 [1280/8000 (16%)]\tLoss: 30.147593\n",
      "Train Epoch: 495 [2560/8000 (32%)]\tLoss: 29.862309\n",
      "Train Epoch: 495 [3840/8000 (48%)]\tLoss: 30.048794\n",
      "Train Epoch: 495 [5120/8000 (64%)]\tLoss: 29.781412\n",
      "Train Epoch: 495 [6400/8000 (80%)]\tLoss: 29.930414\n",
      "Train Epoch: 495 [7680/8000 (96%)]\tLoss: 30.024569\n",
      "====> Epoch: 495 Average loss: 29.8865\n",
      "Train Epoch: 496 [0/8000 (0%)]\tLoss: 29.845856\n",
      "Train Epoch: 496 [1280/8000 (16%)]\tLoss: 29.777189\n",
      "Train Epoch: 496 [2560/8000 (32%)]\tLoss: 29.551662\n",
      "Train Epoch: 496 [3840/8000 (48%)]\tLoss: 30.021215\n",
      "Train Epoch: 496 [5120/8000 (64%)]\tLoss: 29.739712\n",
      "Loss tensor(1915.0447, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 496 [6400/8000 (80%)]\tLoss: 29.843197\n",
      "Train Epoch: 496 [7680/8000 (96%)]\tLoss: 29.947048\n",
      "====> Epoch: 496 Average loss: 29.8846\n",
      "Train Epoch: 497 [0/8000 (0%)]\tLoss: 29.668310\n",
      "Loss tensor(1898.0703, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 497 [1280/8000 (16%)]\tLoss: 29.824673\n",
      "Train Epoch: 497 [2560/8000 (32%)]\tLoss: 29.788229\n",
      "Train Epoch: 497 [3840/8000 (48%)]\tLoss: 30.012875\n",
      "Loss tensor(1905.6323, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 497 [5120/8000 (64%)]\tLoss: 30.253326\n",
      "Train Epoch: 497 [6400/8000 (80%)]\tLoss: 30.036648\n",
      "Loss tensor(1914.7545, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1929.2559, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 497 [7680/8000 (96%)]\tLoss: 29.415348\n",
      "====> Epoch: 497 Average loss: 29.8984\n",
      "Train Epoch: 498 [0/8000 (0%)]\tLoss: 29.727562\n",
      "Train Epoch: 498 [1280/8000 (16%)]\tLoss: 29.772343\n",
      "Train Epoch: 498 [2560/8000 (32%)]\tLoss: 29.790375\n",
      "Loss tensor(1913.9675, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1941.6985, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 498 [3840/8000 (48%)]\tLoss: 29.993254\n",
      "Train Epoch: 498 [5120/8000 (64%)]\tLoss: 29.851749\n",
      "Train Epoch: 498 [6400/8000 (80%)]\tLoss: 30.005331\n",
      "Train Epoch: 498 [7680/8000 (96%)]\tLoss: 29.838177\n",
      "====> Epoch: 498 Average loss: 29.8976\n",
      "Train Epoch: 499 [0/8000 (0%)]\tLoss: 29.557468\n",
      "Train Epoch: 499 [1280/8000 (16%)]\tLoss: 29.676334\n",
      "Loss tensor(1909.7399, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 499 [2560/8000 (32%)]\tLoss: 29.858320\n",
      "Train Epoch: 499 [3840/8000 (48%)]\tLoss: 29.730053\n",
      "Train Epoch: 499 [5120/8000 (64%)]\tLoss: 30.031961\n",
      "Train Epoch: 499 [6400/8000 (80%)]\tLoss: 29.818592\n",
      "Train Epoch: 499 [7680/8000 (96%)]\tLoss: 29.941910\n",
      "====> Epoch: 499 Average loss: 29.8936\n",
      "Train Epoch: 500 [0/8000 (0%)]\tLoss: 30.047747\n",
      "Train Epoch: 500 [1280/8000 (16%)]\tLoss: 29.901615\n",
      "Train Epoch: 500 [2560/8000 (32%)]\tLoss: 29.907562\n",
      "Train Epoch: 500 [3840/8000 (48%)]\tLoss: 29.864040\n",
      "Train Epoch: 500 [5120/8000 (64%)]\tLoss: 30.143936\n",
      "Loss tensor(1916.2490, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1927.0125, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 500 [6400/8000 (80%)]\tLoss: 30.109570\n",
      "Train Epoch: 500 [7680/8000 (96%)]\tLoss: 29.916792\n",
      "====> Epoch: 500 Average loss: 29.8975\n",
      "Train Epoch: 501 [0/8000 (0%)]\tLoss: 29.664740\n",
      "Loss tensor(1902.1036, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 501 [1280/8000 (16%)]\tLoss: 29.967598\n",
      "Train Epoch: 501 [2560/8000 (32%)]\tLoss: 29.792166\n",
      "Train Epoch: 501 [3840/8000 (48%)]\tLoss: 29.786434\n",
      "Train Epoch: 501 [5120/8000 (64%)]\tLoss: 29.905138\n",
      "Loss tensor(1891.2560, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1928.1001, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 501 [6400/8000 (80%)]\tLoss: 30.023411\n",
      "Train Epoch: 501 [7680/8000 (96%)]\tLoss: 29.697115\n",
      "====> Epoch: 501 Average loss: 29.8950\n",
      "Train Epoch: 502 [0/8000 (0%)]\tLoss: 30.146845\n",
      "Train Epoch: 502 [1280/8000 (16%)]\tLoss: 30.044662\n",
      "Train Epoch: 502 [2560/8000 (32%)]\tLoss: 29.966208\n",
      "Train Epoch: 502 [3840/8000 (48%)]\tLoss: 29.927553\n",
      "Train Epoch: 502 [5120/8000 (64%)]\tLoss: 29.924259\n",
      "Train Epoch: 502 [6400/8000 (80%)]\tLoss: 29.713228\n",
      "Train Epoch: 502 [7680/8000 (96%)]\tLoss: 30.104540\n",
      "====> Epoch: 502 Average loss: 29.8835\n",
      "Train Epoch: 503 [0/8000 (0%)]\tLoss: 29.838057\n",
      "Train Epoch: 503 [1280/8000 (16%)]\tLoss: 30.030556\n",
      "Train Epoch: 503 [2560/8000 (32%)]\tLoss: 29.849995\n",
      "Train Epoch: 503 [3840/8000 (48%)]\tLoss: 29.756565\n",
      "Train Epoch: 503 [5120/8000 (64%)]\tLoss: 29.835812\n",
      "Train Epoch: 503 [6400/8000 (80%)]\tLoss: 29.789665\n",
      "Train Epoch: 503 [7680/8000 (96%)]\tLoss: 29.964865\n",
      "====> Epoch: 503 Average loss: 29.8810\n",
      "Train Epoch: 504 [0/8000 (0%)]\tLoss: 29.631466\n",
      "Train Epoch: 504 [1280/8000 (16%)]\tLoss: 29.880253\n",
      "Train Epoch: 504 [2560/8000 (32%)]\tLoss: 29.828884\n",
      "Train Epoch: 504 [3840/8000 (48%)]\tLoss: 30.059446\n",
      "Train Epoch: 504 [5120/8000 (64%)]\tLoss: 30.037443\n",
      "Loss tensor(1886.5321, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 504 [6400/8000 (80%)]\tLoss: 29.770981\n",
      "Loss tensor(1880.3932, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 504 [7680/8000 (96%)]\tLoss: 29.929264\n",
      "====> Epoch: 504 Average loss: 29.8769\n",
      "Train Epoch: 505 [0/8000 (0%)]\tLoss: 29.965590\n",
      "Loss tensor(1911.5364, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 505 [1280/8000 (16%)]\tLoss: 29.791529\n",
      "Train Epoch: 505 [2560/8000 (32%)]\tLoss: 29.858955\n",
      "Train Epoch: 505 [3840/8000 (48%)]\tLoss: 29.885988\n",
      "Train Epoch: 505 [5120/8000 (64%)]\tLoss: 29.672691\n",
      "Train Epoch: 505 [6400/8000 (80%)]\tLoss: 29.898651\n",
      "Train Epoch: 505 [7680/8000 (96%)]\tLoss: 29.689859\n",
      "====> Epoch: 505 Average loss: 29.8800\n",
      "Train Epoch: 506 [0/8000 (0%)]\tLoss: 29.828533\n",
      "Train Epoch: 506 [1280/8000 (16%)]\tLoss: 29.818357\n",
      "Train Epoch: 506 [2560/8000 (32%)]\tLoss: 29.702618\n",
      "Train Epoch: 506 [3840/8000 (48%)]\tLoss: 29.696671\n",
      "Train Epoch: 506 [5120/8000 (64%)]\tLoss: 29.403738\n",
      "Train Epoch: 506 [6400/8000 (80%)]\tLoss: 29.712416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 506 [7680/8000 (96%)]\tLoss: 29.896231\n",
      "====> Epoch: 506 Average loss: 29.8799\n",
      "Train Epoch: 507 [0/8000 (0%)]\tLoss: 29.786327\n",
      "Loss tensor(1889.2432, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 507 [1280/8000 (16%)]\tLoss: 30.335651\n",
      "Loss tensor(1906.7241, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 507 [2560/8000 (32%)]\tLoss: 30.074484\n",
      "Loss tensor(1897.6080, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 507 [3840/8000 (48%)]\tLoss: 29.494829\n",
      "Loss tensor(1922.2661, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 507 [5120/8000 (64%)]\tLoss: 29.816689\n",
      "Loss tensor(1919.8910, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 507 [6400/8000 (80%)]\tLoss: 30.098656\n",
      "Train Epoch: 507 [7680/8000 (96%)]\tLoss: 29.997814\n",
      "====> Epoch: 507 Average loss: 29.8865\n",
      "Train Epoch: 508 [0/8000 (0%)]\tLoss: 30.062286\n",
      "Train Epoch: 508 [1280/8000 (16%)]\tLoss: 30.128443\n",
      "Train Epoch: 508 [2560/8000 (32%)]\tLoss: 29.676714\n",
      "Train Epoch: 508 [3840/8000 (48%)]\tLoss: 29.958122\n",
      "Loss tensor(1900.9144, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 508 [5120/8000 (64%)]\tLoss: 30.166267\n",
      "Train Epoch: 508 [6400/8000 (80%)]\tLoss: 29.788717\n",
      "Loss tensor(1920.0865, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 508 [7680/8000 (96%)]\tLoss: 29.654058\n",
      "Loss tensor(1920.2217, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "====> Epoch: 508 Average loss: 29.8691\n",
      "Train Epoch: 509 [0/8000 (0%)]\tLoss: 29.756037\n",
      "Loss tensor(1916.2975, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 509 [1280/8000 (16%)]\tLoss: 29.814148\n",
      "Train Epoch: 509 [2560/8000 (32%)]\tLoss: 29.788601\n",
      "Train Epoch: 509 [3840/8000 (48%)]\tLoss: 29.922316\n",
      "Train Epoch: 509 [5120/8000 (64%)]\tLoss: 29.842112\n",
      "Train Epoch: 509 [6400/8000 (80%)]\tLoss: 29.800623\n",
      "Train Epoch: 509 [7680/8000 (96%)]\tLoss: 29.720606\n",
      "====> Epoch: 509 Average loss: 29.8678\n",
      "Train Epoch: 510 [0/8000 (0%)]\tLoss: 29.748055\n",
      "Loss tensor(1908.5085, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 510 [1280/8000 (16%)]\tLoss: 29.630722\n",
      "Train Epoch: 510 [2560/8000 (32%)]\tLoss: 29.637636\n",
      "Train Epoch: 510 [3840/8000 (48%)]\tLoss: 30.072989\n",
      "Train Epoch: 510 [5120/8000 (64%)]\tLoss: 29.980661\n",
      "Loss tensor(1920.0211, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 510 [6400/8000 (80%)]\tLoss: 30.018959\n",
      "Train Epoch: 510 [7680/8000 (96%)]\tLoss: 29.757595\n",
      "====> Epoch: 510 Average loss: 29.8808\n",
      "Train Epoch: 511 [0/8000 (0%)]\tLoss: 29.887280\n",
      "Train Epoch: 511 [1280/8000 (16%)]\tLoss: 29.580473\n",
      "Train Epoch: 511 [2560/8000 (32%)]\tLoss: 29.997286\n",
      "Train Epoch: 511 [3840/8000 (48%)]\tLoss: 29.955172\n",
      "Loss tensor(1925.3971, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 511 [5120/8000 (64%)]\tLoss: 30.277416\n",
      "Train Epoch: 511 [6400/8000 (80%)]\tLoss: 29.851099\n",
      "Loss tensor(1923.5620, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1921.1523, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 511 [7680/8000 (96%)]\tLoss: 30.053551\n",
      "====> Epoch: 511 Average loss: 29.8744\n",
      "Train Epoch: 512 [0/8000 (0%)]\tLoss: 29.818340\n",
      "Train Epoch: 512 [1280/8000 (16%)]\tLoss: 29.731619\n",
      "Train Epoch: 512 [2560/8000 (32%)]\tLoss: 29.944206\n",
      "Train Epoch: 512 [3840/8000 (48%)]\tLoss: 30.010513\n",
      "Train Epoch: 512 [5120/8000 (64%)]\tLoss: 29.843790\n",
      "Train Epoch: 512 [6400/8000 (80%)]\tLoss: 30.000383\n",
      "Train Epoch: 512 [7680/8000 (96%)]\tLoss: 29.817429\n",
      "====> Epoch: 512 Average loss: 29.8648\n",
      "Train Epoch: 513 [0/8000 (0%)]\tLoss: 29.830038\n",
      "Loss tensor(1913.8705, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 513 [1280/8000 (16%)]\tLoss: 29.752886\n",
      "Loss tensor(1901.2985, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 513 [2560/8000 (32%)]\tLoss: 29.678923\n",
      "Loss tensor(1905.6844, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 513 [3840/8000 (48%)]\tLoss: 29.719288\n",
      "Train Epoch: 513 [5120/8000 (64%)]\tLoss: 30.051245\n",
      "Train Epoch: 513 [6400/8000 (80%)]\tLoss: 29.775093\n",
      "Train Epoch: 513 [7680/8000 (96%)]\tLoss: 29.656168\n",
      "====> Epoch: 513 Average loss: 29.8786\n",
      "Train Epoch: 514 [0/8000 (0%)]\tLoss: 30.121696\n",
      "Loss tensor(1919.9080, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 514 [1280/8000 (16%)]\tLoss: 29.902918\n",
      "Train Epoch: 514 [2560/8000 (32%)]\tLoss: 29.786087\n",
      "Loss tensor(1913.4669, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 514 [3840/8000 (48%)]\tLoss: 29.816971\n",
      "Train Epoch: 514 [5120/8000 (64%)]\tLoss: 30.013901\n",
      "Train Epoch: 514 [6400/8000 (80%)]\tLoss: 29.797548\n",
      "Train Epoch: 514 [7680/8000 (96%)]\tLoss: 29.825859\n",
      "====> Epoch: 514 Average loss: 29.8721\n",
      "Train Epoch: 515 [0/8000 (0%)]\tLoss: 29.695179\n",
      "Train Epoch: 515 [1280/8000 (16%)]\tLoss: 29.933804\n",
      "Train Epoch: 515 [2560/8000 (32%)]\tLoss: 29.638609\n",
      "Loss tensor(1916.7360, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 515 [3840/8000 (48%)]\tLoss: 30.101910\n",
      "Train Epoch: 515 [5120/8000 (64%)]\tLoss: 30.103409\n",
      "Train Epoch: 515 [6400/8000 (80%)]\tLoss: 29.952538\n",
      "Train Epoch: 515 [7680/8000 (96%)]\tLoss: 29.566620\n",
      "Loss tensor(1902.5054, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "====> Epoch: 515 Average loss: 29.8735\n",
      "Train Epoch: 516 [0/8000 (0%)]\tLoss: 29.652534\n",
      "Loss tensor(1915.8899, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 516 [1280/8000 (16%)]\tLoss: 29.728251\n",
      "Loss tensor(1903.6021, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 516 [2560/8000 (32%)]\tLoss: 30.031858\n",
      "Train Epoch: 516 [3840/8000 (48%)]\tLoss: 29.938795\n",
      "Train Epoch: 516 [5120/8000 (64%)]\tLoss: 29.859549\n",
      "Train Epoch: 516 [6400/8000 (80%)]\tLoss: 29.950602\n",
      "Loss tensor(1908.3071, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 516 [7680/8000 (96%)]\tLoss: 29.676210\n",
      "====> Epoch: 516 Average loss: 29.8762\n",
      "Train Epoch: 517 [0/8000 (0%)]\tLoss: 29.854336\n",
      "Train Epoch: 517 [1280/8000 (16%)]\tLoss: 29.908897\n",
      "Loss tensor(1914.5490, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 517 [2560/8000 (32%)]\tLoss: 30.070154\n",
      "Train Epoch: 517 [3840/8000 (48%)]\tLoss: 29.875937\n",
      "Loss tensor(1917.2888, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 517 [5120/8000 (64%)]\tLoss: 29.850822\n",
      "Train Epoch: 517 [6400/8000 (80%)]\tLoss: 29.864380\n",
      "Train Epoch: 517 [7680/8000 (96%)]\tLoss: 30.063375\n",
      "====> Epoch: 517 Average loss: 29.8698\n",
      "Train Epoch: 518 [0/8000 (0%)]\tLoss: 29.995649\n",
      "Train Epoch: 518 [1280/8000 (16%)]\tLoss: 29.762001\n",
      "Train Epoch: 518 [2560/8000 (32%)]\tLoss: 29.995255\n",
      "Train Epoch: 518 [3840/8000 (48%)]\tLoss: 29.834837\n",
      "Train Epoch: 518 [5120/8000 (64%)]\tLoss: 30.099861\n",
      "Train Epoch: 518 [6400/8000 (80%)]\tLoss: 29.581701\n",
      "Train Epoch: 518 [7680/8000 (96%)]\tLoss: 29.967157\n",
      "====> Epoch: 518 Average loss: 29.8700\n",
      "Train Epoch: 519 [0/8000 (0%)]\tLoss: 29.795046\n",
      "Train Epoch: 519 [1280/8000 (16%)]\tLoss: 29.971836\n",
      "Train Epoch: 519 [2560/8000 (32%)]\tLoss: 29.866503\n",
      "Loss tensor(1919.1733, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 519 [3840/8000 (48%)]\tLoss: 30.079771\n",
      "Train Epoch: 519 [5120/8000 (64%)]\tLoss: 30.153912\n",
      "Train Epoch: 519 [6400/8000 (80%)]\tLoss: 30.017254\n",
      "Train Epoch: 519 [7680/8000 (96%)]\tLoss: 30.061380\n",
      "====> Epoch: 519 Average loss: 29.8783\n",
      "Train Epoch: 520 [0/8000 (0%)]\tLoss: 30.353235\n",
      "Loss tensor(1919.0197, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 520 [1280/8000 (16%)]\tLoss: 29.658783\n",
      "Train Epoch: 520 [2560/8000 (32%)]\tLoss: 29.840675\n",
      "Train Epoch: 520 [3840/8000 (48%)]\tLoss: 30.006565\n",
      "Train Epoch: 520 [5120/8000 (64%)]\tLoss: 29.771044\n",
      "Train Epoch: 520 [6400/8000 (80%)]\tLoss: 29.826830\n",
      "Train Epoch: 520 [7680/8000 (96%)]\tLoss: 29.797697\n",
      "Loss tensor(1909.3933, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "====> Epoch: 520 Average loss: 29.8644\n",
      "Train Epoch: 521 [0/8000 (0%)]\tLoss: 29.882256\n",
      "Train Epoch: 521 [1280/8000 (16%)]\tLoss: 29.581833\n",
      "Train Epoch: 521 [2560/8000 (32%)]\tLoss: 29.680561\n",
      "Train Epoch: 521 [3840/8000 (48%)]\tLoss: 29.786501\n",
      "Train Epoch: 521 [5120/8000 (64%)]\tLoss: 30.039259\n",
      "Train Epoch: 521 [6400/8000 (80%)]\tLoss: 29.847954\n",
      "Train Epoch: 521 [7680/8000 (96%)]\tLoss: 29.781031\n",
      "Loss tensor(1907.0841, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "====> Epoch: 521 Average loss: 29.8810\n",
      "Train Epoch: 522 [0/8000 (0%)]\tLoss: 29.969503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 522 [1280/8000 (16%)]\tLoss: 29.570887\n",
      "Train Epoch: 522 [2560/8000 (32%)]\tLoss: 29.877548\n",
      "Train Epoch: 522 [3840/8000 (48%)]\tLoss: 29.976431\n",
      "Train Epoch: 522 [5120/8000 (64%)]\tLoss: 29.848595\n",
      "Loss tensor(1895.9753, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 522 [6400/8000 (80%)]\tLoss: 29.875137\n",
      "Train Epoch: 522 [7680/8000 (96%)]\tLoss: 29.675041\n",
      "====> Epoch: 522 Average loss: 29.8527\n",
      "Train Epoch: 523 [0/8000 (0%)]\tLoss: 29.737967\n",
      "Train Epoch: 523 [1280/8000 (16%)]\tLoss: 29.865084\n",
      "Train Epoch: 523 [2560/8000 (32%)]\tLoss: 29.888908\n",
      "Train Epoch: 523 [3840/8000 (48%)]\tLoss: 29.658222\n",
      "Train Epoch: 523 [5120/8000 (64%)]\tLoss: 29.850752\n",
      "Loss tensor(1919.3112, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 523 [6400/8000 (80%)]\tLoss: 29.815456\n",
      "Loss tensor(1910.7158, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 523 [7680/8000 (96%)]\tLoss: 29.809933\n",
      "====> Epoch: 523 Average loss: 29.8618\n",
      "Train Epoch: 524 [0/8000 (0%)]\tLoss: 29.936182\n",
      "Train Epoch: 524 [1280/8000 (16%)]\tLoss: 29.931377\n",
      "Train Epoch: 524 [2560/8000 (32%)]\tLoss: 29.783293\n",
      "Train Epoch: 524 [3840/8000 (48%)]\tLoss: 29.752211\n",
      "Train Epoch: 524 [5120/8000 (64%)]\tLoss: 29.878998\n",
      "Train Epoch: 524 [6400/8000 (80%)]\tLoss: 29.856426\n",
      "Train Epoch: 524 [7680/8000 (96%)]\tLoss: 30.172300\n",
      "====> Epoch: 524 Average loss: 29.8700\n",
      "Train Epoch: 525 [0/8000 (0%)]\tLoss: 29.747812\n",
      "Train Epoch: 525 [1280/8000 (16%)]\tLoss: 29.625185\n",
      "Train Epoch: 525 [2560/8000 (32%)]\tLoss: 29.903402\n",
      "Train Epoch: 525 [3840/8000 (48%)]\tLoss: 29.869139\n",
      "Train Epoch: 525 [5120/8000 (64%)]\tLoss: 29.853374\n",
      "Train Epoch: 525 [6400/8000 (80%)]\tLoss: 29.751432\n",
      "Train Epoch: 525 [7680/8000 (96%)]\tLoss: 30.108219\n",
      "====> Epoch: 525 Average loss: 29.8457\n",
      "Train Epoch: 526 [0/8000 (0%)]\tLoss: 30.053022\n",
      "Loss tensor(1898.9579, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 526 [1280/8000 (16%)]\tLoss: 29.799812\n",
      "Train Epoch: 526 [2560/8000 (32%)]\tLoss: 29.800041\n",
      "Train Epoch: 526 [3840/8000 (48%)]\tLoss: 29.712612\n",
      "Loss tensor(1910.3318, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1914.2749, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 526 [5120/8000 (64%)]\tLoss: 29.910545\n",
      "Train Epoch: 526 [6400/8000 (80%)]\tLoss: 29.808733\n",
      "Train Epoch: 526 [7680/8000 (96%)]\tLoss: 29.830360\n",
      "====> Epoch: 526 Average loss: 29.8552\n",
      "Train Epoch: 527 [0/8000 (0%)]\tLoss: 29.789917\n",
      "Loss tensor(1912.6614, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 527 [1280/8000 (16%)]\tLoss: 29.623968\n",
      "Train Epoch: 527 [2560/8000 (32%)]\tLoss: 30.238018\n",
      "Train Epoch: 527 [3840/8000 (48%)]\tLoss: 29.807266\n",
      "Train Epoch: 527 [5120/8000 (64%)]\tLoss: 29.847691\n",
      "Train Epoch: 527 [6400/8000 (80%)]\tLoss: 29.777983\n",
      "Loss tensor(1929.7848, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 527 [7680/8000 (96%)]\tLoss: 29.803434\n",
      "====> Epoch: 527 Average loss: 29.8577\n",
      "Train Epoch: 528 [0/8000 (0%)]\tLoss: 29.785881\n",
      "Loss tensor(1913.8195, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 528 [1280/8000 (16%)]\tLoss: 29.569054\n",
      "Train Epoch: 528 [2560/8000 (32%)]\tLoss: 29.964237\n",
      "Loss tensor(1922.3245, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 528 [3840/8000 (48%)]\tLoss: 29.997490\n",
      "Train Epoch: 528 [5120/8000 (64%)]\tLoss: 29.875376\n",
      "Train Epoch: 528 [6400/8000 (80%)]\tLoss: 29.865286\n",
      "Train Epoch: 528 [7680/8000 (96%)]\tLoss: 29.924988\n",
      "====> Epoch: 528 Average loss: 29.8645\n",
      "Train Epoch: 529 [0/8000 (0%)]\tLoss: 29.991034\n",
      "Train Epoch: 529 [1280/8000 (16%)]\tLoss: 29.940868\n",
      "Loss tensor(1903.6101, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 529 [2560/8000 (32%)]\tLoss: 29.759449\n",
      "Train Epoch: 529 [3840/8000 (48%)]\tLoss: 29.763081\n",
      "Train Epoch: 529 [5120/8000 (64%)]\tLoss: 29.922440\n",
      "Loss tensor(1914.8876, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 529 [6400/8000 (80%)]\tLoss: 29.920881\n",
      "Train Epoch: 529 [7680/8000 (96%)]\tLoss: 29.941040\n",
      "====> Epoch: 529 Average loss: 29.8710\n",
      "Train Epoch: 530 [0/8000 (0%)]\tLoss: 30.022675\n",
      "Train Epoch: 530 [1280/8000 (16%)]\tLoss: 29.664946\n",
      "Train Epoch: 530 [2560/8000 (32%)]\tLoss: 30.022516\n",
      "Loss tensor(1902.8586, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 530 [3840/8000 (48%)]\tLoss: 29.789726\n",
      "Train Epoch: 530 [5120/8000 (64%)]\tLoss: 29.783710\n",
      "Train Epoch: 530 [6400/8000 (80%)]\tLoss: 29.772041\n",
      "Loss tensor(1917.2921, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 530 [7680/8000 (96%)]\tLoss: 29.988983\n",
      "====> Epoch: 530 Average loss: 29.8624\n",
      "Train Epoch: 531 [0/8000 (0%)]\tLoss: 29.847530\n",
      "Loss tensor(1928.2518, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 531 [1280/8000 (16%)]\tLoss: 29.877573\n",
      "Train Epoch: 531 [2560/8000 (32%)]\tLoss: 29.686060\n",
      "Train Epoch: 531 [3840/8000 (48%)]\tLoss: 30.156374\n",
      "Train Epoch: 531 [5120/8000 (64%)]\tLoss: 30.180960\n",
      "Train Epoch: 531 [6400/8000 (80%)]\tLoss: 29.742012\n",
      "Train Epoch: 531 [7680/8000 (96%)]\tLoss: 30.211138\n",
      "====> Epoch: 531 Average loss: 29.8640\n",
      "Train Epoch: 532 [0/8000 (0%)]\tLoss: 29.809307\n",
      "Train Epoch: 532 [1280/8000 (16%)]\tLoss: 29.821774\n",
      "Train Epoch: 532 [2560/8000 (32%)]\tLoss: 30.316103\n",
      "Loss tensor(1906.2723, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 532 [3840/8000 (48%)]\tLoss: 29.814426\n",
      "Loss tensor(1907.6587, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 532 [5120/8000 (64%)]\tLoss: 29.953915\n",
      "Train Epoch: 532 [6400/8000 (80%)]\tLoss: 29.766226\n",
      "Train Epoch: 532 [7680/8000 (96%)]\tLoss: 29.675535\n",
      "====> Epoch: 532 Average loss: 29.8702\n",
      "Train Epoch: 533 [0/8000 (0%)]\tLoss: 29.950003\n",
      "Loss tensor(1889.9391, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 533 [1280/8000 (16%)]\tLoss: 29.618458\n",
      "Loss tensor(1925.2634, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 533 [2560/8000 (32%)]\tLoss: 29.705490\n",
      "Loss tensor(1890.0481, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 533 [3840/8000 (48%)]\tLoss: 29.538851\n",
      "Train Epoch: 533 [5120/8000 (64%)]\tLoss: 29.947941\n",
      "Train Epoch: 533 [6400/8000 (80%)]\tLoss: 29.752176\n",
      "Train Epoch: 533 [7680/8000 (96%)]\tLoss: 29.954014\n",
      "====> Epoch: 533 Average loss: 29.8618\n",
      "Train Epoch: 534 [0/8000 (0%)]\tLoss: 29.896156\n",
      "Train Epoch: 534 [1280/8000 (16%)]\tLoss: 29.915937\n",
      "Train Epoch: 534 [2560/8000 (32%)]\tLoss: 29.963665\n",
      "Train Epoch: 534 [3840/8000 (48%)]\tLoss: 29.876339\n",
      "Train Epoch: 534 [5120/8000 (64%)]\tLoss: 29.882389\n",
      "Train Epoch: 534 [6400/8000 (80%)]\tLoss: 30.376585\n",
      "Train Epoch: 534 [7680/8000 (96%)]\tLoss: 29.800989\n",
      "====> Epoch: 534 Average loss: 29.8520\n",
      "Train Epoch: 535 [0/8000 (0%)]\tLoss: 29.954573\n",
      "Train Epoch: 535 [1280/8000 (16%)]\tLoss: 29.909864\n",
      "Train Epoch: 535 [2560/8000 (32%)]\tLoss: 29.478086\n",
      "Train Epoch: 535 [3840/8000 (48%)]\tLoss: 30.095858\n",
      "Train Epoch: 535 [5120/8000 (64%)]\tLoss: 30.026003\n",
      "Train Epoch: 535 [6400/8000 (80%)]\tLoss: 30.006960\n",
      "Train Epoch: 535 [7680/8000 (96%)]\tLoss: 30.403080\n",
      "====> Epoch: 535 Average loss: 29.8630\n",
      "Train Epoch: 536 [0/8000 (0%)]\tLoss: 29.907429\n",
      "Train Epoch: 536 [1280/8000 (16%)]\tLoss: 29.998983\n",
      "Train Epoch: 536 [2560/8000 (32%)]\tLoss: 29.839197\n",
      "Train Epoch: 536 [3840/8000 (48%)]\tLoss: 29.742628\n",
      "Train Epoch: 536 [5120/8000 (64%)]\tLoss: 29.844915\n",
      "Train Epoch: 536 [6400/8000 (80%)]\tLoss: 29.850597\n",
      "Train Epoch: 536 [7680/8000 (96%)]\tLoss: 30.226034\n",
      "====> Epoch: 536 Average loss: 29.8459\n",
      "Train Epoch: 537 [0/8000 (0%)]\tLoss: 30.015976\n",
      "Train Epoch: 537 [1280/8000 (16%)]\tLoss: 29.858582\n",
      "Train Epoch: 537 [2560/8000 (32%)]\tLoss: 29.644369\n",
      "Train Epoch: 537 [3840/8000 (48%)]\tLoss: 29.896374\n",
      "Train Epoch: 537 [5120/8000 (64%)]\tLoss: 29.795881\n",
      "Train Epoch: 537 [6400/8000 (80%)]\tLoss: 29.683722\n",
      "Train Epoch: 537 [7680/8000 (96%)]\tLoss: 29.752707\n",
      "====> Epoch: 537 Average loss: 29.8540\n",
      "Train Epoch: 538 [0/8000 (0%)]\tLoss: 29.951483\n",
      "Train Epoch: 538 [1280/8000 (16%)]\tLoss: 29.585190\n",
      "Loss tensor(1898.4814, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 538 [2560/8000 (32%)]\tLoss: 29.569857\n",
      "Train Epoch: 538 [3840/8000 (48%)]\tLoss: 29.644312\n",
      "Train Epoch: 538 [5120/8000 (64%)]\tLoss: 30.042589\n",
      "Train Epoch: 538 [6400/8000 (80%)]\tLoss: 29.729351\n",
      "Train Epoch: 538 [7680/8000 (96%)]\tLoss: 29.843285\n",
      "====> Epoch: 538 Average loss: 29.8576\n",
      "Train Epoch: 539 [0/8000 (0%)]\tLoss: 29.485983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 539 [1280/8000 (16%)]\tLoss: 29.683321\n",
      "Train Epoch: 539 [2560/8000 (32%)]\tLoss: 29.998705\n",
      "Loss tensor(1910.3300, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1917.7158, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 539 [3840/8000 (48%)]\tLoss: 29.964310\n",
      "Train Epoch: 539 [5120/8000 (64%)]\tLoss: 29.855955\n",
      "Train Epoch: 539 [6400/8000 (80%)]\tLoss: 29.903013\n",
      "Train Epoch: 539 [7680/8000 (96%)]\tLoss: 29.514484\n",
      "====> Epoch: 539 Average loss: 29.8518\n",
      "Train Epoch: 540 [0/8000 (0%)]\tLoss: 29.754705\n",
      "Loss tensor(1902.1372, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1927.7035, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 540 [1280/8000 (16%)]\tLoss: 29.915741\n",
      "Train Epoch: 540 [2560/8000 (32%)]\tLoss: 29.792091\n",
      "Train Epoch: 540 [3840/8000 (48%)]\tLoss: 29.829802\n",
      "Train Epoch: 540 [5120/8000 (64%)]\tLoss: 29.825783\n",
      "Train Epoch: 540 [6400/8000 (80%)]\tLoss: 29.756327\n",
      "Train Epoch: 540 [7680/8000 (96%)]\tLoss: 29.846775\n",
      "====> Epoch: 540 Average loss: 29.8539\n",
      "Train Epoch: 541 [0/8000 (0%)]\tLoss: 29.693741\n",
      "Train Epoch: 541 [1280/8000 (16%)]\tLoss: 29.876247\n",
      "Train Epoch: 541 [2560/8000 (32%)]\tLoss: 29.641102\n",
      "Train Epoch: 541 [3840/8000 (48%)]\tLoss: 29.576021\n",
      "Loss tensor(1903.1487, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1911.8296, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 541 [5120/8000 (64%)]\tLoss: 29.758162\n",
      "Loss tensor(1901.9688, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 541 [6400/8000 (80%)]\tLoss: 29.868914\n",
      "Loss tensor(1904.9890, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 541 [7680/8000 (96%)]\tLoss: 29.766781\n",
      "====> Epoch: 541 Average loss: 29.8519\n",
      "Train Epoch: 542 [0/8000 (0%)]\tLoss: 29.775265\n",
      "Train Epoch: 542 [1280/8000 (16%)]\tLoss: 29.999645\n",
      "Train Epoch: 542 [2560/8000 (32%)]\tLoss: 29.907473\n",
      "Loss tensor(1902.5919, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 542 [3840/8000 (48%)]\tLoss: 29.804169\n",
      "Train Epoch: 542 [5120/8000 (64%)]\tLoss: 30.088528\n",
      "Train Epoch: 542 [6400/8000 (80%)]\tLoss: 29.774551\n",
      "Train Epoch: 542 [7680/8000 (96%)]\tLoss: 29.543344\n",
      "====> Epoch: 542 Average loss: 29.8439\n",
      "Train Epoch: 543 [0/8000 (0%)]\tLoss: 29.853979\n",
      "Train Epoch: 543 [1280/8000 (16%)]\tLoss: 29.990078\n",
      "Train Epoch: 543 [2560/8000 (32%)]\tLoss: 29.800011\n",
      "Train Epoch: 543 [3840/8000 (48%)]\tLoss: 29.817030\n",
      "Train Epoch: 543 [5120/8000 (64%)]\tLoss: 30.193392\n",
      "Loss tensor(1892.5176, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 543 [6400/8000 (80%)]\tLoss: 29.877184\n",
      "Loss tensor(1914.9873, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 543 [7680/8000 (96%)]\tLoss: 29.958790\n",
      "====> Epoch: 543 Average loss: 29.8439\n",
      "Train Epoch: 544 [0/8000 (0%)]\tLoss: 29.698484\n",
      "Train Epoch: 544 [1280/8000 (16%)]\tLoss: 29.690605\n",
      "Train Epoch: 544 [2560/8000 (32%)]\tLoss: 30.112017\n",
      "Train Epoch: 544 [3840/8000 (48%)]\tLoss: 29.792416\n",
      "Train Epoch: 544 [5120/8000 (64%)]\tLoss: 29.642633\n",
      "Train Epoch: 544 [6400/8000 (80%)]\tLoss: 29.814810\n",
      "Train Epoch: 544 [7680/8000 (96%)]\tLoss: 29.930384\n",
      "====> Epoch: 544 Average loss: 29.8331\n",
      "Train Epoch: 545 [0/8000 (0%)]\tLoss: 29.959347\n",
      "Train Epoch: 545 [1280/8000 (16%)]\tLoss: 29.588268\n",
      "Train Epoch: 545 [2560/8000 (32%)]\tLoss: 30.177261\n",
      "Train Epoch: 545 [3840/8000 (48%)]\tLoss: 30.000919\n",
      "Train Epoch: 545 [5120/8000 (64%)]\tLoss: 29.765314\n",
      "Train Epoch: 545 [6400/8000 (80%)]\tLoss: 29.826536\n",
      "Train Epoch: 545 [7680/8000 (96%)]\tLoss: 29.755087\n",
      "====> Epoch: 545 Average loss: 29.8430\n",
      "Train Epoch: 546 [0/8000 (0%)]\tLoss: 29.862274\n",
      "Loss tensor(1910.6185, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 546 [1280/8000 (16%)]\tLoss: 29.915848\n",
      "Train Epoch: 546 [2560/8000 (32%)]\tLoss: 29.760489\n",
      "Train Epoch: 546 [3840/8000 (48%)]\tLoss: 30.047081\n",
      "Train Epoch: 546 [5120/8000 (64%)]\tLoss: 29.835854\n",
      "Train Epoch: 546 [6400/8000 (80%)]\tLoss: 29.512783\n",
      "Train Epoch: 546 [7680/8000 (96%)]\tLoss: 29.740311\n",
      "====> Epoch: 546 Average loss: 29.8455\n",
      "Train Epoch: 547 [0/8000 (0%)]\tLoss: 29.634192\n",
      "Train Epoch: 547 [1280/8000 (16%)]\tLoss: 29.971439\n",
      "Train Epoch: 547 [2560/8000 (32%)]\tLoss: 29.650238\n",
      "Loss tensor(1923.3977, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 547 [3840/8000 (48%)]\tLoss: 30.131044\n",
      "Train Epoch: 547 [5120/8000 (64%)]\tLoss: 29.695621\n",
      "Train Epoch: 547 [6400/8000 (80%)]\tLoss: 30.222828\n",
      "Train Epoch: 547 [7680/8000 (96%)]\tLoss: 29.905079\n",
      "====> Epoch: 547 Average loss: 29.8377\n",
      "Train Epoch: 548 [0/8000 (0%)]\tLoss: 29.770044\n",
      "Loss tensor(1908.7861, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1895.5254, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 548 [1280/8000 (16%)]\tLoss: 30.083090\n",
      "Train Epoch: 548 [2560/8000 (32%)]\tLoss: 30.177732\n",
      "Train Epoch: 548 [3840/8000 (48%)]\tLoss: 29.796808\n",
      "Train Epoch: 548 [5120/8000 (64%)]\tLoss: 29.636381\n",
      "Train Epoch: 548 [6400/8000 (80%)]\tLoss: 29.915651\n",
      "Train Epoch: 548 [7680/8000 (96%)]\tLoss: 29.950142\n",
      "====> Epoch: 548 Average loss: 29.8383\n",
      "Train Epoch: 549 [0/8000 (0%)]\tLoss: 30.002108\n",
      "Train Epoch: 549 [1280/8000 (16%)]\tLoss: 29.955521\n",
      "Train Epoch: 549 [2560/8000 (32%)]\tLoss: 29.871761\n",
      "Train Epoch: 549 [3840/8000 (48%)]\tLoss: 29.718237\n",
      "Train Epoch: 549 [5120/8000 (64%)]\tLoss: 29.982903\n",
      "Train Epoch: 549 [6400/8000 (80%)]\tLoss: 29.463121\n",
      "Train Epoch: 549 [7680/8000 (96%)]\tLoss: 29.795956\n",
      "====> Epoch: 549 Average loss: 29.8332\n",
      "Train Epoch: 550 [0/8000 (0%)]\tLoss: 29.966225\n",
      "Train Epoch: 550 [1280/8000 (16%)]\tLoss: 29.741798\n",
      "Train Epoch: 550 [2560/8000 (32%)]\tLoss: 29.687544\n",
      "Loss tensor(1918.4949, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 550 [3840/8000 (48%)]\tLoss: 29.712019\n",
      "Train Epoch: 550 [5120/8000 (64%)]\tLoss: 29.634583\n",
      "Train Epoch: 550 [6400/8000 (80%)]\tLoss: 29.853971\n",
      "Train Epoch: 550 [7680/8000 (96%)]\tLoss: 29.720369\n",
      "====> Epoch: 550 Average loss: 29.8446\n",
      "Train Epoch: 551 [0/8000 (0%)]\tLoss: 29.526419\n",
      "Train Epoch: 551 [1280/8000 (16%)]\tLoss: 29.673433\n",
      "Train Epoch: 551 [2560/8000 (32%)]\tLoss: 29.753227\n",
      "Train Epoch: 551 [3840/8000 (48%)]\tLoss: 29.765886\n",
      "Train Epoch: 551 [5120/8000 (64%)]\tLoss: 29.819847\n",
      "Train Epoch: 551 [6400/8000 (80%)]\tLoss: 29.714396\n",
      "Train Epoch: 551 [7680/8000 (96%)]\tLoss: 29.683371\n",
      "====> Epoch: 551 Average loss: 29.8329\n",
      "Train Epoch: 552 [0/8000 (0%)]\tLoss: 30.033781\n",
      "Train Epoch: 552 [1280/8000 (16%)]\tLoss: 29.735977\n",
      "Train Epoch: 552 [2560/8000 (32%)]\tLoss: 29.607517\n",
      "Loss tensor(1927.4402, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 552 [3840/8000 (48%)]\tLoss: 29.571701\n",
      "Train Epoch: 552 [5120/8000 (64%)]\tLoss: 29.812634\n",
      "Train Epoch: 552 [6400/8000 (80%)]\tLoss: 29.754421\n",
      "Train Epoch: 552 [7680/8000 (96%)]\tLoss: 29.715527\n",
      "====> Epoch: 552 Average loss: 29.8344\n",
      "Train Epoch: 553 [0/8000 (0%)]\tLoss: 29.806261\n",
      "Train Epoch: 553 [1280/8000 (16%)]\tLoss: 29.604670\n",
      "Train Epoch: 553 [2560/8000 (32%)]\tLoss: 29.812538\n",
      "Train Epoch: 553 [3840/8000 (48%)]\tLoss: 29.683502\n",
      "Train Epoch: 553 [5120/8000 (64%)]\tLoss: 29.948856\n",
      "Loss tensor(1912.8707, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 553 [6400/8000 (80%)]\tLoss: 29.854439\n",
      "Train Epoch: 553 [7680/8000 (96%)]\tLoss: 29.610992\n",
      "====> Epoch: 553 Average loss: 29.8425\n",
      "Train Epoch: 554 [0/8000 (0%)]\tLoss: 29.734974\n",
      "Train Epoch: 554 [1280/8000 (16%)]\tLoss: 29.822571\n",
      "Train Epoch: 554 [2560/8000 (32%)]\tLoss: 29.937468\n",
      "Train Epoch: 554 [3840/8000 (48%)]\tLoss: 29.682291\n",
      "Train Epoch: 554 [5120/8000 (64%)]\tLoss: 29.969398\n",
      "Loss tensor(1909.5355, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 554 [6400/8000 (80%)]\tLoss: 29.836363\n",
      "Train Epoch: 554 [7680/8000 (96%)]\tLoss: 29.971527\n",
      "====> Epoch: 554 Average loss: 29.8319\n",
      "Train Epoch: 555 [0/8000 (0%)]\tLoss: 29.894293\n",
      "Loss tensor(1886.2975, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 555 [1280/8000 (16%)]\tLoss: 30.090399\n",
      "Loss tensor(1901.3252, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 555 [2560/8000 (32%)]\tLoss: 29.810453\n",
      "Train Epoch: 555 [3840/8000 (48%)]\tLoss: 29.762383\n",
      "Loss tensor(1920.0449, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 555 [5120/8000 (64%)]\tLoss: 29.696386\n",
      "Train Epoch: 555 [6400/8000 (80%)]\tLoss: 29.571173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 555 [7680/8000 (96%)]\tLoss: 29.983679\n",
      "====> Epoch: 555 Average loss: 29.8292\n",
      "Train Epoch: 556 [0/8000 (0%)]\tLoss: 29.952673\n",
      "Train Epoch: 556 [1280/8000 (16%)]\tLoss: 29.513725\n",
      "Train Epoch: 556 [2560/8000 (32%)]\tLoss: 29.765987\n",
      "Train Epoch: 556 [3840/8000 (48%)]\tLoss: 29.949089\n",
      "Train Epoch: 556 [5120/8000 (64%)]\tLoss: 29.838608\n",
      "Train Epoch: 556 [6400/8000 (80%)]\tLoss: 29.979843\n",
      "Train Epoch: 556 [7680/8000 (96%)]\tLoss: 30.026787\n",
      "====> Epoch: 556 Average loss: 29.8286\n",
      "Train Epoch: 557 [0/8000 (0%)]\tLoss: 29.766121\n",
      "Train Epoch: 557 [1280/8000 (16%)]\tLoss: 29.840038\n",
      "Train Epoch: 557 [2560/8000 (32%)]\tLoss: 30.040569\n",
      "Train Epoch: 557 [3840/8000 (48%)]\tLoss: 29.744963\n",
      "Train Epoch: 557 [5120/8000 (64%)]\tLoss: 29.647934\n",
      "Train Epoch: 557 [6400/8000 (80%)]\tLoss: 29.807816\n",
      "Train Epoch: 557 [7680/8000 (96%)]\tLoss: 29.720058\n",
      "====> Epoch: 557 Average loss: 29.8185\n",
      "Train Epoch: 558 [0/8000 (0%)]\tLoss: 29.740334\n",
      "Loss tensor(1911.9073, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 558 [1280/8000 (16%)]\tLoss: 29.717274\n",
      "Train Epoch: 558 [2560/8000 (32%)]\tLoss: 29.703726\n",
      "Train Epoch: 558 [3840/8000 (48%)]\tLoss: 29.744558\n",
      "Train Epoch: 558 [5120/8000 (64%)]\tLoss: 29.775188\n",
      "Train Epoch: 558 [6400/8000 (80%)]\tLoss: 30.109531\n",
      "Train Epoch: 558 [7680/8000 (96%)]\tLoss: 29.414221\n",
      "====> Epoch: 558 Average loss: 29.8250\n",
      "Train Epoch: 559 [0/8000 (0%)]\tLoss: 29.740625\n",
      "Train Epoch: 559 [1280/8000 (16%)]\tLoss: 29.879215\n",
      "Train Epoch: 559 [2560/8000 (32%)]\tLoss: 30.088406\n",
      "Train Epoch: 559 [3840/8000 (48%)]\tLoss: 29.908329\n",
      "Loss tensor(1931.0708, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 559 [5120/8000 (64%)]\tLoss: 29.669821\n",
      "Train Epoch: 559 [6400/8000 (80%)]\tLoss: 29.797947\n",
      "Train Epoch: 559 [7680/8000 (96%)]\tLoss: 29.846376\n",
      "====> Epoch: 559 Average loss: 29.8268\n",
      "Train Epoch: 560 [0/8000 (0%)]\tLoss: 29.857227\n",
      "Train Epoch: 560 [1280/8000 (16%)]\tLoss: 29.781118\n",
      "Loss tensor(1901.6656, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 560 [2560/8000 (32%)]\tLoss: 29.804283\n",
      "Train Epoch: 560 [3840/8000 (48%)]\tLoss: 29.781055\n",
      "Train Epoch: 560 [5120/8000 (64%)]\tLoss: 30.140265\n",
      "Train Epoch: 560 [6400/8000 (80%)]\tLoss: 29.931803\n",
      "Train Epoch: 560 [7680/8000 (96%)]\tLoss: 29.872026\n",
      "====> Epoch: 560 Average loss: 29.8264\n",
      "Train Epoch: 561 [0/8000 (0%)]\tLoss: 29.718317\n",
      "Train Epoch: 561 [1280/8000 (16%)]\tLoss: 29.676760\n",
      "Train Epoch: 561 [2560/8000 (32%)]\tLoss: 29.702446\n",
      "Train Epoch: 561 [3840/8000 (48%)]\tLoss: 29.652294\n",
      "Train Epoch: 561 [5120/8000 (64%)]\tLoss: 29.706003\n",
      "Train Epoch: 561 [6400/8000 (80%)]\tLoss: 29.660522\n",
      "Train Epoch: 561 [7680/8000 (96%)]\tLoss: 29.699631\n",
      "====> Epoch: 561 Average loss: 29.8254\n",
      "Train Epoch: 562 [0/8000 (0%)]\tLoss: 29.911135\n",
      "Train Epoch: 562 [1280/8000 (16%)]\tLoss: 29.759436\n",
      "Train Epoch: 562 [2560/8000 (32%)]\tLoss: 29.621284\n",
      "Train Epoch: 562 [3840/8000 (48%)]\tLoss: 29.852219\n",
      "Train Epoch: 562 [5120/8000 (64%)]\tLoss: 29.929203\n",
      "Train Epoch: 562 [6400/8000 (80%)]\tLoss: 29.759331\n",
      "Train Epoch: 562 [7680/8000 (96%)]\tLoss: 29.624203\n",
      "====> Epoch: 562 Average loss: 29.8307\n",
      "Train Epoch: 563 [0/8000 (0%)]\tLoss: 29.896641\n",
      "Train Epoch: 563 [1280/8000 (16%)]\tLoss: 29.681595\n",
      "Train Epoch: 563 [2560/8000 (32%)]\tLoss: 30.231060\n",
      "Train Epoch: 563 [3840/8000 (48%)]\tLoss: 29.776722\n",
      "Train Epoch: 563 [5120/8000 (64%)]\tLoss: 29.727116\n",
      "Train Epoch: 563 [6400/8000 (80%)]\tLoss: 29.366182\n",
      "Loss tensor(1914.9288, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 563 [7680/8000 (96%)]\tLoss: 29.760729\n",
      "====> Epoch: 563 Average loss: 29.8330\n",
      "Train Epoch: 564 [0/8000 (0%)]\tLoss: 30.170029\n",
      "Train Epoch: 564 [1280/8000 (16%)]\tLoss: 29.566093\n",
      "Train Epoch: 564 [2560/8000 (32%)]\tLoss: 29.898472\n",
      "Train Epoch: 564 [3840/8000 (48%)]\tLoss: 29.996395\n",
      "Train Epoch: 564 [5120/8000 (64%)]\tLoss: 29.815481\n",
      "Train Epoch: 564 [6400/8000 (80%)]\tLoss: 29.843742\n",
      "Train Epoch: 564 [7680/8000 (96%)]\tLoss: 29.909292\n",
      "====> Epoch: 564 Average loss: 29.8329\n",
      "Train Epoch: 565 [0/8000 (0%)]\tLoss: 29.827850\n",
      "Loss tensor(1905.7081, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Loss tensor(1916.2942, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 565 [1280/8000 (16%)]\tLoss: 29.839863\n",
      "Train Epoch: 565 [2560/8000 (32%)]\tLoss: 29.378887\n",
      "Train Epoch: 565 [3840/8000 (48%)]\tLoss: 29.839020\n",
      "Loss tensor(1935.4031, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 565 [5120/8000 (64%)]\tLoss: 29.743334\n",
      "Train Epoch: 565 [6400/8000 (80%)]\tLoss: 29.892912\n",
      "Train Epoch: 565 [7680/8000 (96%)]\tLoss: 29.762066\n",
      "====> Epoch: 565 Average loss: 29.8260\n",
      "Train Epoch: 566 [0/8000 (0%)]\tLoss: 29.878614\n",
      "Train Epoch: 566 [1280/8000 (16%)]\tLoss: 29.972422\n",
      "Train Epoch: 566 [2560/8000 (32%)]\tLoss: 29.496382\n",
      "Train Epoch: 566 [3840/8000 (48%)]\tLoss: 29.745396\n",
      "Train Epoch: 566 [5120/8000 (64%)]\tLoss: 29.708841\n",
      "Train Epoch: 566 [6400/8000 (80%)]\tLoss: 29.843792\n",
      "Train Epoch: 566 [7680/8000 (96%)]\tLoss: 29.673229\n",
      "====> Epoch: 566 Average loss: 29.8407\n",
      "Train Epoch: 567 [0/8000 (0%)]\tLoss: 29.953264\n",
      "Train Epoch: 567 [1280/8000 (16%)]\tLoss: 29.673859\n",
      "Train Epoch: 567 [2560/8000 (32%)]\tLoss: 29.784929\n",
      "Train Epoch: 567 [3840/8000 (48%)]\tLoss: 29.743399\n",
      "Train Epoch: 567 [5120/8000 (64%)]\tLoss: 29.710680\n",
      "Train Epoch: 567 [6400/8000 (80%)]\tLoss: 29.782539\n",
      "Train Epoch: 567 [7680/8000 (96%)]\tLoss: 29.624084\n",
      "====> Epoch: 567 Average loss: 29.8302\n",
      "Train Epoch: 568 [0/8000 (0%)]\tLoss: 29.637815\n",
      "Train Epoch: 568 [1280/8000 (16%)]\tLoss: 29.931025\n",
      "Train Epoch: 568 [2560/8000 (32%)]\tLoss: 29.957563\n",
      "Train Epoch: 568 [3840/8000 (48%)]\tLoss: 29.684875\n",
      "Train Epoch: 568 [5120/8000 (64%)]\tLoss: 29.846600\n",
      "Train Epoch: 568 [6400/8000 (80%)]\tLoss: 29.824654\n",
      "Train Epoch: 568 [7680/8000 (96%)]\tLoss: 29.634211\n",
      "====> Epoch: 568 Average loss: 29.8261\n",
      "Train Epoch: 569 [0/8000 (0%)]\tLoss: 29.861965\n",
      "Train Epoch: 569 [1280/8000 (16%)]\tLoss: 29.576103\n",
      "Train Epoch: 569 [2560/8000 (32%)]\tLoss: 29.719385\n",
      "Loss tensor(1936.3644, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 569 [3840/8000 (48%)]\tLoss: 29.546080\n",
      "Train Epoch: 569 [5120/8000 (64%)]\tLoss: 29.998735\n",
      "Loss tensor(1930.0677, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 569 [6400/8000 (80%)]\tLoss: 29.997028\n",
      "Train Epoch: 569 [7680/8000 (96%)]\tLoss: 30.011955\n",
      "====> Epoch: 569 Average loss: 29.8313\n",
      "Train Epoch: 570 [0/8000 (0%)]\tLoss: 29.771238\n",
      "Train Epoch: 570 [1280/8000 (16%)]\tLoss: 29.917429\n",
      "Train Epoch: 570 [2560/8000 (32%)]\tLoss: 29.832867\n",
      "Train Epoch: 570 [3840/8000 (48%)]\tLoss: 29.881941\n",
      "Loss tensor(1902.4238, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 570 [5120/8000 (64%)]\tLoss: 29.952461\n",
      "Train Epoch: 570 [6400/8000 (80%)]\tLoss: 30.122766\n",
      "Train Epoch: 570 [7680/8000 (96%)]\tLoss: 30.066147\n",
      "====> Epoch: 570 Average loss: 29.8295\n",
      "Train Epoch: 571 [0/8000 (0%)]\tLoss: 29.661968\n",
      "Loss tensor(1904.5668, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 571 [1280/8000 (16%)]\tLoss: 29.782930\n",
      "Loss tensor(1896.8810, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 571 [2560/8000 (32%)]\tLoss: 29.737974\n",
      "Train Epoch: 571 [3840/8000 (48%)]\tLoss: 29.830730\n",
      "Train Epoch: 571 [5120/8000 (64%)]\tLoss: 29.909695\n",
      "Train Epoch: 571 [6400/8000 (80%)]\tLoss: 29.860546\n",
      "Train Epoch: 571 [7680/8000 (96%)]\tLoss: 29.788639\n",
      "====> Epoch: 571 Average loss: 29.8252\n",
      "Train Epoch: 572 [0/8000 (0%)]\tLoss: 30.004566\n",
      "Train Epoch: 572 [1280/8000 (16%)]\tLoss: 29.709856\n",
      "Train Epoch: 572 [2560/8000 (32%)]\tLoss: 29.858944\n",
      "Train Epoch: 572 [3840/8000 (48%)]\tLoss: 29.874483\n",
      "Train Epoch: 572 [5120/8000 (64%)]\tLoss: 29.837440\n",
      "Loss tensor(1905.5717, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 572 [6400/8000 (80%)]\tLoss: 29.818539\n",
      "Train Epoch: 572 [7680/8000 (96%)]\tLoss: 29.860538\n",
      "====> Epoch: 572 Average loss: 29.8314\n",
      "Train Epoch: 573 [0/8000 (0%)]\tLoss: 29.940176\n",
      "Loss tensor(1900.0612, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 573 [1280/8000 (16%)]\tLoss: 29.778503\n",
      "Train Epoch: 573 [2560/8000 (32%)]\tLoss: 30.027054\n",
      "Train Epoch: 573 [3840/8000 (48%)]\tLoss: 29.869987\n",
      "Train Epoch: 573 [5120/8000 (64%)]\tLoss: 29.682207\n",
      "Train Epoch: 573 [6400/8000 (80%)]\tLoss: 29.815552\n",
      "Train Epoch: 573 [7680/8000 (96%)]\tLoss: 29.920177\n",
      "Loss tensor(1919.5127, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "====> Epoch: 573 Average loss: 29.8067\n",
      "Train Epoch: 574 [0/8000 (0%)]\tLoss: 29.947737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 574 [1280/8000 (16%)]\tLoss: 30.156626\n",
      "Train Epoch: 574 [2560/8000 (32%)]\tLoss: 29.679291\n",
      "Train Epoch: 574 [3840/8000 (48%)]\tLoss: 29.798655\n",
      "Loss tensor(1917.4095, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 574 [5120/8000 (64%)]\tLoss: 30.122656\n",
      "Train Epoch: 574 [6400/8000 (80%)]\tLoss: 29.895395\n",
      "Train Epoch: 574 [7680/8000 (96%)]\tLoss: 29.584961\n",
      "====> Epoch: 574 Average loss: 29.8154\n",
      "Train Epoch: 575 [0/8000 (0%)]\tLoss: 29.727942\n",
      "Train Epoch: 575 [1280/8000 (16%)]\tLoss: 29.786169\n",
      "Train Epoch: 575 [2560/8000 (32%)]\tLoss: 29.608397\n",
      "Train Epoch: 575 [3840/8000 (48%)]\tLoss: 29.522528\n",
      "Train Epoch: 575 [5120/8000 (64%)]\tLoss: 29.902514\n",
      "Train Epoch: 575 [6400/8000 (80%)]\tLoss: 29.959368\n",
      "Train Epoch: 575 [7680/8000 (96%)]\tLoss: 29.809395\n",
      "====> Epoch: 575 Average loss: 29.8114\n",
      "Train Epoch: 576 [0/8000 (0%)]\tLoss: 29.440115\n",
      "Train Epoch: 576 [1280/8000 (16%)]\tLoss: 29.951160\n",
      "Train Epoch: 576 [2560/8000 (32%)]\tLoss: 30.007204\n",
      "Train Epoch: 576 [3840/8000 (48%)]\tLoss: 30.070606\n",
      "Train Epoch: 576 [5120/8000 (64%)]\tLoss: 29.648624\n",
      "Train Epoch: 576 [6400/8000 (80%)]\tLoss: 29.805033\n",
      "Train Epoch: 576 [7680/8000 (96%)]\tLoss: 30.027245\n",
      "====> Epoch: 576 Average loss: 29.8189\n",
      "Train Epoch: 577 [0/8000 (0%)]\tLoss: 29.708508\n",
      "Train Epoch: 577 [1280/8000 (16%)]\tLoss: 29.663050\n",
      "Loss tensor(1902.6715, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 577 [2560/8000 (32%)]\tLoss: 30.207790\n",
      "Train Epoch: 577 [3840/8000 (48%)]\tLoss: 29.951279\n",
      "Loss tensor(1936.1608, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 577 [5120/8000 (64%)]\tLoss: 30.172060\n",
      "Loss tensor(1911.1588, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 577 [6400/8000 (80%)]\tLoss: 29.843414\n",
      "Train Epoch: 577 [7680/8000 (96%)]\tLoss: 30.095556\n",
      "====> Epoch: 577 Average loss: 29.8251\n",
      "Train Epoch: 578 [0/8000 (0%)]\tLoss: 29.845818\n",
      "Train Epoch: 578 [1280/8000 (16%)]\tLoss: 29.674862\n",
      "Train Epoch: 578 [2560/8000 (32%)]\tLoss: 29.870766\n",
      "Train Epoch: 578 [3840/8000 (48%)]\tLoss: 29.848600\n",
      "Train Epoch: 578 [5120/8000 (64%)]\tLoss: 29.930225\n",
      "Loss tensor(1919.9580, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 578 [6400/8000 (80%)]\tLoss: 29.820873\n",
      "Train Epoch: 578 [7680/8000 (96%)]\tLoss: 29.832304\n",
      "====> Epoch: 578 Average loss: 29.8202\n",
      "Train Epoch: 579 [0/8000 (0%)]\tLoss: 30.189054\n",
      "Train Epoch: 579 [1280/8000 (16%)]\tLoss: 29.730434\n",
      "Train Epoch: 579 [2560/8000 (32%)]\tLoss: 30.014753\n",
      "Train Epoch: 579 [3840/8000 (48%)]\tLoss: 30.230709\n",
      "Train Epoch: 579 [5120/8000 (64%)]\tLoss: 29.730621\n",
      "Train Epoch: 579 [6400/8000 (80%)]\tLoss: 29.692579\n",
      "Train Epoch: 579 [7680/8000 (96%)]\tLoss: 29.793880\n",
      "====> Epoch: 579 Average loss: 29.8144\n",
      "Train Epoch: 580 [0/8000 (0%)]\tLoss: 29.999554\n",
      "Train Epoch: 580 [1280/8000 (16%)]\tLoss: 29.789776\n",
      "Loss tensor(1906.9276, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 580 [2560/8000 (32%)]\tLoss: 30.272247\n",
      "Train Epoch: 580 [3840/8000 (48%)]\tLoss: 30.053612\n",
      "Train Epoch: 580 [5120/8000 (64%)]\tLoss: 29.973719\n",
      "Train Epoch: 580 [6400/8000 (80%)]\tLoss: 29.857590\n",
      "Train Epoch: 580 [7680/8000 (96%)]\tLoss: 30.027487\n",
      "====> Epoch: 580 Average loss: 29.8103\n",
      "Train Epoch: 581 [0/8000 (0%)]\tLoss: 29.680647\n",
      "Train Epoch: 581 [1280/8000 (16%)]\tLoss: 29.785084\n",
      "Train Epoch: 581 [2560/8000 (32%)]\tLoss: 29.598646\n",
      "Train Epoch: 581 [3840/8000 (48%)]\tLoss: 29.782637\n",
      "Train Epoch: 581 [5120/8000 (64%)]\tLoss: 30.242535\n",
      "Train Epoch: 581 [6400/8000 (80%)]\tLoss: 29.801743\n",
      "Train Epoch: 581 [7680/8000 (96%)]\tLoss: 29.494642\n",
      "====> Epoch: 581 Average loss: 29.8181\n",
      "Train Epoch: 582 [0/8000 (0%)]\tLoss: 29.856852\n",
      "Train Epoch: 582 [1280/8000 (16%)]\tLoss: 29.765699\n",
      "Train Epoch: 582 [2560/8000 (32%)]\tLoss: 29.774115\n",
      "Train Epoch: 582 [3840/8000 (48%)]\tLoss: 30.029507\n",
      "Train Epoch: 582 [5120/8000 (64%)]\tLoss: 29.906921\n",
      "Train Epoch: 582 [6400/8000 (80%)]\tLoss: 29.911345\n",
      "Train Epoch: 582 [7680/8000 (96%)]\tLoss: 29.695265\n",
      "====> Epoch: 582 Average loss: 29.8173\n",
      "Train Epoch: 583 [0/8000 (0%)]\tLoss: 30.112427\n",
      "Train Epoch: 583 [1280/8000 (16%)]\tLoss: 29.846058\n",
      "Train Epoch: 583 [2560/8000 (32%)]\tLoss: 29.949345\n",
      "Train Epoch: 583 [3840/8000 (48%)]\tLoss: 29.803942\n",
      "Train Epoch: 583 [5120/8000 (64%)]\tLoss: 30.014084\n",
      "Train Epoch: 583 [6400/8000 (80%)]\tLoss: 29.763533\n",
      "Loss tensor(1906.0618, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 583 [7680/8000 (96%)]\tLoss: 29.730772\n",
      "====> Epoch: 583 Average loss: 29.8115\n",
      "Train Epoch: 584 [0/8000 (0%)]\tLoss: 29.924248\n",
      "Train Epoch: 584 [1280/8000 (16%)]\tLoss: 29.563507\n",
      "Train Epoch: 584 [2560/8000 (32%)]\tLoss: 29.528019\n",
      "Train Epoch: 584 [3840/8000 (48%)]\tLoss: 29.583063\n",
      "Train Epoch: 584 [5120/8000 (64%)]\tLoss: 29.732651\n",
      "Train Epoch: 584 [6400/8000 (80%)]\tLoss: 29.473526\n",
      "Loss tensor(1910.2897, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 584 [7680/8000 (96%)]\tLoss: 29.632547\n",
      "====> Epoch: 584 Average loss: 29.8117\n",
      "Train Epoch: 585 [0/8000 (0%)]\tLoss: 29.828369\n",
      "Train Epoch: 585 [1280/8000 (16%)]\tLoss: 29.794632\n",
      "Train Epoch: 585 [2560/8000 (32%)]\tLoss: 29.642704\n",
      "Train Epoch: 585 [3840/8000 (48%)]\tLoss: 29.762756\n",
      "Train Epoch: 585 [5120/8000 (64%)]\tLoss: 29.996750\n",
      "Train Epoch: 585 [6400/8000 (80%)]\tLoss: 30.152691\n",
      "Loss tensor(1912.8583, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 585 [7680/8000 (96%)]\tLoss: 29.812077\n",
      "====> Epoch: 585 Average loss: 29.8103\n",
      "Train Epoch: 586 [0/8000 (0%)]\tLoss: 29.833912\n",
      "Train Epoch: 586 [1280/8000 (16%)]\tLoss: 29.781240\n",
      "Train Epoch: 586 [2560/8000 (32%)]\tLoss: 29.997057\n",
      "Loss tensor(1912.5306, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 586 [3840/8000 (48%)]\tLoss: 29.827562\n",
      "Train Epoch: 586 [5120/8000 (64%)]\tLoss: 29.840963\n",
      "Loss tensor(1897.0535, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 586 [6400/8000 (80%)]\tLoss: 29.970377\n",
      "Train Epoch: 586 [7680/8000 (96%)]\tLoss: 29.945139\n",
      "====> Epoch: 586 Average loss: 29.8145\n",
      "Train Epoch: 587 [0/8000 (0%)]\tLoss: 29.799768\n",
      "Loss tensor(1911.1321, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 587 [1280/8000 (16%)]\tLoss: 30.009329\n",
      "Train Epoch: 587 [2560/8000 (32%)]\tLoss: 29.919813\n",
      "Train Epoch: 587 [3840/8000 (48%)]\tLoss: 29.785946\n",
      "Train Epoch: 587 [5120/8000 (64%)]\tLoss: 29.568802\n",
      "Train Epoch: 587 [6400/8000 (80%)]\tLoss: 29.739603\n",
      "Loss tensor(1917.3794, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 587 [7680/8000 (96%)]\tLoss: 30.014196\n",
      "====> Epoch: 587 Average loss: 29.8094\n",
      "Train Epoch: 588 [0/8000 (0%)]\tLoss: 30.007891\n",
      "Train Epoch: 588 [1280/8000 (16%)]\tLoss: 30.097878\n",
      "Train Epoch: 588 [2560/8000 (32%)]\tLoss: 29.960167\n",
      "Loss tensor(1909.5554, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 588 [3840/8000 (48%)]\tLoss: 29.892862\n",
      "Train Epoch: 588 [5120/8000 (64%)]\tLoss: 29.946625\n",
      "Train Epoch: 588 [6400/8000 (80%)]\tLoss: 29.909191\n",
      "Train Epoch: 588 [7680/8000 (96%)]\tLoss: 30.139641\n",
      "====> Epoch: 588 Average loss: 29.8094\n",
      "Train Epoch: 589 [0/8000 (0%)]\tLoss: 29.512102\n",
      "Train Epoch: 589 [1280/8000 (16%)]\tLoss: 30.044254\n",
      "Train Epoch: 589 [2560/8000 (32%)]\tLoss: 29.721766\n",
      "Train Epoch: 589 [3840/8000 (48%)]\tLoss: 29.843567\n",
      "Train Epoch: 589 [5120/8000 (64%)]\tLoss: 30.081093\n",
      "Train Epoch: 589 [6400/8000 (80%)]\tLoss: 29.697788\n",
      "Train Epoch: 589 [7680/8000 (96%)]\tLoss: 29.684935\n",
      "====> Epoch: 589 Average loss: 29.8149\n",
      "Train Epoch: 590 [0/8000 (0%)]\tLoss: 29.950623\n",
      "Train Epoch: 590 [1280/8000 (16%)]\tLoss: 29.899870\n",
      "Train Epoch: 590 [2560/8000 (32%)]\tLoss: 29.738720\n",
      "Train Epoch: 590 [3840/8000 (48%)]\tLoss: 29.895693\n",
      "Train Epoch: 590 [5120/8000 (64%)]\tLoss: 29.479952\n",
      "Train Epoch: 590 [6400/8000 (80%)]\tLoss: 29.768187\n",
      "Train Epoch: 590 [7680/8000 (96%)]\tLoss: 30.006403\n",
      "====> Epoch: 590 Average loss: 29.8059\n",
      "Train Epoch: 591 [0/8000 (0%)]\tLoss: 29.889114\n",
      "Train Epoch: 591 [1280/8000 (16%)]\tLoss: 30.007996\n",
      "Train Epoch: 591 [2560/8000 (32%)]\tLoss: 29.805153\n",
      "Train Epoch: 591 [3840/8000 (48%)]\tLoss: 29.655487\n",
      "Train Epoch: 591 [5120/8000 (64%)]\tLoss: 29.537769\n",
      "Train Epoch: 591 [6400/8000 (80%)]\tLoss: 29.974794\n",
      "Train Epoch: 591 [7680/8000 (96%)]\tLoss: 29.616291\n",
      "====> Epoch: 591 Average loss: 29.8020\n",
      "Train Epoch: 592 [0/8000 (0%)]\tLoss: 29.834578\n",
      "Train Epoch: 592 [1280/8000 (16%)]\tLoss: 29.896372\n",
      "Train Epoch: 592 [2560/8000 (32%)]\tLoss: 29.630611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1891.7598, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 592 [3840/8000 (48%)]\tLoss: 29.502441\n",
      "Train Epoch: 592 [5120/8000 (64%)]\tLoss: 29.692448\n",
      "Train Epoch: 592 [6400/8000 (80%)]\tLoss: 29.270248\n",
      "Loss tensor(1903.7798, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 592 [7680/8000 (96%)]\tLoss: 29.723076\n",
      "====> Epoch: 592 Average loss: 29.8142\n",
      "Train Epoch: 593 [0/8000 (0%)]\tLoss: 29.781559\n",
      "Train Epoch: 593 [1280/8000 (16%)]\tLoss: 29.756092\n",
      "Train Epoch: 593 [2560/8000 (32%)]\tLoss: 29.882971\n",
      "Train Epoch: 593 [3840/8000 (48%)]\tLoss: 29.759600\n",
      "Train Epoch: 593 [5120/8000 (64%)]\tLoss: 29.943674\n",
      "Train Epoch: 593 [6400/8000 (80%)]\tLoss: 29.531805\n",
      "Loss tensor(1901.2854, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 593 [7680/8000 (96%)]\tLoss: 29.848345\n",
      "====> Epoch: 593 Average loss: 29.8024\n",
      "Train Epoch: 594 [0/8000 (0%)]\tLoss: 30.199381\n",
      "Train Epoch: 594 [1280/8000 (16%)]\tLoss: 30.018654\n",
      "Train Epoch: 594 [2560/8000 (32%)]\tLoss: 29.571701\n",
      "Train Epoch: 594 [3840/8000 (48%)]\tLoss: 30.096840\n",
      "Train Epoch: 594 [5120/8000 (64%)]\tLoss: 29.820375\n",
      "Train Epoch: 594 [6400/8000 (80%)]\tLoss: 29.657682\n",
      "Train Epoch: 594 [7680/8000 (96%)]\tLoss: 29.958332\n",
      "====> Epoch: 594 Average loss: 29.8053\n",
      "Train Epoch: 595 [0/8000 (0%)]\tLoss: 29.738087\n",
      "Train Epoch: 595 [1280/8000 (16%)]\tLoss: 29.851667\n",
      "Loss tensor(1916.1421, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 595 [2560/8000 (32%)]\tLoss: 29.742954\n",
      "Loss tensor(1913.6688, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 595 [3840/8000 (48%)]\tLoss: 29.601728\n",
      "Train Epoch: 595 [5120/8000 (64%)]\tLoss: 29.767345\n",
      "Train Epoch: 595 [6400/8000 (80%)]\tLoss: 29.783457\n",
      "Train Epoch: 595 [7680/8000 (96%)]\tLoss: 29.699589\n",
      "====> Epoch: 595 Average loss: 29.7997\n",
      "Train Epoch: 596 [0/8000 (0%)]\tLoss: 29.685965\n",
      "Train Epoch: 596 [1280/8000 (16%)]\tLoss: 29.928320\n",
      "Train Epoch: 596 [2560/8000 (32%)]\tLoss: 29.905645\n",
      "Train Epoch: 596 [3840/8000 (48%)]\tLoss: 30.037155\n",
      "Train Epoch: 596 [5120/8000 (64%)]\tLoss: 29.797459\n",
      "Loss tensor(1889.7207, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 596 [6400/8000 (80%)]\tLoss: 29.958004\n",
      "Loss tensor(1913.2987, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 596 [7680/8000 (96%)]\tLoss: 29.678585\n",
      "====> Epoch: 596 Average loss: 29.7983\n",
      "Train Epoch: 597 [0/8000 (0%)]\tLoss: 29.777990\n",
      "Train Epoch: 597 [1280/8000 (16%)]\tLoss: 29.631645\n",
      "Train Epoch: 597 [2560/8000 (32%)]\tLoss: 29.760942\n",
      "Train Epoch: 597 [3840/8000 (48%)]\tLoss: 29.621815\n",
      "Train Epoch: 597 [5120/8000 (64%)]\tLoss: 30.006254\n",
      "Train Epoch: 597 [6400/8000 (80%)]\tLoss: 29.554270\n",
      "Train Epoch: 597 [7680/8000 (96%)]\tLoss: 30.070013\n",
      "====> Epoch: 597 Average loss: 29.8007\n",
      "Train Epoch: 598 [0/8000 (0%)]\tLoss: 29.724564\n",
      "Train Epoch: 598 [1280/8000 (16%)]\tLoss: 30.002548\n",
      "Train Epoch: 598 [2560/8000 (32%)]\tLoss: 29.677599\n",
      "Train Epoch: 598 [3840/8000 (48%)]\tLoss: 29.771292\n",
      "Train Epoch: 598 [5120/8000 (64%)]\tLoss: 30.033724\n",
      "Train Epoch: 598 [6400/8000 (80%)]\tLoss: 29.974073\n",
      "Train Epoch: 598 [7680/8000 (96%)]\tLoss: 29.788160\n",
      "====> Epoch: 598 Average loss: 29.8130\n",
      "Train Epoch: 599 [0/8000 (0%)]\tLoss: 29.913383\n",
      "Train Epoch: 599 [1280/8000 (16%)]\tLoss: 29.949884\n",
      "Train Epoch: 599 [2560/8000 (32%)]\tLoss: 29.753410\n",
      "Train Epoch: 599 [3840/8000 (48%)]\tLoss: 29.868568\n",
      "Train Epoch: 599 [5120/8000 (64%)]\tLoss: 29.935820\n",
      "Train Epoch: 599 [6400/8000 (80%)]\tLoss: 29.690359\n",
      "Train Epoch: 599 [7680/8000 (96%)]\tLoss: 29.692110\n",
      "====> Epoch: 599 Average loss: 29.8011\n",
      "Train Epoch: 600 [0/8000 (0%)]\tLoss: 29.786177\n",
      "Train Epoch: 600 [1280/8000 (16%)]\tLoss: 29.832680\n",
      "Loss tensor(1915.1942, device='cuda:0', grad_fn=<AddBackward0>)Logits Loss 0\n",
      "Train Epoch: 600 [2560/8000 (32%)]\tLoss: 30.034056\n",
      "Train Epoch: 600 [3840/8000 (48%)]\tLoss: 29.636787\n",
      "Train Epoch: 600 [5120/8000 (64%)]\tLoss: 29.848080\n",
      "Train Epoch: 600 [6400/8000 (80%)]\tLoss: 29.634579\n",
      "Train Epoch: 600 [7680/8000 (96%)]\tLoss: 29.870283\n",
      "====> Epoch: 600 Average loss: 29.7943\n"
     ]
    }
   ],
   "source": [
    "gradients_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "gradient_post_burn_in = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_post_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients_gumbel_state(train_data, vae_gumbel_truncated, \n",
    "                                                      vae_gumbel_trunc_optimizer, \n",
    "                                                      epoch, \n",
    "                                                      batch_size, \n",
    "                                                      logits_changed_loss_lambda = logits_changed_loss_lambda,\n",
    "                                                      Dim = 2*D, \n",
    "                                                      DEBUG = True)\n",
    "    \n",
    "    vae_gumbel_truncated.t = max(0.001, vae_gumbel_truncated.t * 0.99)\n",
    "    if epoch <=(n_epochs//5*4):\n",
    "        gradients_before_burnin += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_before_burnin += sample_subset(vae_gumbel_truncated.logit_enc, \n",
    "                                                          vae_gumbel_truncated.k, \n",
    "                                                          vae_gumbel_truncated.t).view(-1)\n",
    "    if epoch == (n_epochs//5*4):\n",
    "        print(\"BURN IN DEBUG\")\n",
    "        vae_gumbel_truncated.set_burned_in()\n",
    "        #vae_gumbel_truncated.t /= 10\n",
    "        print(\"Going post burn in\")\n",
    "    if epoch > (n_epochs//5*4):\n",
    "        gradient_post_burn_in += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_post_burnin += sample_subset(vae_gumbel_truncated.logit_enc, \n",
    "                                                        vae_gumbel_truncated.k, \n",
    "                                                        vae_gumbel_truncated.t).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2648e+08, device='cuda:0')\n",
      "tensor(4.2646e+08, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fec61651e50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(gradients_before_burnin[:(D)].mean())\n",
    "print(gradients_before_burnin[(D):].mean())\n",
    "sns.heatmap(gradients_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(666887.5625, device='cuda:0')\n",
      "tensor(31.8088, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7febe90b6210>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(gradient_post_burn_in[:D].mean())\n",
    "print(gradient_post_burn_in[D:].mean())\n",
    "sns.heatmap(gradient_post_burn_in.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10997.4629, device='cuda:0')\n",
      "tensor(522.5376, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7febe8f15c10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(subset_indices_before_burnin[:D].sum())\n",
    "print(subset_indices_before_burnin[D:].sum())\n",
    "sns.heatmap(subset_indices_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2879.9937, device='cuda:0')\n",
      "tensor(0.0063, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7febe8f0a110>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(subset_indices_post_burnin[:D].sum())\n",
    "print(subset_indices_post_burnin[D:].sum())\n",
    "sns.heatmap(subset_indices_post_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1180e+02, 1.3306e+02, 6.0280e+01, 1.2249e+02, 9.9101e+01, 1.4094e+02,\n",
       "        1.3757e+02, 4.1213e-15, 1.0705e+02, 1.3629e+02, 1.0501e-08, 8.4122e+01,\n",
       "        1.3240e+02, 1.9941e+00, 1.0833e+02, 5.4244e+01, 1.2476e+02, 1.2238e+02,\n",
       "        8.9732e+00, 1.3396e+02, 1.2066e+02, 1.4261e+02, 1.3090e+02, 1.2935e+02,\n",
       "        9.5645e-01, 1.2712e+02, 1.2000e+02, 6.8564e+01, 1.1094e+02, 1.0914e+02],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_indices_post_burnin[:(D)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6597e-05, 6.6677e-23, 7.8887e-16, 5.5094e-17, 1.8915e-23, 1.8068e-19,\n",
       "        8.9318e-21, 6.5295e-20, 1.9533e-25, 6.2159e-03, 9.5017e-05, 1.0084e-10,\n",
       "        5.7757e-17, 6.9492e-28, 2.2657e-21, 8.6651e-19, 1.3383e-23, 5.1032e-27,\n",
       "        8.1666e-16, 1.4619e-16, 3.1992e-28, 1.0044e-27, 4.8661e-23, 4.7717e-19,\n",
       "        1.4917e-21, 4.2972e-34, 2.4594e-22, 5.5718e-22, 2.4387e-22, 5.4606e-23],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_indices_post_burnin[(D):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2879.9873, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_indices_post_burnin[:(D)].sum() - subset_indices_post_burnin[(D):].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ind = torch.argsort(sample_subset(vae_gumbel_truncated.logit_enc, \n",
    "                                                        vae_gumbel_truncated.k, \n",
    "                                                        vae_gumbel_truncated.t).view(-1), \n",
    "                        descending = True)[:vae_gumbel_truncated.k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(top_ind < 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(top_ind >= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.6346,  6.2254,  3.1456,  5.9084,  4.4441,  8.7351,  8.0707,  0.1048,\n",
       "          4.1561,  9.5264,  0.1322,  4.1212, 10.5024,  0.7616,  4.7986,  3.2178,\n",
       "          6.9503,  5.2929,  2.0909,  7.5077,  5.7181,  9.2071, 12.4159, 12.8528,\n",
       "          0.5841,  7.6502, 21.3944,  3.6893,  4.8685,  4.4639, -0.3449, -0.0946,\n",
       "         -0.1178, -0.2080, -0.2338, -0.0882, -0.1054, -0.1083, -0.2241, -0.0749,\n",
       "         -0.1138, -0.1215, -0.1309, -0.1090, -0.1153, -0.0795, -0.1255, -0.1243,\n",
       "         -0.1183, -0.1165, -0.1187, -0.1096, -0.1282, -0.0726, -0.0994, -0.0871,\n",
       "         -0.1515, -0.1474, -0.1030, -0.2300]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated.logit_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.6186,  6.2223,  3.1395,  5.8893,  4.4262,  8.7137,  8.0437,  0.0999,\n",
       "         4.1529,  9.5030,  0.1314,  4.1048, 10.4765,  0.7549,  4.7832,  3.2178,\n",
       "         6.9146,  5.2851,  2.0739,  7.4715,  5.6973,  9.1733, 12.3761, 12.8058,\n",
       "         0.5793,  7.6275, 21.3061,  3.6789,  4.8518,  4.4516, -0.3438, -0.0943,\n",
       "        -0.1173, -0.2073, -0.2331, -0.0879, -0.1051, -0.1080, -0.2235, -0.0746,\n",
       "        -0.1134, -0.1211, -0.1305, -0.1087, -0.1149, -0.0793, -0.1250, -0.1240,\n",
       "        -0.1180, -0.1160, -0.1183, -0.1092, -0.1278, -0.0724, -0.0991, -0.0868,\n",
       "        -0.1510, -0.1469, -0.1026, -0.2293], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated.weight_creator(train_data[64:128, ]).mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9295e-01, 1.0000e+00, 0.0000e+00, 1.0000e+00, 9.9843e-01, 1.0000e+00,\n",
       "         9.9910e-01, 0.0000e+00, 1.8459e-40, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00, 0.0000e+00, 1.0000e+00, 1.0001e+00, 9.9987e-01, 1.0000e+00,\n",
       "         1.0000e+00, 1.0009e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0016e+00, 1.0070e+00,\n",
       "         1.4203e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         4.7378e-42, 0.0000e+00, 3.5942e-29, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.3351e-18, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7132e-21,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0812e-07, 0.0000e+00, 0.0000e+00]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_subset(vae_gumbel_truncated.logit_enc, \n",
    "                                                        vae_gumbel_truncated.k, \n",
    "                                                        vae_gumbel_truncated.t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
