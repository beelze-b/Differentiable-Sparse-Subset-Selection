{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know from Notebook 10 that trying to make it learn one set of logits per batch is good and gets features right.\n",
    "\n",
    "How can we make the logits consistent over batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will collect the gradients and gumbel selected values after 5 epochs in each mode. The two modes will be the behavior before burn-in and the behavior after mode-in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will follow Notebook 10 with slight modifications (described right before). Remember in Notebook 10, we explored behaviors when all the features were real vs when half the features were noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 600\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 10000\n",
    "z_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14, device='cuda:0')\n",
      "tensor(18, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor(17, device='cuda:0')\n",
      "tensor(16, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)\n",
    "\n",
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)\n",
    "\n",
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)\n",
    "\n",
    "actual_data = data_mapper(latent_data)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add noiise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0013,  0.0135,  0.0054,  ..., -0.0047,  0.0033, -0.0097],\n",
       "        [ 0.0080, -0.0057,  0.0010,  ...,  0.0009, -0.0134,  0.0105],\n",
       "        [-0.0103, -0.0029,  0.0185,  ..., -0.0133, -0.0037,  0.0134],\n",
       "        ...,\n",
       "        [ 0.0073, -0.0149, -0.0108,  ..., -0.0047, -0.0137,  0.0070],\n",
       "        [ 0.0006, -0.0141, -0.0124,  ..., -0.0085,  0.0069, -0.0110],\n",
       "        [-0.0159,  0.0177, -0.0087,  ..., -0.0076, -0.0009,  0.0078]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_features = torch.empty(N * D).normal_(mean=0,std=0.01).reshape(N, D).to(device)\n",
    "noise_features.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = torch.cat([actual_data, noise_features], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 60])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)\n",
    "\n",
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_t = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of what worked before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_truncated = VAE_Gumbel(2*D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 42.025238\n",
      "Train Epoch: 1 [1280/8000 (16%)]\tLoss: 41.364944\n",
      "Train Epoch: 1 [2560/8000 (32%)]\tLoss: 40.814362\n",
      "Train Epoch: 1 [3840/8000 (48%)]\tLoss: 40.379112\n",
      "Train Epoch: 1 [5120/8000 (64%)]\tLoss: 39.789692\n",
      "Train Epoch: 1 [6400/8000 (80%)]\tLoss: 39.276009\n",
      "Train Epoch: 1 [7680/8000 (96%)]\tLoss: 38.740120\n",
      "====> Epoch: 1 Average loss: 40.2651\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 38.672668\n",
      "Train Epoch: 2 [1280/8000 (16%)]\tLoss: 37.857777\n",
      "Train Epoch: 2 [2560/8000 (32%)]\tLoss: 37.326408\n",
      "Train Epoch: 2 [3840/8000 (48%)]\tLoss: 36.102455\n",
      "Train Epoch: 2 [5120/8000 (64%)]\tLoss: 35.597836\n",
      "Train Epoch: 2 [6400/8000 (80%)]\tLoss: 34.990261\n",
      "Train Epoch: 2 [7680/8000 (96%)]\tLoss: 34.360985\n",
      "====> Epoch: 2 Average loss: 36.4155\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 34.524200\n",
      "Train Epoch: 3 [1280/8000 (16%)]\tLoss: 34.207508\n",
      "Train Epoch: 3 [2560/8000 (32%)]\tLoss: 33.857361\n",
      "Train Epoch: 3 [3840/8000 (48%)]\tLoss: 33.793507\n",
      "Train Epoch: 3 [5120/8000 (64%)]\tLoss: 33.989220\n",
      "Train Epoch: 3 [6400/8000 (80%)]\tLoss: 33.565918\n",
      "Train Epoch: 3 [7680/8000 (96%)]\tLoss: 33.848133\n",
      "====> Epoch: 3 Average loss: 33.8504\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 33.799870\n",
      "Train Epoch: 4 [1280/8000 (16%)]\tLoss: 33.303463\n",
      "Train Epoch: 4 [2560/8000 (32%)]\tLoss: 33.576908\n",
      "Train Epoch: 4 [3840/8000 (48%)]\tLoss: 33.158379\n",
      "Train Epoch: 4 [5120/8000 (64%)]\tLoss: 33.516277\n",
      "Train Epoch: 4 [6400/8000 (80%)]\tLoss: 33.234558\n",
      "Train Epoch: 4 [7680/8000 (96%)]\tLoss: 33.036366\n",
      "====> Epoch: 4 Average loss: 33.2807\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 33.413303\n",
      "Train Epoch: 5 [1280/8000 (16%)]\tLoss: 32.934505\n",
      "Train Epoch: 5 [2560/8000 (32%)]\tLoss: 33.109653\n",
      "Train Epoch: 5 [3840/8000 (48%)]\tLoss: 33.061752\n",
      "Train Epoch: 5 [5120/8000 (64%)]\tLoss: 33.524673\n",
      "Train Epoch: 5 [6400/8000 (80%)]\tLoss: 32.786255\n",
      "Train Epoch: 5 [7680/8000 (96%)]\tLoss: 32.754799\n",
      "====> Epoch: 5 Average loss: 33.0634\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 32.994030\n",
      "Train Epoch: 6 [1280/8000 (16%)]\tLoss: 32.909019\n",
      "Train Epoch: 6 [2560/8000 (32%)]\tLoss: 32.953922\n",
      "Train Epoch: 6 [3840/8000 (48%)]\tLoss: 33.217514\n",
      "Train Epoch: 6 [5120/8000 (64%)]\tLoss: 32.976513\n",
      "Train Epoch: 6 [6400/8000 (80%)]\tLoss: 32.537460\n",
      "Train Epoch: 6 [7680/8000 (96%)]\tLoss: 32.742439\n",
      "====> Epoch: 6 Average loss: 32.9366\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 32.774220\n",
      "Train Epoch: 7 [1280/8000 (16%)]\tLoss: 33.271671\n",
      "Train Epoch: 7 [2560/8000 (32%)]\tLoss: 32.981609\n",
      "Train Epoch: 7 [3840/8000 (48%)]\tLoss: 32.672764\n",
      "Train Epoch: 7 [5120/8000 (64%)]\tLoss: 33.075489\n",
      "Train Epoch: 7 [6400/8000 (80%)]\tLoss: 32.722992\n",
      "Train Epoch: 7 [7680/8000 (96%)]\tLoss: 32.475712\n",
      "====> Epoch: 7 Average loss: 32.8518\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 32.882839\n",
      "Train Epoch: 8 [1280/8000 (16%)]\tLoss: 32.396839\n",
      "Train Epoch: 8 [2560/8000 (32%)]\tLoss: 32.637543\n",
      "Train Epoch: 8 [3840/8000 (48%)]\tLoss: 33.276424\n",
      "Train Epoch: 8 [5120/8000 (64%)]\tLoss: 32.902107\n",
      "Train Epoch: 8 [6400/8000 (80%)]\tLoss: 32.804340\n",
      "Train Epoch: 8 [7680/8000 (96%)]\tLoss: 32.912449\n",
      "====> Epoch: 8 Average loss: 32.7576\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 32.658039\n",
      "Train Epoch: 9 [1280/8000 (16%)]\tLoss: 32.446171\n",
      "Train Epoch: 9 [2560/8000 (32%)]\tLoss: 32.713200\n",
      "Train Epoch: 9 [3840/8000 (48%)]\tLoss: 33.240654\n",
      "Train Epoch: 9 [5120/8000 (64%)]\tLoss: 32.524624\n",
      "Train Epoch: 9 [6400/8000 (80%)]\tLoss: 32.262215\n",
      "Train Epoch: 9 [7680/8000 (96%)]\tLoss: 32.283157\n",
      "====> Epoch: 9 Average loss: 32.6868\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 32.632927\n",
      "Train Epoch: 10 [1280/8000 (16%)]\tLoss: 32.521542\n",
      "Train Epoch: 10 [2560/8000 (32%)]\tLoss: 32.703678\n",
      "Train Epoch: 10 [3840/8000 (48%)]\tLoss: 32.128487\n",
      "Train Epoch: 10 [5120/8000 (64%)]\tLoss: 32.457207\n",
      "Train Epoch: 10 [6400/8000 (80%)]\tLoss: 32.746052\n",
      "Train Epoch: 10 [7680/8000 (96%)]\tLoss: 32.952740\n",
      "====> Epoch: 10 Average loss: 32.6228\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 32.998230\n",
      "Train Epoch: 11 [1280/8000 (16%)]\tLoss: 32.762627\n",
      "Train Epoch: 11 [2560/8000 (32%)]\tLoss: 32.435284\n",
      "Train Epoch: 11 [3840/8000 (48%)]\tLoss: 32.610306\n",
      "Train Epoch: 11 [5120/8000 (64%)]\tLoss: 33.107044\n",
      "Train Epoch: 11 [6400/8000 (80%)]\tLoss: 32.309689\n",
      "Train Epoch: 11 [7680/8000 (96%)]\tLoss: 32.288441\n",
      "====> Epoch: 11 Average loss: 32.5476\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 32.443794\n",
      "Train Epoch: 12 [1280/8000 (16%)]\tLoss: 32.266121\n",
      "Train Epoch: 12 [2560/8000 (32%)]\tLoss: 32.694561\n",
      "Train Epoch: 12 [3840/8000 (48%)]\tLoss: 32.234856\n",
      "Train Epoch: 12 [5120/8000 (64%)]\tLoss: 33.055431\n",
      "Train Epoch: 12 [6400/8000 (80%)]\tLoss: 32.223148\n",
      "Train Epoch: 12 [7680/8000 (96%)]\tLoss: 32.301590\n",
      "====> Epoch: 12 Average loss: 32.4653\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 32.662106\n",
      "Train Epoch: 13 [1280/8000 (16%)]\tLoss: 32.812073\n",
      "Train Epoch: 13 [2560/8000 (32%)]\tLoss: 32.354168\n",
      "Train Epoch: 13 [3840/8000 (48%)]\tLoss: 32.176258\n",
      "Train Epoch: 13 [5120/8000 (64%)]\tLoss: 32.109077\n",
      "Train Epoch: 13 [6400/8000 (80%)]\tLoss: 32.352154\n",
      "Train Epoch: 13 [7680/8000 (96%)]\tLoss: 32.553181\n",
      "====> Epoch: 13 Average loss: 32.3266\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 32.160580\n",
      "Train Epoch: 14 [1280/8000 (16%)]\tLoss: 31.838955\n",
      "Train Epoch: 14 [2560/8000 (32%)]\tLoss: 32.166397\n",
      "Train Epoch: 14 [3840/8000 (48%)]\tLoss: 32.183048\n",
      "Train Epoch: 14 [5120/8000 (64%)]\tLoss: 32.006023\n",
      "Train Epoch: 14 [6400/8000 (80%)]\tLoss: 32.356613\n",
      "Train Epoch: 14 [7680/8000 (96%)]\tLoss: 32.009132\n",
      "====> Epoch: 14 Average loss: 32.1240\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 32.085678\n",
      "Train Epoch: 15 [1280/8000 (16%)]\tLoss: 32.338913\n",
      "Train Epoch: 15 [2560/8000 (32%)]\tLoss: 32.065174\n",
      "Train Epoch: 15 [3840/8000 (48%)]\tLoss: 31.774622\n",
      "Train Epoch: 15 [5120/8000 (64%)]\tLoss: 31.984354\n",
      "Train Epoch: 15 [6400/8000 (80%)]\tLoss: 31.839842\n",
      "Train Epoch: 15 [7680/8000 (96%)]\tLoss: 31.792555\n",
      "====> Epoch: 15 Average loss: 31.9302\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 31.602533\n",
      "Train Epoch: 16 [1280/8000 (16%)]\tLoss: 31.716820\n",
      "Train Epoch: 16 [2560/8000 (32%)]\tLoss: 31.676506\n",
      "Train Epoch: 16 [3840/8000 (48%)]\tLoss: 31.640638\n",
      "Train Epoch: 16 [5120/8000 (64%)]\tLoss: 31.864273\n",
      "Train Epoch: 16 [6400/8000 (80%)]\tLoss: 31.651772\n",
      "Train Epoch: 16 [7680/8000 (96%)]\tLoss: 31.757149\n",
      "====> Epoch: 16 Average loss: 31.7540\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 31.554667\n",
      "Train Epoch: 17 [1280/8000 (16%)]\tLoss: 31.702419\n",
      "Train Epoch: 17 [2560/8000 (32%)]\tLoss: 31.769871\n",
      "Train Epoch: 17 [3840/8000 (48%)]\tLoss: 31.712790\n",
      "Train Epoch: 17 [5120/8000 (64%)]\tLoss: 31.449863\n",
      "Train Epoch: 17 [6400/8000 (80%)]\tLoss: 31.824049\n",
      "Train Epoch: 17 [7680/8000 (96%)]\tLoss: 31.496225\n",
      "====> Epoch: 17 Average loss: 31.5921\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 31.628248\n",
      "Train Epoch: 18 [1280/8000 (16%)]\tLoss: 31.519743\n",
      "Train Epoch: 18 [2560/8000 (32%)]\tLoss: 31.776915\n",
      "Train Epoch: 18 [3840/8000 (48%)]\tLoss: 31.633543\n",
      "Train Epoch: 18 [5120/8000 (64%)]\tLoss: 31.200100\n",
      "Train Epoch: 18 [6400/8000 (80%)]\tLoss: 31.221024\n",
      "Train Epoch: 18 [7680/8000 (96%)]\tLoss: 31.690159\n",
      "====> Epoch: 18 Average loss: 31.4865\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 31.661751\n",
      "Train Epoch: 19 [1280/8000 (16%)]\tLoss: 31.286144\n",
      "Train Epoch: 19 [2560/8000 (32%)]\tLoss: 31.622602\n",
      "Train Epoch: 19 [3840/8000 (48%)]\tLoss: 31.672235\n",
      "Train Epoch: 19 [5120/8000 (64%)]\tLoss: 31.566187\n",
      "Train Epoch: 19 [6400/8000 (80%)]\tLoss: 31.198631\n",
      "Train Epoch: 19 [7680/8000 (96%)]\tLoss: 31.225433\n",
      "====> Epoch: 19 Average loss: 31.4005\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 31.846987\n",
      "Train Epoch: 20 [1280/8000 (16%)]\tLoss: 31.301657\n",
      "Train Epoch: 20 [2560/8000 (32%)]\tLoss: 31.447721\n",
      "Train Epoch: 20 [3840/8000 (48%)]\tLoss: 31.414515\n",
      "Train Epoch: 20 [5120/8000 (64%)]\tLoss: 30.928062\n",
      "Train Epoch: 20 [6400/8000 (80%)]\tLoss: 31.264458\n",
      "Train Epoch: 20 [7680/8000 (96%)]\tLoss: 31.517467\n",
      "====> Epoch: 20 Average loss: 31.3469\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 30.898067\n",
      "Train Epoch: 21 [1280/8000 (16%)]\tLoss: 31.300766\n",
      "Train Epoch: 21 [2560/8000 (32%)]\tLoss: 31.346365\n",
      "Train Epoch: 21 [3840/8000 (48%)]\tLoss: 31.333395\n",
      "Train Epoch: 21 [5120/8000 (64%)]\tLoss: 30.960016\n",
      "Train Epoch: 21 [6400/8000 (80%)]\tLoss: 31.228962\n",
      "Train Epoch: 21 [7680/8000 (96%)]\tLoss: 30.949421\n",
      "====> Epoch: 21 Average loss: 31.3033\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 31.397312\n",
      "Train Epoch: 22 [1280/8000 (16%)]\tLoss: 31.287304\n",
      "Train Epoch: 22 [2560/8000 (32%)]\tLoss: 31.315557\n",
      "Train Epoch: 22 [3840/8000 (48%)]\tLoss: 31.403999\n",
      "Train Epoch: 22 [5120/8000 (64%)]\tLoss: 30.975504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [6400/8000 (80%)]\tLoss: 31.002998\n",
      "Train Epoch: 22 [7680/8000 (96%)]\tLoss: 31.257687\n",
      "====> Epoch: 22 Average loss: 31.2449\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 31.386927\n",
      "Train Epoch: 23 [1280/8000 (16%)]\tLoss: 31.034521\n",
      "Train Epoch: 23 [2560/8000 (32%)]\tLoss: 31.336792\n",
      "Train Epoch: 23 [3840/8000 (48%)]\tLoss: 31.313383\n",
      "Train Epoch: 23 [5120/8000 (64%)]\tLoss: 31.017269\n",
      "Train Epoch: 23 [6400/8000 (80%)]\tLoss: 31.128880\n",
      "Train Epoch: 23 [7680/8000 (96%)]\tLoss: 31.526924\n",
      "====> Epoch: 23 Average loss: 31.1897\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 31.333666\n",
      "Train Epoch: 24 [1280/8000 (16%)]\tLoss: 31.354042\n",
      "Train Epoch: 24 [2560/8000 (32%)]\tLoss: 31.074629\n",
      "Train Epoch: 24 [3840/8000 (48%)]\tLoss: 31.058277\n",
      "Train Epoch: 24 [5120/8000 (64%)]\tLoss: 31.226425\n",
      "Train Epoch: 24 [6400/8000 (80%)]\tLoss: 30.920198\n",
      "Train Epoch: 24 [7680/8000 (96%)]\tLoss: 30.927282\n",
      "====> Epoch: 24 Average loss: 31.1282\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 31.069897\n",
      "Train Epoch: 25 [1280/8000 (16%)]\tLoss: 31.132643\n",
      "Train Epoch: 25 [2560/8000 (32%)]\tLoss: 31.325737\n",
      "Train Epoch: 25 [3840/8000 (48%)]\tLoss: 30.760889\n",
      "Train Epoch: 25 [5120/8000 (64%)]\tLoss: 30.963699\n",
      "Train Epoch: 25 [6400/8000 (80%)]\tLoss: 31.093763\n",
      "Train Epoch: 25 [7680/8000 (96%)]\tLoss: 30.789448\n",
      "====> Epoch: 25 Average loss: 31.0760\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 31.345116\n",
      "Train Epoch: 26 [1280/8000 (16%)]\tLoss: 31.181999\n",
      "Train Epoch: 26 [2560/8000 (32%)]\tLoss: 30.990536\n",
      "Train Epoch: 26 [3840/8000 (48%)]\tLoss: 30.391350\n",
      "Train Epoch: 26 [5120/8000 (64%)]\tLoss: 31.187237\n",
      "Train Epoch: 26 [6400/8000 (80%)]\tLoss: 31.298229\n",
      "Train Epoch: 26 [7680/8000 (96%)]\tLoss: 31.295650\n",
      "====> Epoch: 26 Average loss: 30.9965\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 30.999716\n",
      "Train Epoch: 27 [1280/8000 (16%)]\tLoss: 31.021372\n",
      "Train Epoch: 27 [2560/8000 (32%)]\tLoss: 31.031082\n",
      "Train Epoch: 27 [3840/8000 (48%)]\tLoss: 30.937462\n",
      "Train Epoch: 27 [5120/8000 (64%)]\tLoss: 30.732470\n",
      "Train Epoch: 27 [6400/8000 (80%)]\tLoss: 30.930380\n",
      "Train Epoch: 27 [7680/8000 (96%)]\tLoss: 30.810141\n",
      "====> Epoch: 27 Average loss: 30.9324\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 31.411095\n",
      "Train Epoch: 28 [1280/8000 (16%)]\tLoss: 30.839569\n",
      "Train Epoch: 28 [2560/8000 (32%)]\tLoss: 30.605730\n",
      "Train Epoch: 28 [3840/8000 (48%)]\tLoss: 31.264437\n",
      "Train Epoch: 28 [5120/8000 (64%)]\tLoss: 30.755882\n",
      "Train Epoch: 28 [6400/8000 (80%)]\tLoss: 30.611368\n",
      "Train Epoch: 28 [7680/8000 (96%)]\tLoss: 30.924812\n",
      "====> Epoch: 28 Average loss: 30.8663\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 31.219711\n",
      "Train Epoch: 29 [1280/8000 (16%)]\tLoss: 31.148447\n",
      "Train Epoch: 29 [2560/8000 (32%)]\tLoss: 30.744963\n",
      "Train Epoch: 29 [3840/8000 (48%)]\tLoss: 31.100080\n",
      "Train Epoch: 29 [5120/8000 (64%)]\tLoss: 31.061958\n",
      "Train Epoch: 29 [6400/8000 (80%)]\tLoss: 30.848608\n",
      "Train Epoch: 29 [7680/8000 (96%)]\tLoss: 30.604452\n",
      "====> Epoch: 29 Average loss: 30.8231\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 30.544241\n",
      "Train Epoch: 30 [1280/8000 (16%)]\tLoss: 30.540091\n",
      "Train Epoch: 30 [2560/8000 (32%)]\tLoss: 30.510828\n",
      "Train Epoch: 30 [3840/8000 (48%)]\tLoss: 30.681211\n",
      "Train Epoch: 30 [5120/8000 (64%)]\tLoss: 30.627148\n",
      "Train Epoch: 30 [6400/8000 (80%)]\tLoss: 30.968918\n",
      "Train Epoch: 30 [7680/8000 (96%)]\tLoss: 30.819656\n",
      "====> Epoch: 30 Average loss: 30.7692\n",
      "Train Epoch: 31 [0/8000 (0%)]\tLoss: 30.724447\n",
      "Train Epoch: 31 [1280/8000 (16%)]\tLoss: 30.572319\n",
      "Train Epoch: 31 [2560/8000 (32%)]\tLoss: 30.624002\n",
      "Train Epoch: 31 [3840/8000 (48%)]\tLoss: 30.467918\n",
      "Train Epoch: 31 [5120/8000 (64%)]\tLoss: 30.872511\n",
      "Train Epoch: 31 [6400/8000 (80%)]\tLoss: 30.476053\n",
      "Train Epoch: 31 [7680/8000 (96%)]\tLoss: 30.919685\n",
      "====> Epoch: 31 Average loss: 30.7463\n",
      "Train Epoch: 32 [0/8000 (0%)]\tLoss: 30.974977\n",
      "Train Epoch: 32 [1280/8000 (16%)]\tLoss: 30.762798\n",
      "Train Epoch: 32 [2560/8000 (32%)]\tLoss: 30.776243\n",
      "Train Epoch: 32 [3840/8000 (48%)]\tLoss: 30.890688\n",
      "Train Epoch: 32 [5120/8000 (64%)]\tLoss: 30.918310\n",
      "Train Epoch: 32 [6400/8000 (80%)]\tLoss: 30.804104\n",
      "Train Epoch: 32 [7680/8000 (96%)]\tLoss: 30.996328\n",
      "====> Epoch: 32 Average loss: 30.7156\n",
      "Train Epoch: 33 [0/8000 (0%)]\tLoss: 30.546164\n",
      "Train Epoch: 33 [1280/8000 (16%)]\tLoss: 30.717712\n",
      "Train Epoch: 33 [2560/8000 (32%)]\tLoss: 30.567940\n",
      "Train Epoch: 33 [3840/8000 (48%)]\tLoss: 30.621641\n",
      "Train Epoch: 33 [5120/8000 (64%)]\tLoss: 30.944834\n",
      "Train Epoch: 33 [6400/8000 (80%)]\tLoss: 30.752729\n",
      "Train Epoch: 33 [7680/8000 (96%)]\tLoss: 30.190918\n",
      "====> Epoch: 33 Average loss: 30.6846\n",
      "Train Epoch: 34 [0/8000 (0%)]\tLoss: 30.734093\n",
      "Train Epoch: 34 [1280/8000 (16%)]\tLoss: 30.911144\n",
      "Train Epoch: 34 [2560/8000 (32%)]\tLoss: 30.565552\n",
      "Train Epoch: 34 [3840/8000 (48%)]\tLoss: 30.978815\n",
      "Train Epoch: 34 [5120/8000 (64%)]\tLoss: 30.667128\n",
      "Train Epoch: 34 [6400/8000 (80%)]\tLoss: 30.805405\n",
      "Train Epoch: 34 [7680/8000 (96%)]\tLoss: 30.830660\n",
      "====> Epoch: 34 Average loss: 30.6678\n",
      "Train Epoch: 35 [0/8000 (0%)]\tLoss: 30.509022\n",
      "Train Epoch: 35 [1280/8000 (16%)]\tLoss: 30.592136\n",
      "Train Epoch: 35 [2560/8000 (32%)]\tLoss: 30.534079\n",
      "Train Epoch: 35 [3840/8000 (48%)]\tLoss: 30.622890\n",
      "Train Epoch: 35 [5120/8000 (64%)]\tLoss: 30.538937\n",
      "Train Epoch: 35 [6400/8000 (80%)]\tLoss: 30.741913\n",
      "Train Epoch: 35 [7680/8000 (96%)]\tLoss: 30.659887\n",
      "====> Epoch: 35 Average loss: 30.6336\n",
      "Train Epoch: 36 [0/8000 (0%)]\tLoss: 30.756613\n",
      "Train Epoch: 36 [1280/8000 (16%)]\tLoss: 30.612083\n",
      "Train Epoch: 36 [2560/8000 (32%)]\tLoss: 30.847382\n",
      "Train Epoch: 36 [3840/8000 (48%)]\tLoss: 30.569569\n",
      "Train Epoch: 36 [5120/8000 (64%)]\tLoss: 30.594810\n",
      "Train Epoch: 36 [6400/8000 (80%)]\tLoss: 30.536781\n",
      "Train Epoch: 36 [7680/8000 (96%)]\tLoss: 30.818413\n",
      "====> Epoch: 36 Average loss: 30.6315\n",
      "Train Epoch: 37 [0/8000 (0%)]\tLoss: 30.803120\n",
      "Train Epoch: 37 [1280/8000 (16%)]\tLoss: 30.737488\n",
      "Train Epoch: 37 [2560/8000 (32%)]\tLoss: 30.163515\n",
      "Train Epoch: 37 [3840/8000 (48%)]\tLoss: 30.360943\n",
      "Train Epoch: 37 [5120/8000 (64%)]\tLoss: 30.783054\n",
      "Train Epoch: 37 [6400/8000 (80%)]\tLoss: 30.673744\n",
      "Train Epoch: 37 [7680/8000 (96%)]\tLoss: 30.276329\n",
      "====> Epoch: 37 Average loss: 30.6043\n",
      "Train Epoch: 38 [0/8000 (0%)]\tLoss: 30.749268\n",
      "Train Epoch: 38 [1280/8000 (16%)]\tLoss: 30.538029\n",
      "Train Epoch: 38 [2560/8000 (32%)]\tLoss: 30.623739\n",
      "Train Epoch: 38 [3840/8000 (48%)]\tLoss: 30.658094\n",
      "Train Epoch: 38 [5120/8000 (64%)]\tLoss: 30.282120\n",
      "Train Epoch: 38 [6400/8000 (80%)]\tLoss: 30.607889\n",
      "Train Epoch: 38 [7680/8000 (96%)]\tLoss: 30.829344\n",
      "====> Epoch: 38 Average loss: 30.5846\n",
      "Train Epoch: 39 [0/8000 (0%)]\tLoss: 30.570480\n",
      "Train Epoch: 39 [1280/8000 (16%)]\tLoss: 30.509211\n",
      "Train Epoch: 39 [2560/8000 (32%)]\tLoss: 30.525963\n",
      "Train Epoch: 39 [3840/8000 (48%)]\tLoss: 30.760983\n",
      "Train Epoch: 39 [5120/8000 (64%)]\tLoss: 30.562071\n",
      "Train Epoch: 39 [6400/8000 (80%)]\tLoss: 30.283062\n",
      "Train Epoch: 39 [7680/8000 (96%)]\tLoss: 30.599960\n",
      "====> Epoch: 39 Average loss: 30.5556\n",
      "Train Epoch: 40 [0/8000 (0%)]\tLoss: 30.534569\n",
      "Train Epoch: 40 [1280/8000 (16%)]\tLoss: 30.648764\n",
      "Train Epoch: 40 [2560/8000 (32%)]\tLoss: 30.670582\n",
      "Train Epoch: 40 [3840/8000 (48%)]\tLoss: 30.565704\n",
      "Train Epoch: 40 [5120/8000 (64%)]\tLoss: 30.492767\n",
      "Train Epoch: 40 [6400/8000 (80%)]\tLoss: 30.347454\n",
      "Train Epoch: 40 [7680/8000 (96%)]\tLoss: 30.541317\n",
      "====> Epoch: 40 Average loss: 30.5465\n",
      "Train Epoch: 41 [0/8000 (0%)]\tLoss: 30.542652\n",
      "Train Epoch: 41 [1280/8000 (16%)]\tLoss: 30.695110\n",
      "Train Epoch: 41 [2560/8000 (32%)]\tLoss: 30.473782\n",
      "Train Epoch: 41 [3840/8000 (48%)]\tLoss: 30.421865\n",
      "Train Epoch: 41 [5120/8000 (64%)]\tLoss: 30.446409\n",
      "Train Epoch: 41 [6400/8000 (80%)]\tLoss: 30.746254\n",
      "Train Epoch: 41 [7680/8000 (96%)]\tLoss: 30.247858\n",
      "====> Epoch: 41 Average loss: 30.5278\n",
      "Train Epoch: 42 [0/8000 (0%)]\tLoss: 30.229637\n",
      "Train Epoch: 42 [1280/8000 (16%)]\tLoss: 30.471193\n",
      "Train Epoch: 42 [2560/8000 (32%)]\tLoss: 31.016144\n",
      "Train Epoch: 42 [3840/8000 (48%)]\tLoss: 30.375114\n",
      "Train Epoch: 42 [5120/8000 (64%)]\tLoss: 30.829250\n",
      "Train Epoch: 42 [6400/8000 (80%)]\tLoss: 30.525665\n",
      "Train Epoch: 42 [7680/8000 (96%)]\tLoss: 30.024839\n",
      "====> Epoch: 42 Average loss: 30.4999\n",
      "Train Epoch: 43 [0/8000 (0%)]\tLoss: 30.466454\n",
      "Train Epoch: 43 [1280/8000 (16%)]\tLoss: 30.559338\n",
      "Train Epoch: 43 [2560/8000 (32%)]\tLoss: 30.450031\n",
      "Train Epoch: 43 [3840/8000 (48%)]\tLoss: 30.579544\n",
      "Train Epoch: 43 [5120/8000 (64%)]\tLoss: 30.681602\n",
      "Train Epoch: 43 [6400/8000 (80%)]\tLoss: 30.371241\n",
      "Train Epoch: 43 [7680/8000 (96%)]\tLoss: 30.451809\n",
      "====> Epoch: 43 Average loss: 30.4889\n",
      "Train Epoch: 44 [0/8000 (0%)]\tLoss: 30.266289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 [1280/8000 (16%)]\tLoss: 30.636246\n",
      "Train Epoch: 44 [2560/8000 (32%)]\tLoss: 30.380066\n",
      "Train Epoch: 44 [3840/8000 (48%)]\tLoss: 30.260462\n",
      "Train Epoch: 44 [5120/8000 (64%)]\tLoss: 30.643415\n",
      "Train Epoch: 44 [6400/8000 (80%)]\tLoss: 30.413063\n",
      "Train Epoch: 44 [7680/8000 (96%)]\tLoss: 30.506575\n",
      "====> Epoch: 44 Average loss: 30.4689\n",
      "Train Epoch: 45 [0/8000 (0%)]\tLoss: 30.496719\n",
      "Train Epoch: 45 [1280/8000 (16%)]\tLoss: 30.805012\n",
      "Train Epoch: 45 [2560/8000 (32%)]\tLoss: 30.479860\n",
      "Train Epoch: 45 [3840/8000 (48%)]\tLoss: 29.870306\n",
      "Train Epoch: 45 [5120/8000 (64%)]\tLoss: 30.277435\n",
      "Train Epoch: 45 [6400/8000 (80%)]\tLoss: 30.269686\n",
      "Train Epoch: 45 [7680/8000 (96%)]\tLoss: 30.485321\n",
      "====> Epoch: 45 Average loss: 30.4502\n",
      "Train Epoch: 46 [0/8000 (0%)]\tLoss: 30.353239\n",
      "Train Epoch: 46 [1280/8000 (16%)]\tLoss: 30.411106\n",
      "Train Epoch: 46 [2560/8000 (32%)]\tLoss: 30.417006\n",
      "Train Epoch: 46 [3840/8000 (48%)]\tLoss: 30.377390\n",
      "Train Epoch: 46 [5120/8000 (64%)]\tLoss: 30.554312\n",
      "Train Epoch: 46 [6400/8000 (80%)]\tLoss: 30.333534\n",
      "Train Epoch: 46 [7680/8000 (96%)]\tLoss: 30.056545\n",
      "====> Epoch: 46 Average loss: 30.4311\n",
      "Train Epoch: 47 [0/8000 (0%)]\tLoss: 30.307779\n",
      "Train Epoch: 47 [1280/8000 (16%)]\tLoss: 30.612091\n",
      "Train Epoch: 47 [2560/8000 (32%)]\tLoss: 30.292131\n",
      "Train Epoch: 47 [3840/8000 (48%)]\tLoss: 30.250044\n",
      "Train Epoch: 47 [5120/8000 (64%)]\tLoss: 30.081209\n",
      "Train Epoch: 47 [6400/8000 (80%)]\tLoss: 30.556421\n",
      "Train Epoch: 47 [7680/8000 (96%)]\tLoss: 30.725319\n",
      "====> Epoch: 47 Average loss: 30.4191\n",
      "Train Epoch: 48 [0/8000 (0%)]\tLoss: 30.431877\n",
      "Train Epoch: 48 [1280/8000 (16%)]\tLoss: 30.312750\n",
      "Train Epoch: 48 [2560/8000 (32%)]\tLoss: 30.228067\n",
      "Train Epoch: 48 [3840/8000 (48%)]\tLoss: 30.165131\n",
      "Train Epoch: 48 [5120/8000 (64%)]\tLoss: 30.055637\n",
      "Train Epoch: 48 [6400/8000 (80%)]\tLoss: 30.177378\n",
      "Train Epoch: 48 [7680/8000 (96%)]\tLoss: 30.514805\n",
      "====> Epoch: 48 Average loss: 30.4012\n",
      "Train Epoch: 49 [0/8000 (0%)]\tLoss: 30.478844\n",
      "Train Epoch: 49 [1280/8000 (16%)]\tLoss: 30.451279\n",
      "Train Epoch: 49 [2560/8000 (32%)]\tLoss: 30.422581\n",
      "Train Epoch: 49 [3840/8000 (48%)]\tLoss: 30.230555\n",
      "Train Epoch: 49 [5120/8000 (64%)]\tLoss: 30.609875\n",
      "Train Epoch: 49 [6400/8000 (80%)]\tLoss: 30.375498\n",
      "Train Epoch: 49 [7680/8000 (96%)]\tLoss: 30.343164\n",
      "====> Epoch: 49 Average loss: 30.3838\n",
      "Train Epoch: 50 [0/8000 (0%)]\tLoss: 30.288044\n",
      "Train Epoch: 50 [1280/8000 (16%)]\tLoss: 30.514090\n",
      "Train Epoch: 50 [2560/8000 (32%)]\tLoss: 30.338631\n",
      "Train Epoch: 50 [3840/8000 (48%)]\tLoss: 30.378546\n",
      "Train Epoch: 50 [5120/8000 (64%)]\tLoss: 30.468901\n",
      "Train Epoch: 50 [6400/8000 (80%)]\tLoss: 30.078487\n",
      "Train Epoch: 50 [7680/8000 (96%)]\tLoss: 30.440418\n",
      "====> Epoch: 50 Average loss: 30.3628\n",
      "Train Epoch: 51 [0/8000 (0%)]\tLoss: 30.096510\n",
      "Train Epoch: 51 [1280/8000 (16%)]\tLoss: 30.284962\n",
      "Train Epoch: 51 [2560/8000 (32%)]\tLoss: 30.344915\n",
      "Train Epoch: 51 [3840/8000 (48%)]\tLoss: 30.131578\n",
      "Train Epoch: 51 [5120/8000 (64%)]\tLoss: 30.061647\n",
      "Train Epoch: 51 [6400/8000 (80%)]\tLoss: 30.354033\n",
      "Train Epoch: 51 [7680/8000 (96%)]\tLoss: 30.657230\n",
      "====> Epoch: 51 Average loss: 30.3526\n",
      "Train Epoch: 52 [0/8000 (0%)]\tLoss: 30.199003\n",
      "Train Epoch: 52 [1280/8000 (16%)]\tLoss: 30.249821\n",
      "Train Epoch: 52 [2560/8000 (32%)]\tLoss: 30.365496\n",
      "Train Epoch: 52 [3840/8000 (48%)]\tLoss: 30.153696\n",
      "Train Epoch: 52 [5120/8000 (64%)]\tLoss: 30.356833\n",
      "Train Epoch: 52 [6400/8000 (80%)]\tLoss: 30.620186\n",
      "Train Epoch: 52 [7680/8000 (96%)]\tLoss: 29.936428\n",
      "====> Epoch: 52 Average loss: 30.3404\n",
      "Train Epoch: 53 [0/8000 (0%)]\tLoss: 30.378798\n",
      "Train Epoch: 53 [1280/8000 (16%)]\tLoss: 30.329987\n",
      "Train Epoch: 53 [2560/8000 (32%)]\tLoss: 30.273851\n",
      "Train Epoch: 53 [3840/8000 (48%)]\tLoss: 30.395370\n",
      "Train Epoch: 53 [5120/8000 (64%)]\tLoss: 30.153521\n",
      "Train Epoch: 53 [6400/8000 (80%)]\tLoss: 30.380171\n",
      "Train Epoch: 53 [7680/8000 (96%)]\tLoss: 30.691137\n",
      "====> Epoch: 53 Average loss: 30.3199\n",
      "Train Epoch: 54 [0/8000 (0%)]\tLoss: 30.231293\n",
      "Train Epoch: 54 [1280/8000 (16%)]\tLoss: 30.168789\n",
      "Train Epoch: 54 [2560/8000 (32%)]\tLoss: 30.293413\n",
      "Train Epoch: 54 [3840/8000 (48%)]\tLoss: 30.275322\n",
      "Train Epoch: 54 [5120/8000 (64%)]\tLoss: 30.379362\n",
      "Train Epoch: 54 [6400/8000 (80%)]\tLoss: 30.388470\n",
      "Train Epoch: 54 [7680/8000 (96%)]\tLoss: 30.025467\n",
      "====> Epoch: 54 Average loss: 30.3058\n",
      "Train Epoch: 55 [0/8000 (0%)]\tLoss: 30.409887\n",
      "Train Epoch: 55 [1280/8000 (16%)]\tLoss: 30.153795\n",
      "Train Epoch: 55 [2560/8000 (32%)]\tLoss: 30.722389\n",
      "Train Epoch: 55 [3840/8000 (48%)]\tLoss: 30.672668\n",
      "Train Epoch: 55 [5120/8000 (64%)]\tLoss: 30.189358\n",
      "Train Epoch: 55 [6400/8000 (80%)]\tLoss: 30.377138\n",
      "Train Epoch: 55 [7680/8000 (96%)]\tLoss: 30.542908\n",
      "====> Epoch: 55 Average loss: 30.2949\n",
      "Train Epoch: 56 [0/8000 (0%)]\tLoss: 30.172884\n",
      "Train Epoch: 56 [1280/8000 (16%)]\tLoss: 30.030653\n",
      "Train Epoch: 56 [2560/8000 (32%)]\tLoss: 30.319157\n",
      "Train Epoch: 56 [3840/8000 (48%)]\tLoss: 30.392611\n",
      "Train Epoch: 56 [5120/8000 (64%)]\tLoss: 30.411505\n",
      "Train Epoch: 56 [6400/8000 (80%)]\tLoss: 30.066761\n",
      "Train Epoch: 56 [7680/8000 (96%)]\tLoss: 30.150232\n",
      "====> Epoch: 56 Average loss: 30.2863\n",
      "Train Epoch: 57 [0/8000 (0%)]\tLoss: 30.227242\n",
      "Train Epoch: 57 [1280/8000 (16%)]\tLoss: 30.066910\n",
      "Train Epoch: 57 [2560/8000 (32%)]\tLoss: 30.358036\n",
      "Train Epoch: 57 [3840/8000 (48%)]\tLoss: 30.579596\n",
      "Train Epoch: 57 [5120/8000 (64%)]\tLoss: 30.095758\n",
      "Train Epoch: 57 [6400/8000 (80%)]\tLoss: 30.469303\n",
      "Train Epoch: 57 [7680/8000 (96%)]\tLoss: 30.341650\n",
      "====> Epoch: 57 Average loss: 30.2750\n",
      "Train Epoch: 58 [0/8000 (0%)]\tLoss: 30.298199\n",
      "Train Epoch: 58 [1280/8000 (16%)]\tLoss: 30.274122\n",
      "Train Epoch: 58 [2560/8000 (32%)]\tLoss: 30.164373\n",
      "Train Epoch: 58 [3840/8000 (48%)]\tLoss: 30.170731\n",
      "Train Epoch: 58 [5120/8000 (64%)]\tLoss: 30.391426\n",
      "Train Epoch: 58 [6400/8000 (80%)]\tLoss: 30.166962\n",
      "Train Epoch: 58 [7680/8000 (96%)]\tLoss: 30.466103\n",
      "====> Epoch: 58 Average loss: 30.2649\n",
      "Train Epoch: 59 [0/8000 (0%)]\tLoss: 30.275072\n",
      "Train Epoch: 59 [1280/8000 (16%)]\tLoss: 30.126520\n",
      "Train Epoch: 59 [2560/8000 (32%)]\tLoss: 30.428137\n",
      "Train Epoch: 59 [3840/8000 (48%)]\tLoss: 30.121229\n",
      "Train Epoch: 59 [5120/8000 (64%)]\tLoss: 30.318094\n",
      "Train Epoch: 59 [6400/8000 (80%)]\tLoss: 30.131350\n",
      "Train Epoch: 59 [7680/8000 (96%)]\tLoss: 30.553270\n",
      "====> Epoch: 59 Average loss: 30.2583\n",
      "Train Epoch: 60 [0/8000 (0%)]\tLoss: 30.486219\n",
      "Train Epoch: 60 [1280/8000 (16%)]\tLoss: 30.083357\n",
      "Train Epoch: 60 [2560/8000 (32%)]\tLoss: 30.118177\n",
      "Train Epoch: 60 [3840/8000 (48%)]\tLoss: 30.303404\n",
      "Train Epoch: 60 [5120/8000 (64%)]\tLoss: 30.069330\n",
      "Train Epoch: 60 [6400/8000 (80%)]\tLoss: 30.410818\n",
      "Train Epoch: 60 [7680/8000 (96%)]\tLoss: 30.120964\n",
      "====> Epoch: 60 Average loss: 30.2435\n",
      "Train Epoch: 61 [0/8000 (0%)]\tLoss: 30.059675\n",
      "Train Epoch: 61 [1280/8000 (16%)]\tLoss: 30.081593\n",
      "Train Epoch: 61 [2560/8000 (32%)]\tLoss: 30.442907\n",
      "Train Epoch: 61 [3840/8000 (48%)]\tLoss: 30.422897\n",
      "Train Epoch: 61 [5120/8000 (64%)]\tLoss: 30.185043\n",
      "Train Epoch: 61 [6400/8000 (80%)]\tLoss: 30.018730\n",
      "Train Epoch: 61 [7680/8000 (96%)]\tLoss: 30.204393\n",
      "====> Epoch: 61 Average loss: 30.2352\n",
      "Train Epoch: 62 [0/8000 (0%)]\tLoss: 30.276852\n",
      "Train Epoch: 62 [1280/8000 (16%)]\tLoss: 30.103148\n",
      "Train Epoch: 62 [2560/8000 (32%)]\tLoss: 30.366798\n",
      "Train Epoch: 62 [3840/8000 (48%)]\tLoss: 30.266720\n",
      "Train Epoch: 62 [5120/8000 (64%)]\tLoss: 30.202116\n",
      "Train Epoch: 62 [6400/8000 (80%)]\tLoss: 30.244740\n",
      "Train Epoch: 62 [7680/8000 (96%)]\tLoss: 30.133665\n",
      "====> Epoch: 62 Average loss: 30.2307\n",
      "Train Epoch: 63 [0/8000 (0%)]\tLoss: 30.038507\n",
      "Train Epoch: 63 [1280/8000 (16%)]\tLoss: 30.207855\n",
      "Train Epoch: 63 [2560/8000 (32%)]\tLoss: 30.136288\n",
      "Train Epoch: 63 [3840/8000 (48%)]\tLoss: 30.289370\n",
      "Train Epoch: 63 [5120/8000 (64%)]\tLoss: 30.038370\n",
      "Train Epoch: 63 [6400/8000 (80%)]\tLoss: 30.291586\n",
      "Train Epoch: 63 [7680/8000 (96%)]\tLoss: 30.361259\n",
      "====> Epoch: 63 Average loss: 30.2210\n",
      "Train Epoch: 64 [0/8000 (0%)]\tLoss: 30.157232\n",
      "Train Epoch: 64 [1280/8000 (16%)]\tLoss: 30.056282\n",
      "Train Epoch: 64 [2560/8000 (32%)]\tLoss: 29.942560\n",
      "Train Epoch: 64 [3840/8000 (48%)]\tLoss: 30.150726\n",
      "Train Epoch: 64 [5120/8000 (64%)]\tLoss: 30.304596\n",
      "Train Epoch: 64 [6400/8000 (80%)]\tLoss: 30.021439\n",
      "Train Epoch: 64 [7680/8000 (96%)]\tLoss: 30.333239\n",
      "====> Epoch: 64 Average loss: 30.2115\n",
      "Train Epoch: 65 [0/8000 (0%)]\tLoss: 30.308651\n",
      "Train Epoch: 65 [1280/8000 (16%)]\tLoss: 30.367142\n",
      "Train Epoch: 65 [2560/8000 (32%)]\tLoss: 30.013716\n",
      "Train Epoch: 65 [3840/8000 (48%)]\tLoss: 29.929173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 65 [5120/8000 (64%)]\tLoss: 30.261757\n",
      "Train Epoch: 65 [6400/8000 (80%)]\tLoss: 30.214964\n",
      "Train Epoch: 65 [7680/8000 (96%)]\tLoss: 30.428467\n",
      "====> Epoch: 65 Average loss: 30.2000\n",
      "Train Epoch: 66 [0/8000 (0%)]\tLoss: 30.205038\n",
      "Train Epoch: 66 [1280/8000 (16%)]\tLoss: 30.331818\n",
      "Train Epoch: 66 [2560/8000 (32%)]\tLoss: 30.319281\n",
      "Train Epoch: 66 [3840/8000 (48%)]\tLoss: 30.250435\n",
      "Train Epoch: 66 [5120/8000 (64%)]\tLoss: 30.162100\n",
      "Train Epoch: 66 [6400/8000 (80%)]\tLoss: 30.575787\n",
      "Train Epoch: 66 [7680/8000 (96%)]\tLoss: 30.229120\n",
      "====> Epoch: 66 Average loss: 30.1954\n",
      "Train Epoch: 67 [0/8000 (0%)]\tLoss: 30.468554\n",
      "Train Epoch: 67 [1280/8000 (16%)]\tLoss: 30.096970\n",
      "Train Epoch: 67 [2560/8000 (32%)]\tLoss: 29.976551\n",
      "Train Epoch: 67 [3840/8000 (48%)]\tLoss: 29.963739\n",
      "Train Epoch: 67 [5120/8000 (64%)]\tLoss: 29.879070\n",
      "Train Epoch: 67 [6400/8000 (80%)]\tLoss: 30.164143\n",
      "Train Epoch: 67 [7680/8000 (96%)]\tLoss: 30.291126\n",
      "====> Epoch: 67 Average loss: 30.1869\n",
      "Train Epoch: 68 [0/8000 (0%)]\tLoss: 30.123741\n",
      "Train Epoch: 68 [1280/8000 (16%)]\tLoss: 30.140886\n",
      "Train Epoch: 68 [2560/8000 (32%)]\tLoss: 30.250460\n",
      "Train Epoch: 68 [3840/8000 (48%)]\tLoss: 30.069017\n",
      "Train Epoch: 68 [5120/8000 (64%)]\tLoss: 30.207933\n",
      "Train Epoch: 68 [6400/8000 (80%)]\tLoss: 30.262060\n",
      "Train Epoch: 68 [7680/8000 (96%)]\tLoss: 29.827835\n",
      "====> Epoch: 68 Average loss: 30.1808\n",
      "Train Epoch: 69 [0/8000 (0%)]\tLoss: 30.224182\n",
      "Train Epoch: 69 [1280/8000 (16%)]\tLoss: 30.062586\n",
      "Train Epoch: 69 [2560/8000 (32%)]\tLoss: 30.400873\n",
      "Train Epoch: 69 [3840/8000 (48%)]\tLoss: 29.967621\n",
      "Train Epoch: 69 [5120/8000 (64%)]\tLoss: 30.178246\n",
      "Train Epoch: 69 [6400/8000 (80%)]\tLoss: 30.134142\n",
      "Train Epoch: 69 [7680/8000 (96%)]\tLoss: 30.210812\n",
      "====> Epoch: 69 Average loss: 30.1693\n",
      "Train Epoch: 70 [0/8000 (0%)]\tLoss: 30.279976\n",
      "Train Epoch: 70 [1280/8000 (16%)]\tLoss: 30.116262\n",
      "Train Epoch: 70 [2560/8000 (32%)]\tLoss: 30.176453\n",
      "Train Epoch: 70 [3840/8000 (48%)]\tLoss: 30.184044\n",
      "Train Epoch: 70 [5120/8000 (64%)]\tLoss: 29.945986\n",
      "Train Epoch: 70 [6400/8000 (80%)]\tLoss: 30.174707\n",
      "Train Epoch: 70 [7680/8000 (96%)]\tLoss: 30.295769\n",
      "====> Epoch: 70 Average loss: 30.1607\n",
      "Train Epoch: 71 [0/8000 (0%)]\tLoss: 29.921673\n",
      "Train Epoch: 71 [1280/8000 (16%)]\tLoss: 30.260595\n",
      "Train Epoch: 71 [2560/8000 (32%)]\tLoss: 30.083523\n",
      "Train Epoch: 71 [3840/8000 (48%)]\tLoss: 30.254686\n",
      "Train Epoch: 71 [5120/8000 (64%)]\tLoss: 30.116522\n",
      "Train Epoch: 71 [6400/8000 (80%)]\tLoss: 30.416174\n",
      "Train Epoch: 71 [7680/8000 (96%)]\tLoss: 30.118814\n",
      "====> Epoch: 71 Average loss: 30.1505\n",
      "Train Epoch: 72 [0/8000 (0%)]\tLoss: 30.421740\n",
      "Train Epoch: 72 [1280/8000 (16%)]\tLoss: 30.098696\n",
      "Train Epoch: 72 [2560/8000 (32%)]\tLoss: 29.992552\n",
      "Train Epoch: 72 [3840/8000 (48%)]\tLoss: 30.013372\n",
      "Train Epoch: 72 [5120/8000 (64%)]\tLoss: 29.986488\n",
      "Train Epoch: 72 [6400/8000 (80%)]\tLoss: 29.873552\n",
      "Train Epoch: 72 [7680/8000 (96%)]\tLoss: 30.063961\n",
      "====> Epoch: 72 Average loss: 30.1368\n",
      "Train Epoch: 73 [0/8000 (0%)]\tLoss: 30.058718\n",
      "Train Epoch: 73 [1280/8000 (16%)]\tLoss: 30.017214\n",
      "Train Epoch: 73 [2560/8000 (32%)]\tLoss: 30.264076\n",
      "Train Epoch: 73 [3840/8000 (48%)]\tLoss: 29.929758\n",
      "Train Epoch: 73 [5120/8000 (64%)]\tLoss: 30.227594\n",
      "Train Epoch: 73 [6400/8000 (80%)]\tLoss: 30.011129\n",
      "Train Epoch: 73 [7680/8000 (96%)]\tLoss: 30.140614\n",
      "====> Epoch: 73 Average loss: 30.1276\n",
      "Train Epoch: 74 [0/8000 (0%)]\tLoss: 30.124046\n",
      "Train Epoch: 74 [1280/8000 (16%)]\tLoss: 30.412586\n",
      "Train Epoch: 74 [2560/8000 (32%)]\tLoss: 29.782713\n",
      "Train Epoch: 74 [3840/8000 (48%)]\tLoss: 30.135164\n",
      "Train Epoch: 74 [5120/8000 (64%)]\tLoss: 30.113157\n",
      "Train Epoch: 74 [6400/8000 (80%)]\tLoss: 30.248358\n",
      "Train Epoch: 74 [7680/8000 (96%)]\tLoss: 30.255524\n",
      "====> Epoch: 74 Average loss: 30.1094\n",
      "Train Epoch: 75 [0/8000 (0%)]\tLoss: 30.141811\n",
      "Train Epoch: 75 [1280/8000 (16%)]\tLoss: 30.179701\n",
      "Train Epoch: 75 [2560/8000 (32%)]\tLoss: 30.228725\n",
      "Train Epoch: 75 [3840/8000 (48%)]\tLoss: 30.058767\n",
      "Train Epoch: 75 [5120/8000 (64%)]\tLoss: 29.935049\n",
      "Train Epoch: 75 [6400/8000 (80%)]\tLoss: 30.015392\n",
      "Train Epoch: 75 [7680/8000 (96%)]\tLoss: 30.309906\n",
      "====> Epoch: 75 Average loss: 30.1012\n",
      "Train Epoch: 76 [0/8000 (0%)]\tLoss: 30.209398\n",
      "Train Epoch: 76 [1280/8000 (16%)]\tLoss: 30.293364\n",
      "Train Epoch: 76 [2560/8000 (32%)]\tLoss: 30.131609\n",
      "Train Epoch: 76 [3840/8000 (48%)]\tLoss: 29.623291\n",
      "Train Epoch: 76 [5120/8000 (64%)]\tLoss: 30.215757\n",
      "Train Epoch: 76 [6400/8000 (80%)]\tLoss: 30.156631\n",
      "Train Epoch: 76 [7680/8000 (96%)]\tLoss: 30.158367\n",
      "====> Epoch: 76 Average loss: 30.0955\n",
      "Train Epoch: 77 [0/8000 (0%)]\tLoss: 30.166109\n",
      "Train Epoch: 77 [1280/8000 (16%)]\tLoss: 30.220043\n",
      "Train Epoch: 77 [2560/8000 (32%)]\tLoss: 29.912764\n",
      "Train Epoch: 77 [3840/8000 (48%)]\tLoss: 30.111528\n",
      "Train Epoch: 77 [5120/8000 (64%)]\tLoss: 30.073938\n",
      "Train Epoch: 77 [6400/8000 (80%)]\tLoss: 29.941572\n",
      "Train Epoch: 77 [7680/8000 (96%)]\tLoss: 29.966114\n",
      "====> Epoch: 77 Average loss: 30.0804\n",
      "Train Epoch: 78 [0/8000 (0%)]\tLoss: 29.957890\n",
      "Train Epoch: 78 [1280/8000 (16%)]\tLoss: 29.974262\n",
      "Train Epoch: 78 [2560/8000 (32%)]\tLoss: 29.720789\n",
      "Train Epoch: 78 [3840/8000 (48%)]\tLoss: 29.909550\n",
      "Train Epoch: 78 [5120/8000 (64%)]\tLoss: 30.044882\n",
      "Train Epoch: 78 [6400/8000 (80%)]\tLoss: 30.163752\n",
      "Train Epoch: 78 [7680/8000 (96%)]\tLoss: 29.952618\n",
      "====> Epoch: 78 Average loss: 30.0731\n",
      "Train Epoch: 79 [0/8000 (0%)]\tLoss: 29.982147\n",
      "Train Epoch: 79 [1280/8000 (16%)]\tLoss: 30.352442\n",
      "Train Epoch: 79 [2560/8000 (32%)]\tLoss: 30.191196\n",
      "Train Epoch: 79 [3840/8000 (48%)]\tLoss: 30.174503\n",
      "Train Epoch: 79 [5120/8000 (64%)]\tLoss: 30.367804\n",
      "Train Epoch: 79 [6400/8000 (80%)]\tLoss: 30.135395\n",
      "Train Epoch: 79 [7680/8000 (96%)]\tLoss: 29.989948\n",
      "====> Epoch: 79 Average loss: 30.0679\n",
      "Train Epoch: 80 [0/8000 (0%)]\tLoss: 30.202192\n",
      "Train Epoch: 80 [1280/8000 (16%)]\tLoss: 30.183491\n",
      "Train Epoch: 80 [2560/8000 (32%)]\tLoss: 29.989906\n",
      "Train Epoch: 80 [3840/8000 (48%)]\tLoss: 30.076273\n",
      "Train Epoch: 80 [5120/8000 (64%)]\tLoss: 30.042706\n",
      "Train Epoch: 80 [6400/8000 (80%)]\tLoss: 29.974592\n",
      "Train Epoch: 80 [7680/8000 (96%)]\tLoss: 29.728935\n",
      "====> Epoch: 80 Average loss: 30.0649\n",
      "Train Epoch: 81 [0/8000 (0%)]\tLoss: 30.002342\n",
      "Train Epoch: 81 [1280/8000 (16%)]\tLoss: 30.006680\n",
      "Train Epoch: 81 [2560/8000 (32%)]\tLoss: 29.857176\n",
      "Train Epoch: 81 [3840/8000 (48%)]\tLoss: 29.719362\n",
      "Train Epoch: 81 [5120/8000 (64%)]\tLoss: 29.900602\n",
      "Train Epoch: 81 [6400/8000 (80%)]\tLoss: 30.036707\n",
      "Train Epoch: 81 [7680/8000 (96%)]\tLoss: 29.858530\n",
      "====> Epoch: 81 Average loss: 30.0531\n",
      "Train Epoch: 82 [0/8000 (0%)]\tLoss: 30.084713\n",
      "Train Epoch: 82 [1280/8000 (16%)]\tLoss: 29.967966\n",
      "Train Epoch: 82 [2560/8000 (32%)]\tLoss: 30.368826\n",
      "Train Epoch: 82 [3840/8000 (48%)]\tLoss: 30.086582\n",
      "Train Epoch: 82 [5120/8000 (64%)]\tLoss: 30.063917\n",
      "Train Epoch: 82 [6400/8000 (80%)]\tLoss: 30.045374\n",
      "Train Epoch: 82 [7680/8000 (96%)]\tLoss: 29.975695\n",
      "====> Epoch: 82 Average loss: 30.0429\n",
      "Train Epoch: 83 [0/8000 (0%)]\tLoss: 30.035709\n",
      "Train Epoch: 83 [1280/8000 (16%)]\tLoss: 30.000690\n",
      "Train Epoch: 83 [2560/8000 (32%)]\tLoss: 30.077192\n",
      "Train Epoch: 83 [3840/8000 (48%)]\tLoss: 30.239325\n",
      "Train Epoch: 83 [5120/8000 (64%)]\tLoss: 30.026115\n",
      "Train Epoch: 83 [6400/8000 (80%)]\tLoss: 29.824238\n",
      "Train Epoch: 83 [7680/8000 (96%)]\tLoss: 29.790018\n",
      "====> Epoch: 83 Average loss: 30.0466\n",
      "Train Epoch: 84 [0/8000 (0%)]\tLoss: 29.995068\n",
      "Train Epoch: 84 [1280/8000 (16%)]\tLoss: 30.058210\n",
      "Train Epoch: 84 [2560/8000 (32%)]\tLoss: 29.970276\n",
      "Train Epoch: 84 [3840/8000 (48%)]\tLoss: 30.164444\n",
      "Train Epoch: 84 [5120/8000 (64%)]\tLoss: 29.624926\n",
      "Train Epoch: 84 [6400/8000 (80%)]\tLoss: 29.914913\n",
      "Train Epoch: 84 [7680/8000 (96%)]\tLoss: 29.991514\n",
      "====> Epoch: 84 Average loss: 30.0372\n",
      "Train Epoch: 85 [0/8000 (0%)]\tLoss: 29.904448\n",
      "Train Epoch: 85 [1280/8000 (16%)]\tLoss: 29.841591\n",
      "Train Epoch: 85 [2560/8000 (32%)]\tLoss: 30.093979\n",
      "Train Epoch: 85 [3840/8000 (48%)]\tLoss: 30.453419\n",
      "Train Epoch: 85 [5120/8000 (64%)]\tLoss: 30.349188\n",
      "Train Epoch: 85 [6400/8000 (80%)]\tLoss: 30.021008\n",
      "Train Epoch: 85 [7680/8000 (96%)]\tLoss: 30.056911\n",
      "====> Epoch: 85 Average loss: 30.0308\n",
      "Train Epoch: 86 [0/8000 (0%)]\tLoss: 29.942978\n",
      "Train Epoch: 86 [1280/8000 (16%)]\tLoss: 29.988308\n",
      "Train Epoch: 86 [2560/8000 (32%)]\tLoss: 30.093065\n",
      "Train Epoch: 86 [3840/8000 (48%)]\tLoss: 29.916655\n",
      "Train Epoch: 86 [5120/8000 (64%)]\tLoss: 30.337957\n",
      "Train Epoch: 86 [6400/8000 (80%)]\tLoss: 29.817451\n",
      "Train Epoch: 86 [7680/8000 (96%)]\tLoss: 29.960590\n",
      "====> Epoch: 86 Average loss: 30.0240\n",
      "Train Epoch: 87 [0/8000 (0%)]\tLoss: 30.044611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 87 [1280/8000 (16%)]\tLoss: 30.097862\n",
      "Train Epoch: 87 [2560/8000 (32%)]\tLoss: 29.968924\n",
      "Train Epoch: 87 [3840/8000 (48%)]\tLoss: 30.154158\n",
      "Train Epoch: 87 [5120/8000 (64%)]\tLoss: 30.106409\n",
      "Train Epoch: 87 [6400/8000 (80%)]\tLoss: 30.150116\n",
      "Train Epoch: 87 [7680/8000 (96%)]\tLoss: 29.816467\n",
      "====> Epoch: 87 Average loss: 30.0200\n",
      "Train Epoch: 88 [0/8000 (0%)]\tLoss: 29.852245\n",
      "Train Epoch: 88 [1280/8000 (16%)]\tLoss: 29.997995\n",
      "Train Epoch: 88 [2560/8000 (32%)]\tLoss: 30.010605\n",
      "Train Epoch: 88 [3840/8000 (48%)]\tLoss: 29.869608\n",
      "Train Epoch: 88 [5120/8000 (64%)]\tLoss: 29.999699\n",
      "Train Epoch: 88 [6400/8000 (80%)]\tLoss: 29.869789\n",
      "Train Epoch: 88 [7680/8000 (96%)]\tLoss: 29.824991\n",
      "====> Epoch: 88 Average loss: 30.0172\n",
      "Train Epoch: 89 [0/8000 (0%)]\tLoss: 29.926689\n",
      "Train Epoch: 89 [1280/8000 (16%)]\tLoss: 30.235100\n",
      "Train Epoch: 89 [2560/8000 (32%)]\tLoss: 30.369614\n",
      "Train Epoch: 89 [3840/8000 (48%)]\tLoss: 29.906677\n",
      "Train Epoch: 89 [5120/8000 (64%)]\tLoss: 29.949619\n",
      "Train Epoch: 89 [6400/8000 (80%)]\tLoss: 29.808514\n",
      "Train Epoch: 89 [7680/8000 (96%)]\tLoss: 29.906485\n",
      "====> Epoch: 89 Average loss: 30.0103\n",
      "Train Epoch: 90 [0/8000 (0%)]\tLoss: 30.109968\n",
      "Train Epoch: 90 [1280/8000 (16%)]\tLoss: 30.162294\n",
      "Train Epoch: 90 [2560/8000 (32%)]\tLoss: 29.769611\n",
      "Train Epoch: 90 [3840/8000 (48%)]\tLoss: 29.935707\n",
      "Train Epoch: 90 [5120/8000 (64%)]\tLoss: 30.023642\n",
      "Train Epoch: 90 [6400/8000 (80%)]\tLoss: 30.032654\n",
      "Train Epoch: 90 [7680/8000 (96%)]\tLoss: 30.130028\n",
      "====> Epoch: 90 Average loss: 30.0053\n",
      "Train Epoch: 91 [0/8000 (0%)]\tLoss: 29.914696\n",
      "Train Epoch: 91 [1280/8000 (16%)]\tLoss: 29.979521\n",
      "Train Epoch: 91 [2560/8000 (32%)]\tLoss: 30.099691\n",
      "Train Epoch: 91 [3840/8000 (48%)]\tLoss: 30.121500\n",
      "Train Epoch: 91 [5120/8000 (64%)]\tLoss: 29.930571\n",
      "Train Epoch: 91 [6400/8000 (80%)]\tLoss: 29.723473\n",
      "Train Epoch: 91 [7680/8000 (96%)]\tLoss: 30.117933\n",
      "====> Epoch: 91 Average loss: 29.9967\n",
      "Train Epoch: 92 [0/8000 (0%)]\tLoss: 30.200615\n",
      "Train Epoch: 92 [1280/8000 (16%)]\tLoss: 29.947651\n",
      "Train Epoch: 92 [2560/8000 (32%)]\tLoss: 30.220259\n",
      "Train Epoch: 92 [3840/8000 (48%)]\tLoss: 30.005402\n",
      "Train Epoch: 92 [5120/8000 (64%)]\tLoss: 29.757488\n",
      "Train Epoch: 92 [6400/8000 (80%)]\tLoss: 30.277145\n",
      "Train Epoch: 92 [7680/8000 (96%)]\tLoss: 30.191216\n",
      "====> Epoch: 92 Average loss: 29.9965\n",
      "Train Epoch: 93 [0/8000 (0%)]\tLoss: 30.074963\n",
      "Train Epoch: 93 [1280/8000 (16%)]\tLoss: 29.929771\n",
      "Train Epoch: 93 [2560/8000 (32%)]\tLoss: 29.994852\n",
      "Train Epoch: 93 [3840/8000 (48%)]\tLoss: 29.939169\n",
      "Train Epoch: 93 [5120/8000 (64%)]\tLoss: 29.929911\n",
      "Train Epoch: 93 [6400/8000 (80%)]\tLoss: 30.193544\n",
      "Train Epoch: 93 [7680/8000 (96%)]\tLoss: 29.912567\n",
      "====> Epoch: 93 Average loss: 29.9994\n",
      "Train Epoch: 94 [0/8000 (0%)]\tLoss: 30.052355\n",
      "Train Epoch: 94 [1280/8000 (16%)]\tLoss: 29.806667\n",
      "Train Epoch: 94 [2560/8000 (32%)]\tLoss: 29.785311\n",
      "Train Epoch: 94 [3840/8000 (48%)]\tLoss: 30.054569\n",
      "Train Epoch: 94 [5120/8000 (64%)]\tLoss: 29.933706\n",
      "Train Epoch: 94 [6400/8000 (80%)]\tLoss: 30.008343\n",
      "Train Epoch: 94 [7680/8000 (96%)]\tLoss: 30.040440\n",
      "====> Epoch: 94 Average loss: 29.9848\n",
      "Train Epoch: 95 [0/8000 (0%)]\tLoss: 30.140556\n",
      "Train Epoch: 95 [1280/8000 (16%)]\tLoss: 29.902533\n",
      "Train Epoch: 95 [2560/8000 (32%)]\tLoss: 30.214724\n",
      "Train Epoch: 95 [3840/8000 (48%)]\tLoss: 30.025932\n",
      "Train Epoch: 95 [5120/8000 (64%)]\tLoss: 30.183258\n",
      "Train Epoch: 95 [6400/8000 (80%)]\tLoss: 29.783688\n",
      "Train Epoch: 95 [7680/8000 (96%)]\tLoss: 29.941996\n",
      "====> Epoch: 95 Average loss: 29.9826\n",
      "Train Epoch: 96 [0/8000 (0%)]\tLoss: 30.298872\n",
      "Train Epoch: 96 [1280/8000 (16%)]\tLoss: 29.694250\n",
      "Train Epoch: 96 [2560/8000 (32%)]\tLoss: 29.965532\n",
      "Train Epoch: 96 [3840/8000 (48%)]\tLoss: 30.065819\n",
      "Train Epoch: 96 [5120/8000 (64%)]\tLoss: 29.748760\n",
      "Train Epoch: 96 [6400/8000 (80%)]\tLoss: 29.934717\n",
      "Train Epoch: 96 [7680/8000 (96%)]\tLoss: 29.960487\n",
      "====> Epoch: 96 Average loss: 29.9717\n",
      "Train Epoch: 97 [0/8000 (0%)]\tLoss: 30.062946\n",
      "Train Epoch: 97 [1280/8000 (16%)]\tLoss: 29.958122\n",
      "Train Epoch: 97 [2560/8000 (32%)]\tLoss: 30.020901\n",
      "Train Epoch: 97 [3840/8000 (48%)]\tLoss: 30.231153\n",
      "Train Epoch: 97 [5120/8000 (64%)]\tLoss: 29.587437\n",
      "Train Epoch: 97 [6400/8000 (80%)]\tLoss: 30.149445\n",
      "Train Epoch: 97 [7680/8000 (96%)]\tLoss: 30.042490\n",
      "====> Epoch: 97 Average loss: 29.9671\n",
      "Train Epoch: 98 [0/8000 (0%)]\tLoss: 30.034225\n",
      "Train Epoch: 98 [1280/8000 (16%)]\tLoss: 29.740261\n",
      "Train Epoch: 98 [2560/8000 (32%)]\tLoss: 29.964020\n",
      "Train Epoch: 98 [3840/8000 (48%)]\tLoss: 29.823179\n",
      "Train Epoch: 98 [5120/8000 (64%)]\tLoss: 30.056332\n",
      "Train Epoch: 98 [6400/8000 (80%)]\tLoss: 29.799227\n",
      "Train Epoch: 98 [7680/8000 (96%)]\tLoss: 30.088343\n",
      "====> Epoch: 98 Average loss: 29.9638\n",
      "Train Epoch: 99 [0/8000 (0%)]\tLoss: 30.106258\n",
      "Train Epoch: 99 [1280/8000 (16%)]\tLoss: 30.020638\n",
      "Train Epoch: 99 [2560/8000 (32%)]\tLoss: 29.956064\n",
      "Train Epoch: 99 [3840/8000 (48%)]\tLoss: 30.060993\n",
      "Train Epoch: 99 [5120/8000 (64%)]\tLoss: 30.111561\n",
      "Train Epoch: 99 [6400/8000 (80%)]\tLoss: 29.947626\n",
      "Train Epoch: 99 [7680/8000 (96%)]\tLoss: 30.231735\n",
      "====> Epoch: 99 Average loss: 29.9608\n",
      "Train Epoch: 100 [0/8000 (0%)]\tLoss: 29.958807\n",
      "Train Epoch: 100 [1280/8000 (16%)]\tLoss: 30.017883\n",
      "Train Epoch: 100 [2560/8000 (32%)]\tLoss: 30.298914\n",
      "Train Epoch: 100 [3840/8000 (48%)]\tLoss: 30.024645\n",
      "Train Epoch: 100 [5120/8000 (64%)]\tLoss: 30.129789\n",
      "Train Epoch: 100 [6400/8000 (80%)]\tLoss: 29.920845\n",
      "Train Epoch: 100 [7680/8000 (96%)]\tLoss: 29.790779\n",
      "====> Epoch: 100 Average loss: 29.9546\n",
      "Train Epoch: 101 [0/8000 (0%)]\tLoss: 29.838818\n",
      "Train Epoch: 101 [1280/8000 (16%)]\tLoss: 30.189434\n",
      "Train Epoch: 101 [2560/8000 (32%)]\tLoss: 30.204927\n",
      "Train Epoch: 101 [3840/8000 (48%)]\tLoss: 29.946196\n",
      "Train Epoch: 101 [5120/8000 (64%)]\tLoss: 29.941044\n",
      "Train Epoch: 101 [6400/8000 (80%)]\tLoss: 29.938637\n",
      "Train Epoch: 101 [7680/8000 (96%)]\tLoss: 29.751251\n",
      "====> Epoch: 101 Average loss: 29.9448\n",
      "Train Epoch: 102 [0/8000 (0%)]\tLoss: 30.350973\n",
      "Train Epoch: 102 [1280/8000 (16%)]\tLoss: 29.665369\n",
      "Train Epoch: 102 [2560/8000 (32%)]\tLoss: 30.152107\n",
      "Train Epoch: 102 [3840/8000 (48%)]\tLoss: 30.052135\n",
      "Train Epoch: 102 [5120/8000 (64%)]\tLoss: 29.927994\n",
      "Train Epoch: 102 [6400/8000 (80%)]\tLoss: 30.150618\n",
      "Train Epoch: 102 [7680/8000 (96%)]\tLoss: 30.021999\n",
      "====> Epoch: 102 Average loss: 29.9455\n",
      "Train Epoch: 103 [0/8000 (0%)]\tLoss: 29.724972\n",
      "Train Epoch: 103 [1280/8000 (16%)]\tLoss: 29.865215\n",
      "Train Epoch: 103 [2560/8000 (32%)]\tLoss: 29.999165\n",
      "Train Epoch: 103 [3840/8000 (48%)]\tLoss: 30.209759\n",
      "Train Epoch: 103 [5120/8000 (64%)]\tLoss: 29.806154\n",
      "Train Epoch: 103 [6400/8000 (80%)]\tLoss: 29.672392\n",
      "Train Epoch: 103 [7680/8000 (96%)]\tLoss: 29.672417\n",
      "====> Epoch: 103 Average loss: 29.9392\n",
      "Train Epoch: 104 [0/8000 (0%)]\tLoss: 29.758057\n",
      "Train Epoch: 104 [1280/8000 (16%)]\tLoss: 30.257183\n",
      "Train Epoch: 104 [2560/8000 (32%)]\tLoss: 29.716425\n",
      "Train Epoch: 104 [3840/8000 (48%)]\tLoss: 29.740803\n",
      "Train Epoch: 104 [5120/8000 (64%)]\tLoss: 29.953035\n",
      "Train Epoch: 104 [6400/8000 (80%)]\tLoss: 30.012146\n",
      "Train Epoch: 104 [7680/8000 (96%)]\tLoss: 30.163233\n",
      "====> Epoch: 104 Average loss: 29.9327\n",
      "Train Epoch: 105 [0/8000 (0%)]\tLoss: 29.943264\n",
      "Train Epoch: 105 [1280/8000 (16%)]\tLoss: 30.385109\n",
      "Train Epoch: 105 [2560/8000 (32%)]\tLoss: 29.974997\n",
      "Train Epoch: 105 [3840/8000 (48%)]\tLoss: 29.851671\n",
      "Train Epoch: 105 [5120/8000 (64%)]\tLoss: 29.889227\n",
      "Train Epoch: 105 [6400/8000 (80%)]\tLoss: 29.733400\n",
      "Train Epoch: 105 [7680/8000 (96%)]\tLoss: 29.709665\n",
      "====> Epoch: 105 Average loss: 29.9291\n",
      "Train Epoch: 106 [0/8000 (0%)]\tLoss: 29.738449\n",
      "Train Epoch: 106 [1280/8000 (16%)]\tLoss: 29.792459\n",
      "Train Epoch: 106 [2560/8000 (32%)]\tLoss: 29.852356\n",
      "Train Epoch: 106 [3840/8000 (48%)]\tLoss: 29.644211\n",
      "Train Epoch: 106 [5120/8000 (64%)]\tLoss: 29.790058\n",
      "Train Epoch: 106 [6400/8000 (80%)]\tLoss: 29.999905\n",
      "Train Epoch: 106 [7680/8000 (96%)]\tLoss: 29.856133\n",
      "====> Epoch: 106 Average loss: 29.9274\n",
      "Train Epoch: 107 [0/8000 (0%)]\tLoss: 29.717508\n",
      "Train Epoch: 107 [1280/8000 (16%)]\tLoss: 30.034609\n",
      "Train Epoch: 107 [2560/8000 (32%)]\tLoss: 29.865925\n",
      "Train Epoch: 107 [3840/8000 (48%)]\tLoss: 29.956871\n",
      "Train Epoch: 107 [5120/8000 (64%)]\tLoss: 29.890127\n",
      "Train Epoch: 107 [6400/8000 (80%)]\tLoss: 29.957552\n",
      "Train Epoch: 107 [7680/8000 (96%)]\tLoss: 29.885820\n",
      "====> Epoch: 107 Average loss: 29.9179\n",
      "Train Epoch: 108 [0/8000 (0%)]\tLoss: 29.686501\n",
      "Train Epoch: 108 [1280/8000 (16%)]\tLoss: 30.128586\n",
      "Train Epoch: 108 [2560/8000 (32%)]\tLoss: 30.097183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 108 [3840/8000 (48%)]\tLoss: 30.111988\n",
      "Train Epoch: 108 [5120/8000 (64%)]\tLoss: 29.967243\n",
      "Train Epoch: 108 [6400/8000 (80%)]\tLoss: 29.746244\n",
      "Train Epoch: 108 [7680/8000 (96%)]\tLoss: 30.104080\n",
      "====> Epoch: 108 Average loss: 29.9219\n",
      "Train Epoch: 109 [0/8000 (0%)]\tLoss: 29.962467\n",
      "Train Epoch: 109 [1280/8000 (16%)]\tLoss: 30.001184\n",
      "Train Epoch: 109 [2560/8000 (32%)]\tLoss: 30.052572\n",
      "Train Epoch: 109 [3840/8000 (48%)]\tLoss: 29.810211\n",
      "Train Epoch: 109 [5120/8000 (64%)]\tLoss: 29.958689\n",
      "Train Epoch: 109 [6400/8000 (80%)]\tLoss: 30.108978\n",
      "Train Epoch: 109 [7680/8000 (96%)]\tLoss: 29.713139\n",
      "====> Epoch: 109 Average loss: 29.9106\n",
      "Train Epoch: 110 [0/8000 (0%)]\tLoss: 30.066280\n",
      "Train Epoch: 110 [1280/8000 (16%)]\tLoss: 29.868971\n",
      "Train Epoch: 110 [2560/8000 (32%)]\tLoss: 29.816673\n",
      "Train Epoch: 110 [3840/8000 (48%)]\tLoss: 30.107143\n",
      "Train Epoch: 110 [5120/8000 (64%)]\tLoss: 29.900051\n",
      "Train Epoch: 110 [6400/8000 (80%)]\tLoss: 30.089832\n",
      "Train Epoch: 110 [7680/8000 (96%)]\tLoss: 29.938700\n",
      "====> Epoch: 110 Average loss: 29.9080\n",
      "Train Epoch: 111 [0/8000 (0%)]\tLoss: 29.920137\n",
      "Train Epoch: 111 [1280/8000 (16%)]\tLoss: 29.929665\n",
      "Train Epoch: 111 [2560/8000 (32%)]\tLoss: 29.932014\n",
      "Train Epoch: 111 [3840/8000 (48%)]\tLoss: 29.831949\n",
      "Train Epoch: 111 [5120/8000 (64%)]\tLoss: 30.026577\n",
      "Train Epoch: 111 [6400/8000 (80%)]\tLoss: 30.287323\n",
      "Train Epoch: 111 [7680/8000 (96%)]\tLoss: 29.914814\n",
      "====> Epoch: 111 Average loss: 29.9080\n",
      "Train Epoch: 112 [0/8000 (0%)]\tLoss: 29.817858\n",
      "Train Epoch: 112 [1280/8000 (16%)]\tLoss: 30.047163\n",
      "Train Epoch: 112 [2560/8000 (32%)]\tLoss: 30.088127\n",
      "Train Epoch: 112 [3840/8000 (48%)]\tLoss: 29.636894\n",
      "Train Epoch: 112 [5120/8000 (64%)]\tLoss: 30.052811\n",
      "Train Epoch: 112 [6400/8000 (80%)]\tLoss: 29.997713\n",
      "Train Epoch: 112 [7680/8000 (96%)]\tLoss: 29.841845\n",
      "====> Epoch: 112 Average loss: 29.9010\n",
      "Train Epoch: 113 [0/8000 (0%)]\tLoss: 29.978510\n",
      "Train Epoch: 113 [1280/8000 (16%)]\tLoss: 29.856339\n",
      "Train Epoch: 113 [2560/8000 (32%)]\tLoss: 29.617325\n",
      "Train Epoch: 113 [3840/8000 (48%)]\tLoss: 29.938677\n",
      "Train Epoch: 113 [5120/8000 (64%)]\tLoss: 29.876879\n",
      "Train Epoch: 113 [6400/8000 (80%)]\tLoss: 30.041355\n",
      "Train Epoch: 113 [7680/8000 (96%)]\tLoss: 30.004189\n",
      "====> Epoch: 113 Average loss: 29.9059\n",
      "Train Epoch: 114 [0/8000 (0%)]\tLoss: 29.787760\n",
      "Train Epoch: 114 [1280/8000 (16%)]\tLoss: 30.028084\n",
      "Train Epoch: 114 [2560/8000 (32%)]\tLoss: 30.084166\n",
      "Train Epoch: 114 [3840/8000 (48%)]\tLoss: 29.891430\n",
      "Train Epoch: 114 [5120/8000 (64%)]\tLoss: 29.962406\n",
      "Train Epoch: 114 [6400/8000 (80%)]\tLoss: 29.970909\n",
      "Train Epoch: 114 [7680/8000 (96%)]\tLoss: 29.703074\n",
      "====> Epoch: 114 Average loss: 29.9034\n",
      "Train Epoch: 115 [0/8000 (0%)]\tLoss: 29.839989\n",
      "Train Epoch: 115 [1280/8000 (16%)]\tLoss: 29.854628\n",
      "Train Epoch: 115 [2560/8000 (32%)]\tLoss: 29.985384\n",
      "Train Epoch: 115 [3840/8000 (48%)]\tLoss: 29.581287\n",
      "Train Epoch: 115 [5120/8000 (64%)]\tLoss: 29.980976\n",
      "Train Epoch: 115 [6400/8000 (80%)]\tLoss: 29.582155\n",
      "Train Epoch: 115 [7680/8000 (96%)]\tLoss: 29.896015\n",
      "====> Epoch: 115 Average loss: 29.8945\n",
      "Train Epoch: 116 [0/8000 (0%)]\tLoss: 29.826517\n",
      "Train Epoch: 116 [1280/8000 (16%)]\tLoss: 29.896797\n",
      "Train Epoch: 116 [2560/8000 (32%)]\tLoss: 30.064192\n",
      "Train Epoch: 116 [3840/8000 (48%)]\tLoss: 29.806463\n",
      "Train Epoch: 116 [5120/8000 (64%)]\tLoss: 29.739443\n",
      "Train Epoch: 116 [6400/8000 (80%)]\tLoss: 30.152021\n",
      "Train Epoch: 116 [7680/8000 (96%)]\tLoss: 29.968285\n",
      "====> Epoch: 116 Average loss: 29.8949\n",
      "Train Epoch: 117 [0/8000 (0%)]\tLoss: 29.831532\n",
      "Train Epoch: 117 [1280/8000 (16%)]\tLoss: 30.069740\n",
      "Train Epoch: 117 [2560/8000 (32%)]\tLoss: 29.567900\n",
      "Train Epoch: 117 [3840/8000 (48%)]\tLoss: 29.889977\n",
      "Train Epoch: 117 [5120/8000 (64%)]\tLoss: 29.897478\n",
      "Train Epoch: 117 [6400/8000 (80%)]\tLoss: 29.832373\n",
      "Train Epoch: 117 [7680/8000 (96%)]\tLoss: 29.894466\n",
      "====> Epoch: 117 Average loss: 29.8930\n",
      "Train Epoch: 118 [0/8000 (0%)]\tLoss: 29.769409\n",
      "Train Epoch: 118 [1280/8000 (16%)]\tLoss: 30.242153\n",
      "Train Epoch: 118 [2560/8000 (32%)]\tLoss: 29.969234\n",
      "Train Epoch: 118 [3840/8000 (48%)]\tLoss: 29.793608\n",
      "Train Epoch: 118 [5120/8000 (64%)]\tLoss: 29.941713\n",
      "Train Epoch: 118 [6400/8000 (80%)]\tLoss: 29.789333\n",
      "Train Epoch: 118 [7680/8000 (96%)]\tLoss: 30.096235\n",
      "====> Epoch: 118 Average loss: 29.8893\n",
      "Train Epoch: 119 [0/8000 (0%)]\tLoss: 29.871468\n",
      "Train Epoch: 119 [1280/8000 (16%)]\tLoss: 30.048027\n",
      "Train Epoch: 119 [2560/8000 (32%)]\tLoss: 30.125208\n",
      "Train Epoch: 119 [3840/8000 (48%)]\tLoss: 30.008835\n",
      "Train Epoch: 119 [5120/8000 (64%)]\tLoss: 29.846613\n",
      "Train Epoch: 119 [6400/8000 (80%)]\tLoss: 29.883200\n",
      "Train Epoch: 119 [7680/8000 (96%)]\tLoss: 29.876579\n",
      "====> Epoch: 119 Average loss: 29.8883\n",
      "Train Epoch: 120 [0/8000 (0%)]\tLoss: 29.913906\n",
      "Train Epoch: 120 [1280/8000 (16%)]\tLoss: 29.932264\n",
      "Train Epoch: 120 [2560/8000 (32%)]\tLoss: 29.699242\n",
      "Train Epoch: 120 [3840/8000 (48%)]\tLoss: 29.642782\n",
      "Train Epoch: 120 [5120/8000 (64%)]\tLoss: 29.885323\n",
      "Train Epoch: 120 [6400/8000 (80%)]\tLoss: 29.728153\n",
      "Train Epoch: 120 [7680/8000 (96%)]\tLoss: 30.050636\n",
      "====> Epoch: 120 Average loss: 29.8885\n",
      "Train Epoch: 121 [0/8000 (0%)]\tLoss: 29.686760\n",
      "Train Epoch: 121 [1280/8000 (16%)]\tLoss: 29.969219\n",
      "Train Epoch: 121 [2560/8000 (32%)]\tLoss: 30.019571\n",
      "Train Epoch: 121 [3840/8000 (48%)]\tLoss: 29.596767\n",
      "Train Epoch: 121 [5120/8000 (64%)]\tLoss: 29.645323\n",
      "Train Epoch: 121 [6400/8000 (80%)]\tLoss: 29.844263\n",
      "Train Epoch: 121 [7680/8000 (96%)]\tLoss: 29.621479\n",
      "====> Epoch: 121 Average loss: 29.8894\n",
      "Train Epoch: 122 [0/8000 (0%)]\tLoss: 29.794149\n",
      "Train Epoch: 122 [1280/8000 (16%)]\tLoss: 29.661392\n",
      "Train Epoch: 122 [2560/8000 (32%)]\tLoss: 29.635944\n",
      "Train Epoch: 122 [3840/8000 (48%)]\tLoss: 29.846384\n",
      "Train Epoch: 122 [5120/8000 (64%)]\tLoss: 30.138199\n",
      "Train Epoch: 122 [6400/8000 (80%)]\tLoss: 29.899151\n",
      "Train Epoch: 122 [7680/8000 (96%)]\tLoss: 29.958227\n",
      "====> Epoch: 122 Average loss: 29.8825\n",
      "Train Epoch: 123 [0/8000 (0%)]\tLoss: 29.749016\n",
      "Train Epoch: 123 [1280/8000 (16%)]\tLoss: 29.983316\n",
      "Train Epoch: 123 [2560/8000 (32%)]\tLoss: 29.735044\n",
      "Train Epoch: 123 [3840/8000 (48%)]\tLoss: 29.849195\n",
      "Train Epoch: 123 [5120/8000 (64%)]\tLoss: 29.691303\n",
      "Train Epoch: 123 [6400/8000 (80%)]\tLoss: 29.865133\n",
      "Train Epoch: 123 [7680/8000 (96%)]\tLoss: 29.967119\n",
      "====> Epoch: 123 Average loss: 29.8839\n",
      "Train Epoch: 124 [0/8000 (0%)]\tLoss: 29.739613\n",
      "Train Epoch: 124 [1280/8000 (16%)]\tLoss: 29.694904\n",
      "Train Epoch: 124 [2560/8000 (32%)]\tLoss: 30.240561\n",
      "Train Epoch: 124 [3840/8000 (48%)]\tLoss: 30.093941\n",
      "Train Epoch: 124 [5120/8000 (64%)]\tLoss: 29.862045\n",
      "Train Epoch: 124 [6400/8000 (80%)]\tLoss: 29.798721\n",
      "Train Epoch: 124 [7680/8000 (96%)]\tLoss: 29.912470\n",
      "====> Epoch: 124 Average loss: 29.8798\n",
      "Train Epoch: 125 [0/8000 (0%)]\tLoss: 29.842344\n",
      "Train Epoch: 125 [1280/8000 (16%)]\tLoss: 29.962015\n",
      "Train Epoch: 125 [2560/8000 (32%)]\tLoss: 29.626688\n",
      "Train Epoch: 125 [3840/8000 (48%)]\tLoss: 29.801004\n",
      "Train Epoch: 125 [5120/8000 (64%)]\tLoss: 29.915876\n",
      "Train Epoch: 125 [6400/8000 (80%)]\tLoss: 29.740097\n",
      "Train Epoch: 125 [7680/8000 (96%)]\tLoss: 30.161596\n",
      "====> Epoch: 125 Average loss: 29.8785\n",
      "Train Epoch: 126 [0/8000 (0%)]\tLoss: 29.760221\n",
      "Train Epoch: 126 [1280/8000 (16%)]\tLoss: 29.948631\n",
      "Train Epoch: 126 [2560/8000 (32%)]\tLoss: 29.874584\n",
      "Train Epoch: 126 [3840/8000 (48%)]\tLoss: 29.770395\n",
      "Train Epoch: 126 [5120/8000 (64%)]\tLoss: 29.900822\n",
      "Train Epoch: 126 [6400/8000 (80%)]\tLoss: 29.873283\n",
      "Train Epoch: 126 [7680/8000 (96%)]\tLoss: 29.922421\n",
      "====> Epoch: 126 Average loss: 29.8789\n",
      "Train Epoch: 127 [0/8000 (0%)]\tLoss: 29.713772\n",
      "Train Epoch: 127 [1280/8000 (16%)]\tLoss: 29.897810\n",
      "Train Epoch: 127 [2560/8000 (32%)]\tLoss: 29.839045\n",
      "Train Epoch: 127 [3840/8000 (48%)]\tLoss: 29.932392\n",
      "Train Epoch: 127 [5120/8000 (64%)]\tLoss: 29.650143\n",
      "Train Epoch: 127 [6400/8000 (80%)]\tLoss: 29.936249\n",
      "Train Epoch: 127 [7680/8000 (96%)]\tLoss: 30.235945\n",
      "====> Epoch: 127 Average loss: 29.8781\n",
      "Train Epoch: 128 [0/8000 (0%)]\tLoss: 30.061958\n",
      "Train Epoch: 128 [1280/8000 (16%)]\tLoss: 30.167688\n",
      "Train Epoch: 128 [2560/8000 (32%)]\tLoss: 30.062439\n",
      "Train Epoch: 128 [3840/8000 (48%)]\tLoss: 30.091311\n",
      "Train Epoch: 128 [5120/8000 (64%)]\tLoss: 29.946909\n",
      "Train Epoch: 128 [6400/8000 (80%)]\tLoss: 29.777246\n",
      "Train Epoch: 128 [7680/8000 (96%)]\tLoss: 29.829885\n",
      "====> Epoch: 128 Average loss: 29.8803\n",
      "Train Epoch: 129 [0/8000 (0%)]\tLoss: 29.827454\n",
      "Train Epoch: 129 [1280/8000 (16%)]\tLoss: 29.912117\n",
      "Train Epoch: 129 [2560/8000 (32%)]\tLoss: 29.889452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 129 [3840/8000 (48%)]\tLoss: 29.881001\n",
      "Train Epoch: 129 [5120/8000 (64%)]\tLoss: 29.819818\n",
      "Train Epoch: 129 [6400/8000 (80%)]\tLoss: 30.101986\n",
      "Train Epoch: 129 [7680/8000 (96%)]\tLoss: 29.729719\n",
      "====> Epoch: 129 Average loss: 29.8715\n",
      "Train Epoch: 130 [0/8000 (0%)]\tLoss: 30.021824\n",
      "Train Epoch: 130 [1280/8000 (16%)]\tLoss: 30.123554\n",
      "Train Epoch: 130 [2560/8000 (32%)]\tLoss: 29.801779\n",
      "Train Epoch: 130 [3840/8000 (48%)]\tLoss: 30.012585\n",
      "Train Epoch: 130 [5120/8000 (64%)]\tLoss: 30.269943\n",
      "Train Epoch: 130 [6400/8000 (80%)]\tLoss: 29.789417\n",
      "Train Epoch: 130 [7680/8000 (96%)]\tLoss: 30.012442\n",
      "====> Epoch: 130 Average loss: 29.8695\n",
      "Train Epoch: 131 [0/8000 (0%)]\tLoss: 29.933905\n",
      "Train Epoch: 131 [1280/8000 (16%)]\tLoss: 29.873446\n",
      "Train Epoch: 131 [2560/8000 (32%)]\tLoss: 29.807938\n",
      "Train Epoch: 131 [3840/8000 (48%)]\tLoss: 29.885004\n",
      "Train Epoch: 131 [5120/8000 (64%)]\tLoss: 29.556086\n",
      "Train Epoch: 131 [6400/8000 (80%)]\tLoss: 29.942333\n",
      "Train Epoch: 131 [7680/8000 (96%)]\tLoss: 30.014631\n",
      "====> Epoch: 131 Average loss: 29.8694\n",
      "Train Epoch: 132 [0/8000 (0%)]\tLoss: 30.245939\n",
      "Train Epoch: 132 [1280/8000 (16%)]\tLoss: 29.782707\n",
      "Train Epoch: 132 [2560/8000 (32%)]\tLoss: 29.999941\n",
      "Train Epoch: 132 [3840/8000 (48%)]\tLoss: 29.890764\n",
      "Train Epoch: 132 [5120/8000 (64%)]\tLoss: 29.844576\n",
      "Train Epoch: 132 [6400/8000 (80%)]\tLoss: 29.993246\n",
      "Train Epoch: 132 [7680/8000 (96%)]\tLoss: 29.781425\n",
      "====> Epoch: 132 Average loss: 29.8706\n",
      "Train Epoch: 133 [0/8000 (0%)]\tLoss: 30.044937\n",
      "Train Epoch: 133 [1280/8000 (16%)]\tLoss: 30.358276\n",
      "Train Epoch: 133 [2560/8000 (32%)]\tLoss: 29.659790\n",
      "Train Epoch: 133 [3840/8000 (48%)]\tLoss: 30.177849\n",
      "Train Epoch: 133 [5120/8000 (64%)]\tLoss: 30.016420\n",
      "Train Epoch: 133 [6400/8000 (80%)]\tLoss: 29.748363\n",
      "Train Epoch: 133 [7680/8000 (96%)]\tLoss: 29.792511\n",
      "====> Epoch: 133 Average loss: 29.8644\n",
      "Train Epoch: 134 [0/8000 (0%)]\tLoss: 29.782888\n",
      "Train Epoch: 134 [1280/8000 (16%)]\tLoss: 29.938463\n",
      "Train Epoch: 134 [2560/8000 (32%)]\tLoss: 29.769232\n",
      "Train Epoch: 134 [3840/8000 (48%)]\tLoss: 29.878296\n",
      "Train Epoch: 134 [5120/8000 (64%)]\tLoss: 29.688690\n",
      "Train Epoch: 134 [6400/8000 (80%)]\tLoss: 29.991718\n",
      "Train Epoch: 134 [7680/8000 (96%)]\tLoss: 29.778639\n",
      "====> Epoch: 134 Average loss: 29.8663\n",
      "Train Epoch: 135 [0/8000 (0%)]\tLoss: 29.982182\n",
      "Train Epoch: 135 [1280/8000 (16%)]\tLoss: 29.770262\n",
      "Train Epoch: 135 [2560/8000 (32%)]\tLoss: 29.823807\n",
      "Train Epoch: 135 [3840/8000 (48%)]\tLoss: 29.783321\n",
      "Train Epoch: 135 [5120/8000 (64%)]\tLoss: 29.739820\n",
      "Train Epoch: 135 [6400/8000 (80%)]\tLoss: 30.204901\n",
      "Train Epoch: 135 [7680/8000 (96%)]\tLoss: 29.962831\n",
      "====> Epoch: 135 Average loss: 29.8636\n",
      "Train Epoch: 136 [0/8000 (0%)]\tLoss: 29.779991\n",
      "Train Epoch: 136 [1280/8000 (16%)]\tLoss: 29.750257\n",
      "Train Epoch: 136 [2560/8000 (32%)]\tLoss: 29.845234\n",
      "Train Epoch: 136 [3840/8000 (48%)]\tLoss: 29.881338\n",
      "Train Epoch: 136 [5120/8000 (64%)]\tLoss: 30.098856\n",
      "Train Epoch: 136 [6400/8000 (80%)]\tLoss: 29.985298\n",
      "Train Epoch: 136 [7680/8000 (96%)]\tLoss: 29.679827\n",
      "====> Epoch: 136 Average loss: 29.8602\n",
      "Train Epoch: 137 [0/8000 (0%)]\tLoss: 29.894316\n",
      "Train Epoch: 137 [1280/8000 (16%)]\tLoss: 30.194750\n",
      "Train Epoch: 137 [2560/8000 (32%)]\tLoss: 30.013922\n",
      "Train Epoch: 137 [3840/8000 (48%)]\tLoss: 29.859667\n",
      "Train Epoch: 137 [5120/8000 (64%)]\tLoss: 29.829529\n",
      "Train Epoch: 137 [6400/8000 (80%)]\tLoss: 30.209406\n",
      "Train Epoch: 137 [7680/8000 (96%)]\tLoss: 30.002071\n",
      "====> Epoch: 137 Average loss: 29.8580\n",
      "Train Epoch: 138 [0/8000 (0%)]\tLoss: 29.885708\n",
      "Train Epoch: 138 [1280/8000 (16%)]\tLoss: 29.915979\n",
      "Train Epoch: 138 [2560/8000 (32%)]\tLoss: 29.794912\n",
      "Train Epoch: 138 [3840/8000 (48%)]\tLoss: 30.014420\n",
      "Train Epoch: 138 [5120/8000 (64%)]\tLoss: 29.953720\n",
      "Train Epoch: 138 [6400/8000 (80%)]\tLoss: 29.823807\n",
      "Train Epoch: 138 [7680/8000 (96%)]\tLoss: 29.935787\n",
      "====> Epoch: 138 Average loss: 29.8565\n",
      "Train Epoch: 139 [0/8000 (0%)]\tLoss: 29.778980\n",
      "Train Epoch: 139 [1280/8000 (16%)]\tLoss: 29.886620\n",
      "Train Epoch: 139 [2560/8000 (32%)]\tLoss: 29.692329\n",
      "Train Epoch: 139 [3840/8000 (48%)]\tLoss: 29.839222\n",
      "Train Epoch: 139 [5120/8000 (64%)]\tLoss: 29.809847\n",
      "Train Epoch: 139 [6400/8000 (80%)]\tLoss: 29.706118\n",
      "Train Epoch: 139 [7680/8000 (96%)]\tLoss: 29.924004\n",
      "====> Epoch: 139 Average loss: 29.8588\n",
      "Train Epoch: 140 [0/8000 (0%)]\tLoss: 29.889442\n",
      "Train Epoch: 140 [1280/8000 (16%)]\tLoss: 29.586288\n",
      "Train Epoch: 140 [2560/8000 (32%)]\tLoss: 29.851444\n",
      "Train Epoch: 140 [3840/8000 (48%)]\tLoss: 30.173504\n",
      "Train Epoch: 140 [5120/8000 (64%)]\tLoss: 29.862020\n",
      "Train Epoch: 140 [6400/8000 (80%)]\tLoss: 29.916931\n",
      "Train Epoch: 140 [7680/8000 (96%)]\tLoss: 29.812744\n",
      "====> Epoch: 140 Average loss: 29.8606\n",
      "Train Epoch: 141 [0/8000 (0%)]\tLoss: 29.823282\n",
      "Train Epoch: 141 [1280/8000 (16%)]\tLoss: 29.605522\n",
      "Train Epoch: 141 [2560/8000 (32%)]\tLoss: 29.710016\n",
      "Train Epoch: 141 [3840/8000 (48%)]\tLoss: 30.125185\n",
      "Train Epoch: 141 [5120/8000 (64%)]\tLoss: 29.899096\n",
      "Train Epoch: 141 [6400/8000 (80%)]\tLoss: 29.542456\n",
      "Train Epoch: 141 [7680/8000 (96%)]\tLoss: 29.745991\n",
      "====> Epoch: 141 Average loss: 29.8576\n",
      "Train Epoch: 142 [0/8000 (0%)]\tLoss: 29.975731\n",
      "Train Epoch: 142 [1280/8000 (16%)]\tLoss: 29.600327\n",
      "Train Epoch: 142 [2560/8000 (32%)]\tLoss: 29.939138\n",
      "Train Epoch: 142 [3840/8000 (48%)]\tLoss: 29.927164\n",
      "Train Epoch: 142 [5120/8000 (64%)]\tLoss: 30.047607\n",
      "Train Epoch: 142 [6400/8000 (80%)]\tLoss: 29.921167\n",
      "Train Epoch: 142 [7680/8000 (96%)]\tLoss: 29.800940\n",
      "====> Epoch: 142 Average loss: 29.8558\n",
      "Train Epoch: 143 [0/8000 (0%)]\tLoss: 29.727608\n",
      "Train Epoch: 143 [1280/8000 (16%)]\tLoss: 29.779793\n",
      "Train Epoch: 143 [2560/8000 (32%)]\tLoss: 30.055779\n",
      "Train Epoch: 143 [3840/8000 (48%)]\tLoss: 29.933382\n",
      "Train Epoch: 143 [5120/8000 (64%)]\tLoss: 29.862858\n",
      "Train Epoch: 143 [6400/8000 (80%)]\tLoss: 29.930767\n",
      "Train Epoch: 143 [7680/8000 (96%)]\tLoss: 29.747162\n",
      "====> Epoch: 143 Average loss: 29.8547\n",
      "Train Epoch: 144 [0/8000 (0%)]\tLoss: 29.688169\n",
      "Train Epoch: 144 [1280/8000 (16%)]\tLoss: 29.704815\n",
      "Train Epoch: 144 [2560/8000 (32%)]\tLoss: 29.876923\n",
      "Train Epoch: 144 [3840/8000 (48%)]\tLoss: 29.802254\n",
      "Train Epoch: 144 [5120/8000 (64%)]\tLoss: 29.894102\n",
      "Train Epoch: 144 [6400/8000 (80%)]\tLoss: 30.199577\n",
      "Train Epoch: 144 [7680/8000 (96%)]\tLoss: 30.185406\n",
      "====> Epoch: 144 Average loss: 29.8532\n",
      "Train Epoch: 145 [0/8000 (0%)]\tLoss: 30.059927\n",
      "Train Epoch: 145 [1280/8000 (16%)]\tLoss: 30.001638\n",
      "Train Epoch: 145 [2560/8000 (32%)]\tLoss: 29.744194\n",
      "Train Epoch: 145 [3840/8000 (48%)]\tLoss: 29.870365\n",
      "Train Epoch: 145 [5120/8000 (64%)]\tLoss: 29.858353\n",
      "Train Epoch: 145 [6400/8000 (80%)]\tLoss: 30.258909\n",
      "Train Epoch: 145 [7680/8000 (96%)]\tLoss: 29.828327\n",
      "====> Epoch: 145 Average loss: 29.8516\n",
      "Train Epoch: 146 [0/8000 (0%)]\tLoss: 29.661757\n",
      "Train Epoch: 146 [1280/8000 (16%)]\tLoss: 29.657804\n",
      "Train Epoch: 146 [2560/8000 (32%)]\tLoss: 29.745129\n",
      "Train Epoch: 146 [3840/8000 (48%)]\tLoss: 29.921148\n",
      "Train Epoch: 146 [5120/8000 (64%)]\tLoss: 30.375175\n",
      "Train Epoch: 146 [6400/8000 (80%)]\tLoss: 29.601187\n",
      "Train Epoch: 146 [7680/8000 (96%)]\tLoss: 29.858494\n",
      "====> Epoch: 146 Average loss: 29.8516\n",
      "Train Epoch: 147 [0/8000 (0%)]\tLoss: 29.822062\n",
      "Train Epoch: 147 [1280/8000 (16%)]\tLoss: 29.941999\n",
      "Train Epoch: 147 [2560/8000 (32%)]\tLoss: 29.695248\n",
      "Train Epoch: 147 [3840/8000 (48%)]\tLoss: 29.686251\n",
      "Train Epoch: 147 [5120/8000 (64%)]\tLoss: 30.055113\n",
      "Train Epoch: 147 [6400/8000 (80%)]\tLoss: 29.925222\n",
      "Train Epoch: 147 [7680/8000 (96%)]\tLoss: 29.942661\n",
      "====> Epoch: 147 Average loss: 29.8461\n",
      "Train Epoch: 148 [0/8000 (0%)]\tLoss: 30.036716\n",
      "Train Epoch: 148 [1280/8000 (16%)]\tLoss: 29.961206\n",
      "Train Epoch: 148 [2560/8000 (32%)]\tLoss: 29.791533\n",
      "Train Epoch: 148 [3840/8000 (48%)]\tLoss: 29.903706\n",
      "Train Epoch: 148 [5120/8000 (64%)]\tLoss: 29.879761\n",
      "Train Epoch: 148 [6400/8000 (80%)]\tLoss: 29.872585\n",
      "Train Epoch: 148 [7680/8000 (96%)]\tLoss: 29.777935\n",
      "====> Epoch: 148 Average loss: 29.8466\n",
      "Train Epoch: 149 [0/8000 (0%)]\tLoss: 29.571344\n",
      "Train Epoch: 149 [1280/8000 (16%)]\tLoss: 29.807587\n",
      "Train Epoch: 149 [2560/8000 (32%)]\tLoss: 29.760178\n",
      "Train Epoch: 149 [3840/8000 (48%)]\tLoss: 29.699923\n",
      "Train Epoch: 149 [5120/8000 (64%)]\tLoss: 29.885208\n",
      "Train Epoch: 149 [6400/8000 (80%)]\tLoss: 29.679276\n",
      "Train Epoch: 149 [7680/8000 (96%)]\tLoss: 29.827034\n",
      "====> Epoch: 149 Average loss: 29.8427\n",
      "Train Epoch: 150 [0/8000 (0%)]\tLoss: 29.574999\n",
      "Train Epoch: 150 [1280/8000 (16%)]\tLoss: 29.844069\n",
      "Train Epoch: 150 [2560/8000 (32%)]\tLoss: 29.843580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 150 [3840/8000 (48%)]\tLoss: 29.635920\n",
      "Train Epoch: 150 [5120/8000 (64%)]\tLoss: 29.854179\n",
      "Train Epoch: 150 [6400/8000 (80%)]\tLoss: 29.923254\n",
      "Train Epoch: 150 [7680/8000 (96%)]\tLoss: 29.756056\n",
      "====> Epoch: 150 Average loss: 29.8447\n",
      "Train Epoch: 151 [0/8000 (0%)]\tLoss: 29.892080\n",
      "Train Epoch: 151 [1280/8000 (16%)]\tLoss: 29.775129\n",
      "Train Epoch: 151 [2560/8000 (32%)]\tLoss: 30.039793\n",
      "Train Epoch: 151 [3840/8000 (48%)]\tLoss: 29.996382\n",
      "Train Epoch: 151 [5120/8000 (64%)]\tLoss: 29.645355\n",
      "Train Epoch: 151 [6400/8000 (80%)]\tLoss: 30.122829\n",
      "Train Epoch: 151 [7680/8000 (96%)]\tLoss: 29.612366\n",
      "====> Epoch: 151 Average loss: 29.8476\n",
      "Train Epoch: 152 [0/8000 (0%)]\tLoss: 29.872206\n",
      "Train Epoch: 152 [1280/8000 (16%)]\tLoss: 30.007320\n",
      "Train Epoch: 152 [2560/8000 (32%)]\tLoss: 29.966110\n",
      "Train Epoch: 152 [3840/8000 (48%)]\tLoss: 29.747076\n",
      "Train Epoch: 152 [5120/8000 (64%)]\tLoss: 29.732082\n",
      "Train Epoch: 152 [6400/8000 (80%)]\tLoss: 29.661459\n",
      "Train Epoch: 152 [7680/8000 (96%)]\tLoss: 29.898163\n",
      "====> Epoch: 152 Average loss: 29.8418\n",
      "Train Epoch: 153 [0/8000 (0%)]\tLoss: 29.763300\n",
      "Train Epoch: 153 [1280/8000 (16%)]\tLoss: 29.484903\n",
      "Train Epoch: 153 [2560/8000 (32%)]\tLoss: 30.020281\n",
      "Train Epoch: 153 [3840/8000 (48%)]\tLoss: 29.655491\n",
      "Train Epoch: 153 [5120/8000 (64%)]\tLoss: 29.731224\n",
      "Train Epoch: 153 [6400/8000 (80%)]\tLoss: 29.753639\n",
      "Train Epoch: 153 [7680/8000 (96%)]\tLoss: 29.668797\n",
      "====> Epoch: 153 Average loss: 29.8429\n",
      "Train Epoch: 154 [0/8000 (0%)]\tLoss: 29.963219\n",
      "Train Epoch: 154 [1280/8000 (16%)]\tLoss: 29.682484\n",
      "Train Epoch: 154 [2560/8000 (32%)]\tLoss: 29.939432\n",
      "Train Epoch: 154 [3840/8000 (48%)]\tLoss: 30.310204\n",
      "Train Epoch: 154 [5120/8000 (64%)]\tLoss: 29.751211\n",
      "Train Epoch: 154 [6400/8000 (80%)]\tLoss: 29.537790\n",
      "Train Epoch: 154 [7680/8000 (96%)]\tLoss: 29.761370\n",
      "====> Epoch: 154 Average loss: 29.8526\n",
      "Train Epoch: 155 [0/8000 (0%)]\tLoss: 29.767513\n",
      "Train Epoch: 155 [1280/8000 (16%)]\tLoss: 29.714983\n",
      "Train Epoch: 155 [2560/8000 (32%)]\tLoss: 29.817699\n",
      "Train Epoch: 155 [3840/8000 (48%)]\tLoss: 29.812609\n",
      "Train Epoch: 155 [5120/8000 (64%)]\tLoss: 29.740892\n",
      "Train Epoch: 155 [6400/8000 (80%)]\tLoss: 29.789371\n",
      "Train Epoch: 155 [7680/8000 (96%)]\tLoss: 29.867495\n",
      "====> Epoch: 155 Average loss: 29.8450\n",
      "Train Epoch: 156 [0/8000 (0%)]\tLoss: 29.860334\n",
      "Train Epoch: 156 [1280/8000 (16%)]\tLoss: 29.635778\n",
      "Train Epoch: 156 [2560/8000 (32%)]\tLoss: 29.810249\n",
      "Train Epoch: 156 [3840/8000 (48%)]\tLoss: 29.849625\n",
      "Train Epoch: 156 [5120/8000 (64%)]\tLoss: 29.870436\n",
      "Train Epoch: 156 [6400/8000 (80%)]\tLoss: 29.971279\n",
      "Train Epoch: 156 [7680/8000 (96%)]\tLoss: 29.923525\n",
      "====> Epoch: 156 Average loss: 29.8394\n",
      "Train Epoch: 157 [0/8000 (0%)]\tLoss: 29.860075\n",
      "Train Epoch: 157 [1280/8000 (16%)]\tLoss: 29.862200\n",
      "Train Epoch: 157 [2560/8000 (32%)]\tLoss: 29.727098\n",
      "Train Epoch: 157 [3840/8000 (48%)]\tLoss: 29.587946\n",
      "Train Epoch: 157 [5120/8000 (64%)]\tLoss: 29.969469\n",
      "Train Epoch: 157 [6400/8000 (80%)]\tLoss: 29.993134\n",
      "Train Epoch: 157 [7680/8000 (96%)]\tLoss: 30.059372\n",
      "====> Epoch: 157 Average loss: 29.8360\n",
      "Train Epoch: 158 [0/8000 (0%)]\tLoss: 30.127205\n",
      "Train Epoch: 158 [1280/8000 (16%)]\tLoss: 29.947792\n",
      "Train Epoch: 158 [2560/8000 (32%)]\tLoss: 29.982981\n",
      "Train Epoch: 158 [3840/8000 (48%)]\tLoss: 29.928944\n",
      "Train Epoch: 158 [5120/8000 (64%)]\tLoss: 29.713461\n",
      "Train Epoch: 158 [6400/8000 (80%)]\tLoss: 29.825796\n",
      "Train Epoch: 158 [7680/8000 (96%)]\tLoss: 29.688419\n",
      "====> Epoch: 158 Average loss: 29.8390\n",
      "Train Epoch: 159 [0/8000 (0%)]\tLoss: 29.859129\n",
      "Train Epoch: 159 [1280/8000 (16%)]\tLoss: 30.022751\n",
      "Train Epoch: 159 [2560/8000 (32%)]\tLoss: 29.720253\n",
      "Train Epoch: 159 [3840/8000 (48%)]\tLoss: 29.714245\n",
      "Train Epoch: 159 [5120/8000 (64%)]\tLoss: 29.833914\n",
      "Train Epoch: 159 [6400/8000 (80%)]\tLoss: 29.934895\n",
      "Train Epoch: 159 [7680/8000 (96%)]\tLoss: 30.176065\n",
      "====> Epoch: 159 Average loss: 29.8350\n",
      "Train Epoch: 160 [0/8000 (0%)]\tLoss: 29.854668\n",
      "Train Epoch: 160 [1280/8000 (16%)]\tLoss: 29.780993\n",
      "Train Epoch: 160 [2560/8000 (32%)]\tLoss: 30.082043\n",
      "Train Epoch: 160 [3840/8000 (48%)]\tLoss: 29.866489\n",
      "Train Epoch: 160 [5120/8000 (64%)]\tLoss: 29.784409\n",
      "Train Epoch: 160 [6400/8000 (80%)]\tLoss: 29.973063\n",
      "Train Epoch: 160 [7680/8000 (96%)]\tLoss: 29.901939\n",
      "====> Epoch: 160 Average loss: 29.8354\n",
      "Train Epoch: 161 [0/8000 (0%)]\tLoss: 30.008965\n",
      "Train Epoch: 161 [1280/8000 (16%)]\tLoss: 29.525608\n",
      "Train Epoch: 161 [2560/8000 (32%)]\tLoss: 29.945133\n",
      "Train Epoch: 161 [3840/8000 (48%)]\tLoss: 29.532711\n",
      "Train Epoch: 161 [5120/8000 (64%)]\tLoss: 29.961567\n",
      "Train Epoch: 161 [6400/8000 (80%)]\tLoss: 29.773979\n",
      "Train Epoch: 161 [7680/8000 (96%)]\tLoss: 30.009254\n",
      "====> Epoch: 161 Average loss: 29.8401\n",
      "Train Epoch: 162 [0/8000 (0%)]\tLoss: 29.785627\n",
      "Train Epoch: 162 [1280/8000 (16%)]\tLoss: 30.004511\n",
      "Train Epoch: 162 [2560/8000 (32%)]\tLoss: 29.668833\n",
      "Train Epoch: 162 [3840/8000 (48%)]\tLoss: 29.661026\n",
      "Train Epoch: 162 [5120/8000 (64%)]\tLoss: 29.693491\n",
      "Train Epoch: 162 [6400/8000 (80%)]\tLoss: 29.867348\n",
      "Train Epoch: 162 [7680/8000 (96%)]\tLoss: 29.939651\n",
      "====> Epoch: 162 Average loss: 29.8344\n",
      "Train Epoch: 163 [0/8000 (0%)]\tLoss: 29.888926\n",
      "Train Epoch: 163 [1280/8000 (16%)]\tLoss: 29.801085\n",
      "Train Epoch: 163 [2560/8000 (32%)]\tLoss: 29.859055\n",
      "Train Epoch: 163 [3840/8000 (48%)]\tLoss: 29.931381\n",
      "Train Epoch: 163 [5120/8000 (64%)]\tLoss: 29.794708\n",
      "Train Epoch: 163 [6400/8000 (80%)]\tLoss: 29.585209\n",
      "Train Epoch: 163 [7680/8000 (96%)]\tLoss: 29.765160\n",
      "====> Epoch: 163 Average loss: 29.8292\n",
      "Train Epoch: 164 [0/8000 (0%)]\tLoss: 29.749794\n",
      "Train Epoch: 164 [1280/8000 (16%)]\tLoss: 29.788185\n",
      "Train Epoch: 164 [2560/8000 (32%)]\tLoss: 29.627398\n",
      "Train Epoch: 164 [3840/8000 (48%)]\tLoss: 30.151995\n",
      "Train Epoch: 164 [5120/8000 (64%)]\tLoss: 29.665627\n",
      "Train Epoch: 164 [6400/8000 (80%)]\tLoss: 29.760736\n",
      "Train Epoch: 164 [7680/8000 (96%)]\tLoss: 29.912926\n",
      "====> Epoch: 164 Average loss: 29.8355\n",
      "Train Epoch: 165 [0/8000 (0%)]\tLoss: 29.646494\n",
      "Train Epoch: 165 [1280/8000 (16%)]\tLoss: 29.647474\n",
      "Train Epoch: 165 [2560/8000 (32%)]\tLoss: 30.224125\n",
      "Train Epoch: 165 [3840/8000 (48%)]\tLoss: 29.723206\n",
      "Train Epoch: 165 [5120/8000 (64%)]\tLoss: 29.710115\n",
      "Train Epoch: 165 [6400/8000 (80%)]\tLoss: 29.814293\n",
      "Train Epoch: 165 [7680/8000 (96%)]\tLoss: 29.759647\n",
      "====> Epoch: 165 Average loss: 29.8304\n",
      "Train Epoch: 166 [0/8000 (0%)]\tLoss: 29.736710\n",
      "Train Epoch: 166 [1280/8000 (16%)]\tLoss: 29.770378\n",
      "Train Epoch: 166 [2560/8000 (32%)]\tLoss: 29.742022\n",
      "Train Epoch: 166 [3840/8000 (48%)]\tLoss: 29.856194\n",
      "Train Epoch: 166 [5120/8000 (64%)]\tLoss: 29.926348\n",
      "Train Epoch: 166 [6400/8000 (80%)]\tLoss: 29.541378\n",
      "Train Epoch: 166 [7680/8000 (96%)]\tLoss: 29.872362\n",
      "====> Epoch: 166 Average loss: 29.8314\n",
      "Train Epoch: 167 [0/8000 (0%)]\tLoss: 29.850218\n",
      "Train Epoch: 167 [1280/8000 (16%)]\tLoss: 29.856688\n",
      "Train Epoch: 167 [2560/8000 (32%)]\tLoss: 29.844374\n",
      "Train Epoch: 167 [3840/8000 (48%)]\tLoss: 29.755766\n",
      "Train Epoch: 167 [5120/8000 (64%)]\tLoss: 29.677082\n",
      "Train Epoch: 167 [6400/8000 (80%)]\tLoss: 29.860857\n",
      "Train Epoch: 167 [7680/8000 (96%)]\tLoss: 29.803637\n",
      "====> Epoch: 167 Average loss: 29.8278\n",
      "Train Epoch: 168 [0/8000 (0%)]\tLoss: 29.738649\n",
      "Train Epoch: 168 [1280/8000 (16%)]\tLoss: 29.811876\n",
      "Train Epoch: 168 [2560/8000 (32%)]\tLoss: 29.744225\n",
      "Train Epoch: 168 [3840/8000 (48%)]\tLoss: 29.640295\n",
      "Train Epoch: 168 [5120/8000 (64%)]\tLoss: 29.700878\n",
      "Train Epoch: 168 [6400/8000 (80%)]\tLoss: 30.068792\n",
      "Train Epoch: 168 [7680/8000 (96%)]\tLoss: 30.059168\n",
      "====> Epoch: 168 Average loss: 29.8302\n",
      "Train Epoch: 169 [0/8000 (0%)]\tLoss: 29.808859\n",
      "Train Epoch: 169 [1280/8000 (16%)]\tLoss: 29.822668\n",
      "Train Epoch: 169 [2560/8000 (32%)]\tLoss: 30.113863\n",
      "Train Epoch: 169 [3840/8000 (48%)]\tLoss: 29.835459\n",
      "Train Epoch: 169 [5120/8000 (64%)]\tLoss: 29.690922\n",
      "Train Epoch: 169 [6400/8000 (80%)]\tLoss: 29.884312\n",
      "Train Epoch: 169 [7680/8000 (96%)]\tLoss: 29.967434\n",
      "====> Epoch: 169 Average loss: 29.8276\n",
      "Train Epoch: 170 [0/8000 (0%)]\tLoss: 29.800200\n",
      "Train Epoch: 170 [1280/8000 (16%)]\tLoss: 29.919741\n",
      "Train Epoch: 170 [2560/8000 (32%)]\tLoss: 29.947550\n",
      "Train Epoch: 170 [3840/8000 (48%)]\tLoss: 29.755896\n",
      "Train Epoch: 170 [5120/8000 (64%)]\tLoss: 29.704096\n",
      "Train Epoch: 170 [6400/8000 (80%)]\tLoss: 29.953430\n",
      "Train Epoch: 170 [7680/8000 (96%)]\tLoss: 29.837702\n",
      "====> Epoch: 170 Average loss: 29.8254\n",
      "Train Epoch: 171 [0/8000 (0%)]\tLoss: 29.955048\n",
      "Train Epoch: 171 [1280/8000 (16%)]\tLoss: 29.667873\n",
      "Train Epoch: 171 [2560/8000 (32%)]\tLoss: 29.944521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 171 [3840/8000 (48%)]\tLoss: 29.691778\n",
      "Train Epoch: 171 [5120/8000 (64%)]\tLoss: 29.820354\n",
      "Train Epoch: 171 [6400/8000 (80%)]\tLoss: 29.697687\n",
      "Train Epoch: 171 [7680/8000 (96%)]\tLoss: 29.529900\n",
      "====> Epoch: 171 Average loss: 29.8322\n",
      "Train Epoch: 172 [0/8000 (0%)]\tLoss: 30.050375\n",
      "Train Epoch: 172 [1280/8000 (16%)]\tLoss: 29.819826\n",
      "Train Epoch: 172 [2560/8000 (32%)]\tLoss: 29.893105\n",
      "Train Epoch: 172 [3840/8000 (48%)]\tLoss: 29.835381\n",
      "Train Epoch: 172 [5120/8000 (64%)]\tLoss: 29.854538\n",
      "Train Epoch: 172 [6400/8000 (80%)]\tLoss: 29.700729\n",
      "Train Epoch: 172 [7680/8000 (96%)]\tLoss: 29.893032\n",
      "====> Epoch: 172 Average loss: 29.8292\n",
      "Train Epoch: 173 [0/8000 (0%)]\tLoss: 29.822430\n",
      "Train Epoch: 173 [1280/8000 (16%)]\tLoss: 30.048073\n",
      "Train Epoch: 173 [2560/8000 (32%)]\tLoss: 29.792807\n",
      "Train Epoch: 173 [3840/8000 (48%)]\tLoss: 29.933641\n",
      "Train Epoch: 173 [5120/8000 (64%)]\tLoss: 30.016602\n",
      "Train Epoch: 173 [6400/8000 (80%)]\tLoss: 29.747728\n",
      "Train Epoch: 173 [7680/8000 (96%)]\tLoss: 29.592339\n",
      "====> Epoch: 173 Average loss: 29.8246\n",
      "Train Epoch: 174 [0/8000 (0%)]\tLoss: 30.099756\n",
      "Train Epoch: 174 [1280/8000 (16%)]\tLoss: 29.914568\n",
      "Train Epoch: 174 [2560/8000 (32%)]\tLoss: 29.601320\n",
      "Train Epoch: 174 [3840/8000 (48%)]\tLoss: 29.874485\n",
      "Train Epoch: 174 [5120/8000 (64%)]\tLoss: 29.895231\n",
      "Train Epoch: 174 [6400/8000 (80%)]\tLoss: 29.701897\n",
      "Train Epoch: 174 [7680/8000 (96%)]\tLoss: 29.566437\n",
      "====> Epoch: 174 Average loss: 29.8274\n",
      "Train Epoch: 175 [0/8000 (0%)]\tLoss: 29.642065\n",
      "Train Epoch: 175 [1280/8000 (16%)]\tLoss: 29.598166\n",
      "Train Epoch: 175 [2560/8000 (32%)]\tLoss: 29.722504\n",
      "Train Epoch: 175 [3840/8000 (48%)]\tLoss: 29.420137\n",
      "Train Epoch: 175 [5120/8000 (64%)]\tLoss: 29.632229\n",
      "Train Epoch: 175 [6400/8000 (80%)]\tLoss: 29.743534\n",
      "Train Epoch: 175 [7680/8000 (96%)]\tLoss: 29.822592\n",
      "====> Epoch: 175 Average loss: 29.8251\n",
      "Train Epoch: 176 [0/8000 (0%)]\tLoss: 29.873903\n",
      "Train Epoch: 176 [1280/8000 (16%)]\tLoss: 30.034555\n",
      "Train Epoch: 176 [2560/8000 (32%)]\tLoss: 29.660620\n",
      "Train Epoch: 176 [3840/8000 (48%)]\tLoss: 29.713276\n",
      "Train Epoch: 176 [5120/8000 (64%)]\tLoss: 29.922810\n",
      "Train Epoch: 176 [6400/8000 (80%)]\tLoss: 29.881262\n",
      "Train Epoch: 176 [7680/8000 (96%)]\tLoss: 29.685612\n",
      "====> Epoch: 176 Average loss: 29.8204\n",
      "Train Epoch: 177 [0/8000 (0%)]\tLoss: 29.661747\n",
      "Train Epoch: 177 [1280/8000 (16%)]\tLoss: 29.990891\n",
      "Train Epoch: 177 [2560/8000 (32%)]\tLoss: 29.981262\n",
      "Train Epoch: 177 [3840/8000 (48%)]\tLoss: 29.937117\n",
      "Train Epoch: 177 [5120/8000 (64%)]\tLoss: 29.868196\n",
      "Train Epoch: 177 [6400/8000 (80%)]\tLoss: 29.742266\n",
      "Train Epoch: 177 [7680/8000 (96%)]\tLoss: 29.851669\n",
      "====> Epoch: 177 Average loss: 29.8278\n",
      "Train Epoch: 178 [0/8000 (0%)]\tLoss: 29.899406\n",
      "Train Epoch: 178 [1280/8000 (16%)]\tLoss: 30.117563\n",
      "Train Epoch: 178 [2560/8000 (32%)]\tLoss: 29.965269\n",
      "Train Epoch: 178 [3840/8000 (48%)]\tLoss: 29.651743\n",
      "Train Epoch: 178 [5120/8000 (64%)]\tLoss: 29.724300\n",
      "Train Epoch: 178 [6400/8000 (80%)]\tLoss: 29.623217\n",
      "Train Epoch: 178 [7680/8000 (96%)]\tLoss: 29.813366\n",
      "====> Epoch: 178 Average loss: 29.8190\n",
      "Train Epoch: 179 [0/8000 (0%)]\tLoss: 29.691317\n",
      "Train Epoch: 179 [1280/8000 (16%)]\tLoss: 29.708324\n",
      "Train Epoch: 179 [2560/8000 (32%)]\tLoss: 29.895996\n",
      "Train Epoch: 179 [3840/8000 (48%)]\tLoss: 29.734064\n",
      "Train Epoch: 179 [5120/8000 (64%)]\tLoss: 30.035282\n",
      "Train Epoch: 179 [6400/8000 (80%)]\tLoss: 30.082747\n",
      "Train Epoch: 179 [7680/8000 (96%)]\tLoss: 29.908562\n",
      "====> Epoch: 179 Average loss: 29.8198\n",
      "Train Epoch: 180 [0/8000 (0%)]\tLoss: 29.782373\n",
      "Train Epoch: 180 [1280/8000 (16%)]\tLoss: 29.516321\n",
      "Train Epoch: 180 [2560/8000 (32%)]\tLoss: 30.138264\n",
      "Train Epoch: 180 [3840/8000 (48%)]\tLoss: 29.694380\n",
      "Train Epoch: 180 [5120/8000 (64%)]\tLoss: 29.874992\n",
      "Train Epoch: 180 [6400/8000 (80%)]\tLoss: 29.588085\n",
      "Train Epoch: 180 [7680/8000 (96%)]\tLoss: 29.950306\n",
      "====> Epoch: 180 Average loss: 29.8182\n",
      "Train Epoch: 181 [0/8000 (0%)]\tLoss: 29.932419\n",
      "Train Epoch: 181 [1280/8000 (16%)]\tLoss: 29.755714\n",
      "Train Epoch: 181 [2560/8000 (32%)]\tLoss: 29.990362\n",
      "Train Epoch: 181 [3840/8000 (48%)]\tLoss: 29.792976\n",
      "Train Epoch: 181 [5120/8000 (64%)]\tLoss: 29.905270\n",
      "Train Epoch: 181 [6400/8000 (80%)]\tLoss: 29.639246\n",
      "Train Epoch: 181 [7680/8000 (96%)]\tLoss: 29.856697\n",
      "====> Epoch: 181 Average loss: 29.8181\n",
      "Train Epoch: 182 [0/8000 (0%)]\tLoss: 29.733955\n",
      "Train Epoch: 182 [1280/8000 (16%)]\tLoss: 30.002630\n",
      "Train Epoch: 182 [2560/8000 (32%)]\tLoss: 29.546303\n",
      "Train Epoch: 182 [3840/8000 (48%)]\tLoss: 29.332294\n",
      "Train Epoch: 182 [5120/8000 (64%)]\tLoss: 30.108969\n",
      "Train Epoch: 182 [6400/8000 (80%)]\tLoss: 29.789400\n",
      "Train Epoch: 182 [7680/8000 (96%)]\tLoss: 29.821903\n",
      "====> Epoch: 182 Average loss: 29.8201\n",
      "Train Epoch: 183 [0/8000 (0%)]\tLoss: 29.797581\n",
      "Train Epoch: 183 [1280/8000 (16%)]\tLoss: 29.746046\n",
      "Train Epoch: 183 [2560/8000 (32%)]\tLoss: 29.989004\n",
      "Train Epoch: 183 [3840/8000 (48%)]\tLoss: 29.687984\n",
      "Train Epoch: 183 [5120/8000 (64%)]\tLoss: 29.996981\n",
      "Train Epoch: 183 [6400/8000 (80%)]\tLoss: 29.749342\n",
      "Train Epoch: 183 [7680/8000 (96%)]\tLoss: 29.710102\n",
      "====> Epoch: 183 Average loss: 29.8149\n",
      "Train Epoch: 184 [0/8000 (0%)]\tLoss: 29.872387\n",
      "Train Epoch: 184 [1280/8000 (16%)]\tLoss: 29.862476\n",
      "Train Epoch: 184 [2560/8000 (32%)]\tLoss: 29.854042\n",
      "Train Epoch: 184 [3840/8000 (48%)]\tLoss: 29.703064\n",
      "Train Epoch: 184 [5120/8000 (64%)]\tLoss: 29.746521\n",
      "Train Epoch: 184 [6400/8000 (80%)]\tLoss: 29.690630\n",
      "Train Epoch: 184 [7680/8000 (96%)]\tLoss: 29.896132\n",
      "====> Epoch: 184 Average loss: 29.8140\n",
      "Train Epoch: 185 [0/8000 (0%)]\tLoss: 29.916628\n",
      "Train Epoch: 185 [1280/8000 (16%)]\tLoss: 29.714111\n",
      "Train Epoch: 185 [2560/8000 (32%)]\tLoss: 29.478689\n",
      "Train Epoch: 185 [3840/8000 (48%)]\tLoss: 29.965668\n",
      "Train Epoch: 185 [5120/8000 (64%)]\tLoss: 29.948055\n",
      "Train Epoch: 185 [6400/8000 (80%)]\tLoss: 29.757099\n",
      "Train Epoch: 185 [7680/8000 (96%)]\tLoss: 30.119591\n",
      "====> Epoch: 185 Average loss: 29.8138\n",
      "Train Epoch: 186 [0/8000 (0%)]\tLoss: 29.995111\n",
      "Train Epoch: 186 [1280/8000 (16%)]\tLoss: 29.721104\n",
      "Train Epoch: 186 [2560/8000 (32%)]\tLoss: 29.616102\n",
      "Train Epoch: 186 [3840/8000 (48%)]\tLoss: 29.456978\n",
      "Train Epoch: 186 [5120/8000 (64%)]\tLoss: 29.905184\n",
      "Train Epoch: 186 [6400/8000 (80%)]\tLoss: 29.864584\n",
      "Train Epoch: 186 [7680/8000 (96%)]\tLoss: 29.899679\n",
      "====> Epoch: 186 Average loss: 29.8144\n",
      "Train Epoch: 187 [0/8000 (0%)]\tLoss: 29.708242\n",
      "Train Epoch: 187 [1280/8000 (16%)]\tLoss: 29.689268\n",
      "Train Epoch: 187 [2560/8000 (32%)]\tLoss: 29.674726\n",
      "Train Epoch: 187 [3840/8000 (48%)]\tLoss: 29.850533\n",
      "Train Epoch: 187 [5120/8000 (64%)]\tLoss: 29.799746\n",
      "Train Epoch: 187 [6400/8000 (80%)]\tLoss: 29.642475\n",
      "Train Epoch: 187 [7680/8000 (96%)]\tLoss: 29.850143\n",
      "====> Epoch: 187 Average loss: 29.8204\n",
      "Train Epoch: 188 [0/8000 (0%)]\tLoss: 29.867775\n",
      "Train Epoch: 188 [1280/8000 (16%)]\tLoss: 29.953625\n",
      "Train Epoch: 188 [2560/8000 (32%)]\tLoss: 29.622116\n",
      "Train Epoch: 188 [3840/8000 (48%)]\tLoss: 29.781324\n",
      "Train Epoch: 188 [5120/8000 (64%)]\tLoss: 30.094204\n",
      "Train Epoch: 188 [6400/8000 (80%)]\tLoss: 29.738352\n",
      "Train Epoch: 188 [7680/8000 (96%)]\tLoss: 29.704144\n",
      "====> Epoch: 188 Average loss: 29.8141\n",
      "Train Epoch: 189 [0/8000 (0%)]\tLoss: 30.014809\n",
      "Train Epoch: 189 [1280/8000 (16%)]\tLoss: 29.584394\n",
      "Train Epoch: 189 [2560/8000 (32%)]\tLoss: 29.591490\n",
      "Train Epoch: 189 [3840/8000 (48%)]\tLoss: 29.783266\n",
      "Train Epoch: 189 [5120/8000 (64%)]\tLoss: 29.986298\n",
      "Train Epoch: 189 [6400/8000 (80%)]\tLoss: 29.438938\n",
      "Train Epoch: 189 [7680/8000 (96%)]\tLoss: 29.760372\n",
      "====> Epoch: 189 Average loss: 29.8144\n",
      "Train Epoch: 190 [0/8000 (0%)]\tLoss: 29.962885\n",
      "Train Epoch: 190 [1280/8000 (16%)]\tLoss: 29.922407\n",
      "Train Epoch: 190 [2560/8000 (32%)]\tLoss: 29.576790\n",
      "Train Epoch: 190 [3840/8000 (48%)]\tLoss: 29.958385\n",
      "Train Epoch: 190 [5120/8000 (64%)]\tLoss: 29.701698\n",
      "Train Epoch: 190 [6400/8000 (80%)]\tLoss: 29.758600\n",
      "Train Epoch: 190 [7680/8000 (96%)]\tLoss: 29.930874\n",
      "====> Epoch: 190 Average loss: 29.8149\n",
      "Train Epoch: 191 [0/8000 (0%)]\tLoss: 29.852556\n",
      "Train Epoch: 191 [1280/8000 (16%)]\tLoss: 29.745163\n",
      "Train Epoch: 191 [2560/8000 (32%)]\tLoss: 29.813604\n",
      "Train Epoch: 191 [3840/8000 (48%)]\tLoss: 30.064758\n",
      "Train Epoch: 191 [5120/8000 (64%)]\tLoss: 29.671938\n",
      "Train Epoch: 191 [6400/8000 (80%)]\tLoss: 30.055010\n",
      "Train Epoch: 191 [7680/8000 (96%)]\tLoss: 29.724766\n",
      "====> Epoch: 191 Average loss: 29.8110\n",
      "Train Epoch: 192 [0/8000 (0%)]\tLoss: 29.786533\n",
      "Train Epoch: 192 [1280/8000 (16%)]\tLoss: 29.878263\n",
      "Train Epoch: 192 [2560/8000 (32%)]\tLoss: 29.766127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 192 [3840/8000 (48%)]\tLoss: 30.103653\n",
      "Train Epoch: 192 [5120/8000 (64%)]\tLoss: 29.973534\n",
      "Train Epoch: 192 [6400/8000 (80%)]\tLoss: 30.001276\n",
      "Train Epoch: 192 [7680/8000 (96%)]\tLoss: 29.812340\n",
      "====> Epoch: 192 Average loss: 29.8118\n",
      "Train Epoch: 193 [0/8000 (0%)]\tLoss: 29.887241\n",
      "Train Epoch: 193 [1280/8000 (16%)]\tLoss: 29.800154\n",
      "Train Epoch: 193 [2560/8000 (32%)]\tLoss: 29.862894\n",
      "Train Epoch: 193 [3840/8000 (48%)]\tLoss: 30.167154\n",
      "Train Epoch: 193 [5120/8000 (64%)]\tLoss: 29.892557\n",
      "Train Epoch: 193 [6400/8000 (80%)]\tLoss: 29.848461\n",
      "Train Epoch: 193 [7680/8000 (96%)]\tLoss: 29.772787\n",
      "====> Epoch: 193 Average loss: 29.8129\n",
      "Train Epoch: 194 [0/8000 (0%)]\tLoss: 30.023052\n",
      "Train Epoch: 194 [1280/8000 (16%)]\tLoss: 29.723381\n",
      "Train Epoch: 194 [2560/8000 (32%)]\tLoss: 29.811617\n",
      "Train Epoch: 194 [3840/8000 (48%)]\tLoss: 29.659929\n",
      "Train Epoch: 194 [5120/8000 (64%)]\tLoss: 29.900434\n",
      "Train Epoch: 194 [6400/8000 (80%)]\tLoss: 29.959351\n",
      "Train Epoch: 194 [7680/8000 (96%)]\tLoss: 30.050060\n",
      "====> Epoch: 194 Average loss: 29.8098\n",
      "Train Epoch: 195 [0/8000 (0%)]\tLoss: 29.672085\n",
      "Train Epoch: 195 [1280/8000 (16%)]\tLoss: 29.943893\n",
      "Train Epoch: 195 [2560/8000 (32%)]\tLoss: 29.699596\n",
      "Train Epoch: 195 [3840/8000 (48%)]\tLoss: 29.974583\n",
      "Train Epoch: 195 [5120/8000 (64%)]\tLoss: 29.841438\n",
      "Train Epoch: 195 [6400/8000 (80%)]\tLoss: 29.909590\n",
      "Train Epoch: 195 [7680/8000 (96%)]\tLoss: 29.791918\n",
      "====> Epoch: 195 Average loss: 29.8109\n",
      "Train Epoch: 196 [0/8000 (0%)]\tLoss: 29.796476\n",
      "Train Epoch: 196 [1280/8000 (16%)]\tLoss: 30.002277\n",
      "Train Epoch: 196 [2560/8000 (32%)]\tLoss: 29.926334\n",
      "Train Epoch: 196 [3840/8000 (48%)]\tLoss: 29.691866\n",
      "Train Epoch: 196 [5120/8000 (64%)]\tLoss: 29.482767\n",
      "Train Epoch: 196 [6400/8000 (80%)]\tLoss: 29.988226\n",
      "Train Epoch: 196 [7680/8000 (96%)]\tLoss: 30.014135\n",
      "====> Epoch: 196 Average loss: 29.8084\n",
      "Train Epoch: 197 [0/8000 (0%)]\tLoss: 29.647802\n",
      "Train Epoch: 197 [1280/8000 (16%)]\tLoss: 29.857992\n",
      "Train Epoch: 197 [2560/8000 (32%)]\tLoss: 29.492777\n",
      "Train Epoch: 197 [3840/8000 (48%)]\tLoss: 29.852961\n",
      "Train Epoch: 197 [5120/8000 (64%)]\tLoss: 29.711535\n",
      "Train Epoch: 197 [6400/8000 (80%)]\tLoss: 29.887621\n",
      "Train Epoch: 197 [7680/8000 (96%)]\tLoss: 30.090235\n",
      "====> Epoch: 197 Average loss: 29.8081\n",
      "Train Epoch: 198 [0/8000 (0%)]\tLoss: 29.628222\n",
      "Train Epoch: 198 [1280/8000 (16%)]\tLoss: 29.966719\n",
      "Train Epoch: 198 [2560/8000 (32%)]\tLoss: 29.672207\n",
      "Train Epoch: 198 [3840/8000 (48%)]\tLoss: 29.745846\n",
      "Train Epoch: 198 [5120/8000 (64%)]\tLoss: 29.557486\n",
      "Train Epoch: 198 [6400/8000 (80%)]\tLoss: 29.726702\n",
      "Train Epoch: 198 [7680/8000 (96%)]\tLoss: 29.497808\n",
      "====> Epoch: 198 Average loss: 29.8066\n",
      "Train Epoch: 199 [0/8000 (0%)]\tLoss: 29.931440\n",
      "Train Epoch: 199 [1280/8000 (16%)]\tLoss: 29.508221\n",
      "Train Epoch: 199 [2560/8000 (32%)]\tLoss: 29.658026\n",
      "Train Epoch: 199 [3840/8000 (48%)]\tLoss: 29.606720\n",
      "Train Epoch: 199 [5120/8000 (64%)]\tLoss: 29.959599\n",
      "Train Epoch: 199 [6400/8000 (80%)]\tLoss: 29.887527\n",
      "Train Epoch: 199 [7680/8000 (96%)]\tLoss: 29.683487\n",
      "====> Epoch: 199 Average loss: 29.8009\n",
      "Train Epoch: 200 [0/8000 (0%)]\tLoss: 29.744560\n",
      "Train Epoch: 200 [1280/8000 (16%)]\tLoss: 29.681286\n",
      "Train Epoch: 200 [2560/8000 (32%)]\tLoss: 29.702448\n",
      "Train Epoch: 200 [3840/8000 (48%)]\tLoss: 29.775393\n",
      "Train Epoch: 200 [5120/8000 (64%)]\tLoss: 30.027380\n",
      "Train Epoch: 200 [6400/8000 (80%)]\tLoss: 29.985846\n",
      "Train Epoch: 200 [7680/8000 (96%)]\tLoss: 29.661562\n",
      "====> Epoch: 200 Average loss: 29.8077\n",
      "Train Epoch: 201 [0/8000 (0%)]\tLoss: 29.676968\n",
      "Train Epoch: 201 [1280/8000 (16%)]\tLoss: 29.699018\n",
      "Train Epoch: 201 [2560/8000 (32%)]\tLoss: 29.597952\n",
      "Train Epoch: 201 [3840/8000 (48%)]\tLoss: 29.829771\n",
      "Train Epoch: 201 [5120/8000 (64%)]\tLoss: 29.519400\n",
      "Train Epoch: 201 [6400/8000 (80%)]\tLoss: 29.716923\n",
      "Train Epoch: 201 [7680/8000 (96%)]\tLoss: 29.568096\n",
      "====> Epoch: 201 Average loss: 29.8068\n",
      "Train Epoch: 202 [0/8000 (0%)]\tLoss: 29.780621\n",
      "Train Epoch: 202 [1280/8000 (16%)]\tLoss: 30.021332\n",
      "Train Epoch: 202 [2560/8000 (32%)]\tLoss: 29.776663\n",
      "Train Epoch: 202 [3840/8000 (48%)]\tLoss: 29.656378\n",
      "Train Epoch: 202 [5120/8000 (64%)]\tLoss: 29.775585\n",
      "Train Epoch: 202 [6400/8000 (80%)]\tLoss: 29.946606\n",
      "Train Epoch: 202 [7680/8000 (96%)]\tLoss: 29.901335\n",
      "====> Epoch: 202 Average loss: 29.8049\n",
      "Train Epoch: 203 [0/8000 (0%)]\tLoss: 29.930468\n",
      "Train Epoch: 203 [1280/8000 (16%)]\tLoss: 29.710258\n",
      "Train Epoch: 203 [2560/8000 (32%)]\tLoss: 29.756306\n",
      "Train Epoch: 203 [3840/8000 (48%)]\tLoss: 29.855183\n",
      "Train Epoch: 203 [5120/8000 (64%)]\tLoss: 29.900419\n",
      "Train Epoch: 203 [6400/8000 (80%)]\tLoss: 29.723492\n",
      "Train Epoch: 203 [7680/8000 (96%)]\tLoss: 29.670036\n",
      "====> Epoch: 203 Average loss: 29.8050\n",
      "Train Epoch: 204 [0/8000 (0%)]\tLoss: 29.590183\n",
      "Train Epoch: 204 [1280/8000 (16%)]\tLoss: 29.905067\n",
      "Train Epoch: 204 [2560/8000 (32%)]\tLoss: 29.816549\n",
      "Train Epoch: 204 [3840/8000 (48%)]\tLoss: 30.044479\n",
      "Train Epoch: 204 [5120/8000 (64%)]\tLoss: 29.561014\n",
      "Train Epoch: 204 [6400/8000 (80%)]\tLoss: 29.862070\n",
      "Train Epoch: 204 [7680/8000 (96%)]\tLoss: 29.817181\n",
      "====> Epoch: 204 Average loss: 29.7998\n",
      "Train Epoch: 205 [0/8000 (0%)]\tLoss: 29.741676\n",
      "Train Epoch: 205 [1280/8000 (16%)]\tLoss: 29.859093\n",
      "Train Epoch: 205 [2560/8000 (32%)]\tLoss: 29.758892\n",
      "Train Epoch: 205 [3840/8000 (48%)]\tLoss: 29.493366\n",
      "Train Epoch: 205 [5120/8000 (64%)]\tLoss: 29.620663\n",
      "Train Epoch: 205 [6400/8000 (80%)]\tLoss: 29.986654\n",
      "Train Epoch: 205 [7680/8000 (96%)]\tLoss: 29.652653\n",
      "====> Epoch: 205 Average loss: 29.8019\n",
      "Train Epoch: 206 [0/8000 (0%)]\tLoss: 29.765675\n",
      "Train Epoch: 206 [1280/8000 (16%)]\tLoss: 29.846584\n",
      "Train Epoch: 206 [2560/8000 (32%)]\tLoss: 30.030872\n",
      "Train Epoch: 206 [3840/8000 (48%)]\tLoss: 29.570410\n",
      "Train Epoch: 206 [5120/8000 (64%)]\tLoss: 29.646173\n",
      "Train Epoch: 206 [6400/8000 (80%)]\tLoss: 29.911873\n",
      "Train Epoch: 206 [7680/8000 (96%)]\tLoss: 29.578846\n",
      "====> Epoch: 206 Average loss: 29.8025\n",
      "Train Epoch: 207 [0/8000 (0%)]\tLoss: 29.752522\n",
      "Train Epoch: 207 [1280/8000 (16%)]\tLoss: 29.784203\n",
      "Train Epoch: 207 [2560/8000 (32%)]\tLoss: 29.897306\n",
      "Train Epoch: 207 [3840/8000 (48%)]\tLoss: 30.027382\n",
      "Train Epoch: 207 [5120/8000 (64%)]\tLoss: 29.661161\n",
      "Train Epoch: 207 [6400/8000 (80%)]\tLoss: 29.997433\n",
      "Train Epoch: 207 [7680/8000 (96%)]\tLoss: 29.908800\n",
      "====> Epoch: 207 Average loss: 29.8043\n",
      "Train Epoch: 208 [0/8000 (0%)]\tLoss: 29.893835\n",
      "Train Epoch: 208 [1280/8000 (16%)]\tLoss: 29.551144\n",
      "Train Epoch: 208 [2560/8000 (32%)]\tLoss: 29.864897\n",
      "Train Epoch: 208 [3840/8000 (48%)]\tLoss: 29.903431\n",
      "Train Epoch: 208 [5120/8000 (64%)]\tLoss: 29.670424\n",
      "Train Epoch: 208 [6400/8000 (80%)]\tLoss: 29.902267\n",
      "Train Epoch: 208 [7680/8000 (96%)]\tLoss: 29.791168\n",
      "====> Epoch: 208 Average loss: 29.8042\n",
      "Train Epoch: 209 [0/8000 (0%)]\tLoss: 29.813713\n",
      "Train Epoch: 209 [1280/8000 (16%)]\tLoss: 29.415522\n",
      "Train Epoch: 209 [2560/8000 (32%)]\tLoss: 29.696983\n",
      "Train Epoch: 209 [3840/8000 (48%)]\tLoss: 29.826431\n",
      "Train Epoch: 209 [5120/8000 (64%)]\tLoss: 29.715889\n",
      "Train Epoch: 209 [6400/8000 (80%)]\tLoss: 29.657810\n",
      "Train Epoch: 209 [7680/8000 (96%)]\tLoss: 29.777905\n",
      "====> Epoch: 209 Average loss: 29.7993\n",
      "Train Epoch: 210 [0/8000 (0%)]\tLoss: 29.467974\n",
      "Train Epoch: 210 [1280/8000 (16%)]\tLoss: 29.878948\n",
      "Train Epoch: 210 [2560/8000 (32%)]\tLoss: 29.967422\n",
      "Train Epoch: 210 [3840/8000 (48%)]\tLoss: 29.768478\n",
      "Train Epoch: 210 [5120/8000 (64%)]\tLoss: 29.776121\n",
      "Train Epoch: 210 [6400/8000 (80%)]\tLoss: 29.769302\n",
      "Train Epoch: 210 [7680/8000 (96%)]\tLoss: 29.941954\n",
      "====> Epoch: 210 Average loss: 29.8030\n",
      "Train Epoch: 211 [0/8000 (0%)]\tLoss: 29.751829\n",
      "Train Epoch: 211 [1280/8000 (16%)]\tLoss: 29.658943\n",
      "Train Epoch: 211 [2560/8000 (32%)]\tLoss: 30.199209\n",
      "Train Epoch: 211 [3840/8000 (48%)]\tLoss: 29.764517\n",
      "Train Epoch: 211 [5120/8000 (64%)]\tLoss: 29.600355\n",
      "Train Epoch: 211 [6400/8000 (80%)]\tLoss: 29.630972\n",
      "Train Epoch: 211 [7680/8000 (96%)]\tLoss: 29.755537\n",
      "====> Epoch: 211 Average loss: 29.8005\n",
      "Train Epoch: 212 [0/8000 (0%)]\tLoss: 29.558411\n",
      "Train Epoch: 212 [1280/8000 (16%)]\tLoss: 29.743198\n",
      "Train Epoch: 212 [2560/8000 (32%)]\tLoss: 29.775280\n",
      "Train Epoch: 212 [3840/8000 (48%)]\tLoss: 30.321987\n",
      "Train Epoch: 212 [5120/8000 (64%)]\tLoss: 29.893024\n",
      "Train Epoch: 212 [6400/8000 (80%)]\tLoss: 29.780886\n",
      "Train Epoch: 212 [7680/8000 (96%)]\tLoss: 30.012648\n",
      "====> Epoch: 212 Average loss: 29.7986\n",
      "Train Epoch: 213 [0/8000 (0%)]\tLoss: 29.771513\n",
      "Train Epoch: 213 [1280/8000 (16%)]\tLoss: 29.587505\n",
      "Train Epoch: 213 [2560/8000 (32%)]\tLoss: 29.774538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 213 [3840/8000 (48%)]\tLoss: 30.016869\n",
      "Train Epoch: 213 [5120/8000 (64%)]\tLoss: 29.846630\n",
      "Train Epoch: 213 [6400/8000 (80%)]\tLoss: 29.708797\n",
      "Train Epoch: 213 [7680/8000 (96%)]\tLoss: 29.815838\n",
      "====> Epoch: 213 Average loss: 29.8011\n",
      "Train Epoch: 214 [0/8000 (0%)]\tLoss: 29.759727\n",
      "Train Epoch: 214 [1280/8000 (16%)]\tLoss: 29.817102\n",
      "Train Epoch: 214 [2560/8000 (32%)]\tLoss: 29.760227\n",
      "Train Epoch: 214 [3840/8000 (48%)]\tLoss: 29.571650\n",
      "Train Epoch: 214 [5120/8000 (64%)]\tLoss: 30.152962\n",
      "Train Epoch: 214 [6400/8000 (80%)]\tLoss: 29.853821\n",
      "Train Epoch: 214 [7680/8000 (96%)]\tLoss: 29.901327\n",
      "====> Epoch: 214 Average loss: 29.8008\n",
      "Train Epoch: 215 [0/8000 (0%)]\tLoss: 29.709595\n",
      "Train Epoch: 215 [1280/8000 (16%)]\tLoss: 30.049688\n",
      "Train Epoch: 215 [2560/8000 (32%)]\tLoss: 29.787767\n",
      "Train Epoch: 215 [3840/8000 (48%)]\tLoss: 29.884525\n",
      "Train Epoch: 215 [5120/8000 (64%)]\tLoss: 29.654650\n",
      "Train Epoch: 215 [6400/8000 (80%)]\tLoss: 30.027760\n",
      "Train Epoch: 215 [7680/8000 (96%)]\tLoss: 29.874723\n",
      "====> Epoch: 215 Average loss: 29.7905\n",
      "Train Epoch: 216 [0/8000 (0%)]\tLoss: 29.716963\n",
      "Train Epoch: 216 [1280/8000 (16%)]\tLoss: 29.617773\n",
      "Train Epoch: 216 [2560/8000 (32%)]\tLoss: 29.626833\n",
      "Train Epoch: 216 [3840/8000 (48%)]\tLoss: 29.696655\n",
      "Train Epoch: 216 [5120/8000 (64%)]\tLoss: 30.035606\n",
      "Train Epoch: 216 [6400/8000 (80%)]\tLoss: 30.065159\n",
      "Train Epoch: 216 [7680/8000 (96%)]\tLoss: 30.120794\n",
      "====> Epoch: 216 Average loss: 29.7985\n",
      "Train Epoch: 217 [0/8000 (0%)]\tLoss: 29.523348\n",
      "Train Epoch: 217 [1280/8000 (16%)]\tLoss: 30.124512\n",
      "Train Epoch: 217 [2560/8000 (32%)]\tLoss: 29.797274\n",
      "Train Epoch: 217 [3840/8000 (48%)]\tLoss: 29.738695\n",
      "Train Epoch: 217 [5120/8000 (64%)]\tLoss: 29.680517\n",
      "Train Epoch: 217 [6400/8000 (80%)]\tLoss: 29.786625\n",
      "Train Epoch: 217 [7680/8000 (96%)]\tLoss: 29.981474\n",
      "====> Epoch: 217 Average loss: 29.7989\n",
      "Train Epoch: 218 [0/8000 (0%)]\tLoss: 29.711332\n",
      "Train Epoch: 218 [1280/8000 (16%)]\tLoss: 29.643665\n",
      "Train Epoch: 218 [2560/8000 (32%)]\tLoss: 29.531733\n",
      "Train Epoch: 218 [3840/8000 (48%)]\tLoss: 29.577831\n",
      "Train Epoch: 218 [5120/8000 (64%)]\tLoss: 29.899792\n",
      "Train Epoch: 218 [6400/8000 (80%)]\tLoss: 29.824474\n",
      "Train Epoch: 218 [7680/8000 (96%)]\tLoss: 29.852283\n",
      "====> Epoch: 218 Average loss: 29.7942\n",
      "Train Epoch: 219 [0/8000 (0%)]\tLoss: 29.501312\n",
      "Train Epoch: 219 [1280/8000 (16%)]\tLoss: 29.732944\n",
      "Train Epoch: 219 [2560/8000 (32%)]\tLoss: 29.803688\n",
      "Train Epoch: 219 [3840/8000 (48%)]\tLoss: 29.940607\n",
      "Train Epoch: 219 [5120/8000 (64%)]\tLoss: 29.705202\n",
      "Train Epoch: 219 [6400/8000 (80%)]\tLoss: 29.945004\n",
      "Train Epoch: 219 [7680/8000 (96%)]\tLoss: 29.779470\n",
      "====> Epoch: 219 Average loss: 29.7899\n",
      "Train Epoch: 220 [0/8000 (0%)]\tLoss: 29.673767\n",
      "Train Epoch: 220 [1280/8000 (16%)]\tLoss: 29.824636\n",
      "Train Epoch: 220 [2560/8000 (32%)]\tLoss: 29.588577\n",
      "Train Epoch: 220 [3840/8000 (48%)]\tLoss: 29.823088\n",
      "Train Epoch: 220 [5120/8000 (64%)]\tLoss: 29.919844\n",
      "Train Epoch: 220 [6400/8000 (80%)]\tLoss: 30.074789\n",
      "Train Epoch: 220 [7680/8000 (96%)]\tLoss: 30.004585\n",
      "====> Epoch: 220 Average loss: 29.7915\n",
      "Train Epoch: 221 [0/8000 (0%)]\tLoss: 29.629421\n",
      "Train Epoch: 221 [1280/8000 (16%)]\tLoss: 29.647953\n",
      "Train Epoch: 221 [2560/8000 (32%)]\tLoss: 29.778221\n",
      "Train Epoch: 221 [3840/8000 (48%)]\tLoss: 29.927725\n",
      "Train Epoch: 221 [5120/8000 (64%)]\tLoss: 29.659325\n",
      "Train Epoch: 221 [6400/8000 (80%)]\tLoss: 29.714783\n",
      "Train Epoch: 221 [7680/8000 (96%)]\tLoss: 29.621363\n",
      "====> Epoch: 221 Average loss: 29.7893\n",
      "Train Epoch: 222 [0/8000 (0%)]\tLoss: 29.502541\n",
      "Train Epoch: 222 [1280/8000 (16%)]\tLoss: 30.011080\n",
      "Train Epoch: 222 [2560/8000 (32%)]\tLoss: 29.817270\n",
      "Train Epoch: 222 [3840/8000 (48%)]\tLoss: 29.765356\n",
      "Train Epoch: 222 [5120/8000 (64%)]\tLoss: 29.591146\n",
      "Train Epoch: 222 [6400/8000 (80%)]\tLoss: 29.673124\n",
      "Train Epoch: 222 [7680/8000 (96%)]\tLoss: 29.916006\n",
      "====> Epoch: 222 Average loss: 29.7913\n",
      "Train Epoch: 223 [0/8000 (0%)]\tLoss: 29.735096\n",
      "Train Epoch: 223 [1280/8000 (16%)]\tLoss: 30.059135\n",
      "Train Epoch: 223 [2560/8000 (32%)]\tLoss: 29.895269\n",
      "Train Epoch: 223 [3840/8000 (48%)]\tLoss: 29.908260\n",
      "Train Epoch: 223 [5120/8000 (64%)]\tLoss: 29.559267\n",
      "Train Epoch: 223 [6400/8000 (80%)]\tLoss: 29.543005\n",
      "Train Epoch: 223 [7680/8000 (96%)]\tLoss: 29.819046\n",
      "====> Epoch: 223 Average loss: 29.7917\n",
      "Train Epoch: 224 [0/8000 (0%)]\tLoss: 29.974539\n",
      "Train Epoch: 224 [1280/8000 (16%)]\tLoss: 29.669226\n",
      "Train Epoch: 224 [2560/8000 (32%)]\tLoss: 29.790636\n",
      "Train Epoch: 224 [3840/8000 (48%)]\tLoss: 29.615257\n",
      "Train Epoch: 224 [5120/8000 (64%)]\tLoss: 29.819550\n",
      "Train Epoch: 224 [6400/8000 (80%)]\tLoss: 29.796558\n",
      "Train Epoch: 224 [7680/8000 (96%)]\tLoss: 29.667210\n",
      "====> Epoch: 224 Average loss: 29.7932\n",
      "Train Epoch: 225 [0/8000 (0%)]\tLoss: 30.009117\n",
      "Train Epoch: 225 [1280/8000 (16%)]\tLoss: 29.899288\n",
      "Train Epoch: 225 [2560/8000 (32%)]\tLoss: 29.691345\n",
      "Train Epoch: 225 [3840/8000 (48%)]\tLoss: 29.698746\n",
      "Train Epoch: 225 [5120/8000 (64%)]\tLoss: 29.761032\n",
      "Train Epoch: 225 [6400/8000 (80%)]\tLoss: 29.728920\n",
      "Train Epoch: 225 [7680/8000 (96%)]\tLoss: 29.750725\n",
      "====> Epoch: 225 Average loss: 29.7891\n",
      "Train Epoch: 226 [0/8000 (0%)]\tLoss: 29.621309\n",
      "Train Epoch: 226 [1280/8000 (16%)]\tLoss: 29.740597\n",
      "Train Epoch: 226 [2560/8000 (32%)]\tLoss: 29.742571\n",
      "Train Epoch: 226 [3840/8000 (48%)]\tLoss: 30.011992\n",
      "Train Epoch: 226 [5120/8000 (64%)]\tLoss: 29.753138\n",
      "Train Epoch: 226 [6400/8000 (80%)]\tLoss: 29.858091\n",
      "Train Epoch: 226 [7680/8000 (96%)]\tLoss: 29.858686\n",
      "====> Epoch: 226 Average loss: 29.7922\n",
      "Train Epoch: 227 [0/8000 (0%)]\tLoss: 29.823866\n",
      "Train Epoch: 227 [1280/8000 (16%)]\tLoss: 29.761009\n",
      "Train Epoch: 227 [2560/8000 (32%)]\tLoss: 29.926531\n",
      "Train Epoch: 227 [3840/8000 (48%)]\tLoss: 29.785776\n",
      "Train Epoch: 227 [5120/8000 (64%)]\tLoss: 29.779715\n",
      "Train Epoch: 227 [6400/8000 (80%)]\tLoss: 30.003069\n",
      "Train Epoch: 227 [7680/8000 (96%)]\tLoss: 29.778744\n",
      "====> Epoch: 227 Average loss: 29.7904\n",
      "Train Epoch: 228 [0/8000 (0%)]\tLoss: 29.596382\n",
      "Train Epoch: 228 [1280/8000 (16%)]\tLoss: 29.965355\n",
      "Train Epoch: 228 [2560/8000 (32%)]\tLoss: 29.813452\n",
      "Train Epoch: 228 [3840/8000 (48%)]\tLoss: 29.544483\n",
      "Train Epoch: 228 [5120/8000 (64%)]\tLoss: 29.764910\n",
      "Train Epoch: 228 [6400/8000 (80%)]\tLoss: 29.783909\n",
      "Train Epoch: 228 [7680/8000 (96%)]\tLoss: 29.611744\n",
      "====> Epoch: 228 Average loss: 29.7862\n",
      "Train Epoch: 229 [0/8000 (0%)]\tLoss: 29.942617\n",
      "Train Epoch: 229 [1280/8000 (16%)]\tLoss: 29.792681\n",
      "Train Epoch: 229 [2560/8000 (32%)]\tLoss: 30.087328\n",
      "Train Epoch: 229 [3840/8000 (48%)]\tLoss: 29.477980\n",
      "Train Epoch: 229 [5120/8000 (64%)]\tLoss: 30.033403\n",
      "Train Epoch: 229 [6400/8000 (80%)]\tLoss: 29.868914\n",
      "Train Epoch: 229 [7680/8000 (96%)]\tLoss: 29.716169\n",
      "====> Epoch: 229 Average loss: 29.7842\n",
      "Train Epoch: 230 [0/8000 (0%)]\tLoss: 29.842949\n",
      "Train Epoch: 230 [1280/8000 (16%)]\tLoss: 29.443062\n",
      "Train Epoch: 230 [2560/8000 (32%)]\tLoss: 29.930180\n",
      "Train Epoch: 230 [3840/8000 (48%)]\tLoss: 29.924522\n",
      "Train Epoch: 230 [5120/8000 (64%)]\tLoss: 29.934065\n",
      "Train Epoch: 230 [6400/8000 (80%)]\tLoss: 29.803854\n",
      "Train Epoch: 230 [7680/8000 (96%)]\tLoss: 29.776861\n",
      "====> Epoch: 230 Average loss: 29.7856\n",
      "Train Epoch: 231 [0/8000 (0%)]\tLoss: 29.669804\n",
      "Train Epoch: 231 [1280/8000 (16%)]\tLoss: 29.522429\n",
      "Train Epoch: 231 [2560/8000 (32%)]\tLoss: 29.734900\n",
      "Train Epoch: 231 [3840/8000 (48%)]\tLoss: 29.948675\n",
      "Train Epoch: 231 [5120/8000 (64%)]\tLoss: 29.757387\n",
      "Train Epoch: 231 [6400/8000 (80%)]\tLoss: 29.961317\n",
      "Train Epoch: 231 [7680/8000 (96%)]\tLoss: 29.759014\n",
      "====> Epoch: 231 Average loss: 29.7895\n",
      "Train Epoch: 232 [0/8000 (0%)]\tLoss: 29.567396\n",
      "Train Epoch: 232 [1280/8000 (16%)]\tLoss: 29.656466\n",
      "Train Epoch: 232 [2560/8000 (32%)]\tLoss: 29.917604\n",
      "Train Epoch: 232 [3840/8000 (48%)]\tLoss: 29.719549\n",
      "Train Epoch: 232 [5120/8000 (64%)]\tLoss: 29.628269\n",
      "Train Epoch: 232 [6400/8000 (80%)]\tLoss: 29.720911\n",
      "Train Epoch: 232 [7680/8000 (96%)]\tLoss: 29.590117\n",
      "====> Epoch: 232 Average loss: 29.7864\n",
      "Train Epoch: 233 [0/8000 (0%)]\tLoss: 29.991386\n",
      "Train Epoch: 233 [1280/8000 (16%)]\tLoss: 29.805298\n",
      "Train Epoch: 233 [2560/8000 (32%)]\tLoss: 29.631544\n",
      "Train Epoch: 233 [3840/8000 (48%)]\tLoss: 29.825111\n",
      "Train Epoch: 233 [5120/8000 (64%)]\tLoss: 29.545628\n",
      "Train Epoch: 233 [6400/8000 (80%)]\tLoss: 29.537174\n",
      "Train Epoch: 233 [7680/8000 (96%)]\tLoss: 29.919573\n",
      "====> Epoch: 233 Average loss: 29.7865\n",
      "Train Epoch: 234 [0/8000 (0%)]\tLoss: 29.710585\n",
      "Train Epoch: 234 [1280/8000 (16%)]\tLoss: 29.848009\n",
      "Train Epoch: 234 [2560/8000 (32%)]\tLoss: 29.935337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 234 [3840/8000 (48%)]\tLoss: 29.908997\n",
      "Train Epoch: 234 [5120/8000 (64%)]\tLoss: 29.866787\n",
      "Train Epoch: 234 [6400/8000 (80%)]\tLoss: 29.827703\n",
      "Train Epoch: 234 [7680/8000 (96%)]\tLoss: 29.581121\n",
      "====> Epoch: 234 Average loss: 29.7829\n",
      "Train Epoch: 235 [0/8000 (0%)]\tLoss: 29.701721\n",
      "Train Epoch: 235 [1280/8000 (16%)]\tLoss: 30.068953\n",
      "Train Epoch: 235 [2560/8000 (32%)]\tLoss: 29.704588\n",
      "Train Epoch: 235 [3840/8000 (48%)]\tLoss: 29.709436\n",
      "Train Epoch: 235 [5120/8000 (64%)]\tLoss: 29.659035\n",
      "Train Epoch: 235 [6400/8000 (80%)]\tLoss: 29.860847\n",
      "Train Epoch: 235 [7680/8000 (96%)]\tLoss: 29.674393\n",
      "====> Epoch: 235 Average loss: 29.7795\n",
      "Train Epoch: 236 [0/8000 (0%)]\tLoss: 29.649414\n",
      "Train Epoch: 236 [1280/8000 (16%)]\tLoss: 29.797354\n",
      "Train Epoch: 236 [2560/8000 (32%)]\tLoss: 29.569967\n",
      "Train Epoch: 236 [3840/8000 (48%)]\tLoss: 29.799479\n",
      "Train Epoch: 236 [5120/8000 (64%)]\tLoss: 29.751755\n",
      "Train Epoch: 236 [6400/8000 (80%)]\tLoss: 29.808723\n",
      "Train Epoch: 236 [7680/8000 (96%)]\tLoss: 29.927212\n",
      "====> Epoch: 236 Average loss: 29.7867\n",
      "Train Epoch: 237 [0/8000 (0%)]\tLoss: 29.802767\n",
      "Train Epoch: 237 [1280/8000 (16%)]\tLoss: 29.809713\n",
      "Train Epoch: 237 [2560/8000 (32%)]\tLoss: 29.971264\n",
      "Train Epoch: 237 [3840/8000 (48%)]\tLoss: 30.062252\n",
      "Train Epoch: 237 [5120/8000 (64%)]\tLoss: 30.091009\n",
      "Train Epoch: 237 [6400/8000 (80%)]\tLoss: 29.696239\n",
      "Train Epoch: 237 [7680/8000 (96%)]\tLoss: 29.686796\n",
      "====> Epoch: 237 Average loss: 29.7789\n",
      "Train Epoch: 238 [0/8000 (0%)]\tLoss: 29.942091\n",
      "Train Epoch: 238 [1280/8000 (16%)]\tLoss: 29.869230\n",
      "Train Epoch: 238 [2560/8000 (32%)]\tLoss: 29.874081\n",
      "Train Epoch: 238 [3840/8000 (48%)]\tLoss: 29.492056\n",
      "Train Epoch: 238 [5120/8000 (64%)]\tLoss: 29.541521\n",
      "Train Epoch: 238 [6400/8000 (80%)]\tLoss: 29.566746\n",
      "Train Epoch: 238 [7680/8000 (96%)]\tLoss: 29.671833\n",
      "====> Epoch: 238 Average loss: 29.7795\n",
      "Train Epoch: 239 [0/8000 (0%)]\tLoss: 29.669401\n",
      "Train Epoch: 239 [1280/8000 (16%)]\tLoss: 29.742586\n",
      "Train Epoch: 239 [2560/8000 (32%)]\tLoss: 29.932377\n",
      "Train Epoch: 239 [3840/8000 (48%)]\tLoss: 29.520676\n",
      "Train Epoch: 239 [5120/8000 (64%)]\tLoss: 29.958164\n",
      "Train Epoch: 239 [6400/8000 (80%)]\tLoss: 29.794319\n",
      "Train Epoch: 239 [7680/8000 (96%)]\tLoss: 29.799095\n",
      "====> Epoch: 239 Average loss: 29.7812\n",
      "Train Epoch: 240 [0/8000 (0%)]\tLoss: 29.777855\n",
      "Train Epoch: 240 [1280/8000 (16%)]\tLoss: 29.984015\n",
      "Train Epoch: 240 [2560/8000 (32%)]\tLoss: 30.124527\n",
      "Train Epoch: 240 [3840/8000 (48%)]\tLoss: 29.816555\n",
      "Train Epoch: 240 [5120/8000 (64%)]\tLoss: 29.581785\n",
      "Train Epoch: 240 [6400/8000 (80%)]\tLoss: 29.595078\n",
      "Train Epoch: 240 [7680/8000 (96%)]\tLoss: 29.840752\n",
      "====> Epoch: 240 Average loss: 29.7811\n",
      "Train Epoch: 241 [0/8000 (0%)]\tLoss: 29.871609\n",
      "Train Epoch: 241 [1280/8000 (16%)]\tLoss: 30.416119\n",
      "Train Epoch: 241 [2560/8000 (32%)]\tLoss: 29.441057\n",
      "Train Epoch: 241 [3840/8000 (48%)]\tLoss: 29.713381\n",
      "Train Epoch: 241 [5120/8000 (64%)]\tLoss: 29.473270\n",
      "Train Epoch: 241 [6400/8000 (80%)]\tLoss: 29.958382\n",
      "Train Epoch: 241 [7680/8000 (96%)]\tLoss: 29.443514\n",
      "====> Epoch: 241 Average loss: 29.7824\n",
      "Train Epoch: 242 [0/8000 (0%)]\tLoss: 29.694288\n",
      "Train Epoch: 242 [1280/8000 (16%)]\tLoss: 29.614141\n",
      "Train Epoch: 242 [2560/8000 (32%)]\tLoss: 29.662718\n",
      "Train Epoch: 242 [3840/8000 (48%)]\tLoss: 29.668667\n",
      "Train Epoch: 242 [5120/8000 (64%)]\tLoss: 29.634878\n",
      "Train Epoch: 242 [6400/8000 (80%)]\tLoss: 29.539070\n",
      "Train Epoch: 242 [7680/8000 (96%)]\tLoss: 29.561840\n",
      "====> Epoch: 242 Average loss: 29.7839\n",
      "Train Epoch: 243 [0/8000 (0%)]\tLoss: 29.812243\n",
      "Train Epoch: 243 [1280/8000 (16%)]\tLoss: 29.870583\n",
      "Train Epoch: 243 [2560/8000 (32%)]\tLoss: 29.919773\n",
      "Train Epoch: 243 [3840/8000 (48%)]\tLoss: 29.870756\n",
      "Train Epoch: 243 [5120/8000 (64%)]\tLoss: 29.819422\n",
      "Train Epoch: 243 [6400/8000 (80%)]\tLoss: 29.686768\n",
      "Train Epoch: 243 [7680/8000 (96%)]\tLoss: 29.688848\n",
      "====> Epoch: 243 Average loss: 29.7816\n",
      "Train Epoch: 244 [0/8000 (0%)]\tLoss: 29.695597\n",
      "Train Epoch: 244 [1280/8000 (16%)]\tLoss: 29.825523\n",
      "Train Epoch: 244 [2560/8000 (32%)]\tLoss: 30.117342\n",
      "Train Epoch: 244 [3840/8000 (48%)]\tLoss: 29.753035\n",
      "Train Epoch: 244 [5120/8000 (64%)]\tLoss: 29.848387\n",
      "Train Epoch: 244 [6400/8000 (80%)]\tLoss: 29.484886\n",
      "Train Epoch: 244 [7680/8000 (96%)]\tLoss: 29.738754\n",
      "====> Epoch: 244 Average loss: 29.7783\n",
      "Train Epoch: 245 [0/8000 (0%)]\tLoss: 29.972565\n",
      "Train Epoch: 245 [1280/8000 (16%)]\tLoss: 29.586292\n",
      "Train Epoch: 245 [2560/8000 (32%)]\tLoss: 29.980312\n",
      "Train Epoch: 245 [3840/8000 (48%)]\tLoss: 29.662064\n",
      "Train Epoch: 245 [5120/8000 (64%)]\tLoss: 29.930635\n",
      "Train Epoch: 245 [6400/8000 (80%)]\tLoss: 29.442713\n",
      "Train Epoch: 245 [7680/8000 (96%)]\tLoss: 29.562859\n",
      "====> Epoch: 245 Average loss: 29.7771\n",
      "Train Epoch: 246 [0/8000 (0%)]\tLoss: 29.789637\n",
      "Train Epoch: 246 [1280/8000 (16%)]\tLoss: 30.043253\n",
      "Train Epoch: 246 [2560/8000 (32%)]\tLoss: 29.862518\n",
      "Train Epoch: 246 [3840/8000 (48%)]\tLoss: 29.977171\n",
      "Train Epoch: 246 [5120/8000 (64%)]\tLoss: 30.024984\n",
      "Train Epoch: 246 [6400/8000 (80%)]\tLoss: 30.098099\n",
      "Train Epoch: 246 [7680/8000 (96%)]\tLoss: 29.525595\n",
      "====> Epoch: 246 Average loss: 29.7797\n",
      "Train Epoch: 247 [0/8000 (0%)]\tLoss: 29.618307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7d0fd8b53dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                       \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                       Dim = 2*D)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgradients_before_burnin\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mtrain_truncated_with_gradients\u001b[0;34m(df, model, optimizer, epoch, batch_size, Dim)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mmu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;31m# why clone detach here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# still want gradient with respect to input, but BCE gradient with respect to target is not defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mmu_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36msample_subset\u001b[0;34m(w, k, t, separate, EPSILON)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mEPSILON\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgumbel_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontinuous_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mcontinuous_topk\u001b[0;34m(w, k, t, separate, EPSILON)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# ORIGINAL: onehot_approx = tf.nn.softmax(w / t, axis=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0monehot_approx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# to see if this is flat or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gradients_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "gradient_post_burn_in = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_post_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                                      vae_gumbel_trunc_optimizer, \n",
    "                                                      epoch, \n",
    "                                                      batch_size, \n",
    "                                                      Dim = 2*D)\n",
    "    if epoch <=(n_epochs//5*4):\n",
    "        gradients_before_burnin += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_before_burnin += vae_gumbel_truncated.subset_indices.sum(dim = 0)\n",
    "    if epoch > (n_epochs//5*4):\n",
    "        gradient_post_burn_in += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_post_burnin += vae_gumbel_truncated.subset_indices.sum(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradients_before_burnin[:D].sum())\n",
    "print(gradients_before_burnin[D:].sum())\n",
    "sns.heatmap(gradients_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradient_post_burn_in[:D].sum())\n",
    "print(gradient_post_burn_in[D:].sum())\n",
    "sns.heatmap(gradient_post_burn_in.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_before_burnin[:D].sum())\n",
    "print(subset_indices_before_burnin[D:].sum())\n",
    "sns.heatmap(subset_indices_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_post_burnin[:D].sum())\n",
    "print(subset_indices_post_burnin[(D):].sum())\n",
    "sns.heatmap(subset_indices_post_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices_post_burnin[:(D)].sum() - subset_indices_post_burnin[(D):].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_truncated = VAE_Gumbel_NInstaState(2*D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "gradient_post_burn_in = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_before_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "subset_indices_post_burnin = torch.zeros(train_data.shape[1]).to(device)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients_gumbel_state(train_data, vae_gumbel_truncated, \n",
    "                                                      vae_gumbel_trunc_optimizer, \n",
    "                                                      epoch, \n",
    "                                                      batch_size, \n",
    "                                                      Dim = 2*D)\n",
    "    \n",
    "    vae_gumbel_truncated.t = max(0.01, vae_gumbel_truncated.t * 0.99)\n",
    "    if epoch <=(n_epochs//5*4):\n",
    "        gradients_before_burnin += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_before_burnin += sample_subset(vae_gumbel_truncated.logit_enc, \n",
    "                                                          vae_gumbel_truncated.k, \n",
    "                                                          vae_gumbel_truncated.t).view(-1)\n",
    "    if epoch == (n_epochs//5*4):\n",
    "        print(\"BURN IN DEBUG\")\n",
    "        vae_gumbel_truncated.set_burned_in()\n",
    "        #vae_gumbel_truncated.t /= 10\n",
    "        print(\"Going post burn in\")\n",
    "    if epoch > (n_epochs//5*4):\n",
    "        gradient_post_burn_in += grads\n",
    "        with torch.no_grad():\n",
    "            subset_indices_post_burnin += sample_subset(vae_gumbel_truncated.logit_enc, \n",
    "                                                        vae_gumbel_truncated.k, \n",
    "                                                        vae_gumbel_truncated.t).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradients_before_burnin[:(D)].mean())\n",
    "print(gradients_before_burnin[(D):].mean())\n",
    "sns.heatmap(gradients_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradient_post_burn_in[:D].mean())\n",
    "print(gradient_post_burn_in[D:].mean())\n",
    "sns.heatmap(gradient_post_burn_in.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_before_burnin[:D].sum())\n",
    "print(subset_indices_before_burnin[D:].sum())\n",
    "sns.heatmap(subset_indices_before_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_indices_post_burnin[:D].sum())\n",
    "print(subset_indices_post_burnin[D:].sum())\n",
    "sns.heatmap(subset_indices_post_burnin.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(subset_indices_post_burnin[:(D)] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(subset_indices_post_burnin[(D):] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices_post_burnin[:(D)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices_post_burnin[(D):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices_post_burnin[:(D)].sum() - subset_indices_post_burnin[(D):].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ind = torch.argsort(sample_subset(vae_gumbel_truncated.logit_enc, \n",
    "                                                        vae_gumbel_truncated.k, \n",
    "                                                        vae_gumbel_truncated.t).view(-1), \n",
    "                        descending = True)[:vae_gumbel_truncated.k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(top_ind < 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(top_ind >= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
