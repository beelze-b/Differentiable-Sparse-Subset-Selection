{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just get a quick sparsity overview of the methods so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import gc\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_PATH_DATA = '../data/'\n",
    "BASE_PATH_DATA = '/scratch/ns3429/sparse-subset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "\n",
    "z_size = 50\n",
    "hidden_size = 100\n",
    "\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/zeisel_data.mat')\n",
    "data= a['zeisel_data'].T\n",
    "N,d=data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(d):\n",
    "    #data[i,:]=data[i,:]/np.linalg.norm(data[i,:])\n",
    "    #mi = np.mean(data[:,i])\n",
    "    #std = np.std(data[:,i])\n",
    "    #data[:,i] = (data[:,i] - mi) / std\n",
    "    ma = np.max(data[:,i])\n",
    "    mi = np.min(data[:,i])\n",
    "    data[:, i] = (data[:, i] - mi) / (ma - mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09018117926614051"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data!=0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))\n",
    "\n",
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2263, device='cuda:0')\n",
      "tensor(0.2223, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.std(dim = 0).mean())\n",
    "print(test_data.std(dim = 0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does L1 work if we normalize after every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_diag = VAE_l1_diag(input_size, hidden_size, z_size)\n",
    "\n",
    "model_l1_diag.to(device)\n",
    "model_l1_optimizer = torch.optim.Adam(model_l1_diag.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2404 (0%)]\tLoss: 7817.977539\n",
      "Train Epoch: 1 [1280/2404 (53%)]\tLoss: 2768.614990\n",
      "====> Epoch: 1 Average loss: 2921.7163\n",
      "====> Test set loss: 2625.1514\n",
      "Train Epoch: 2 [0/2404 (0%)]\tLoss: 2687.939209\n",
      "Train Epoch: 2 [1280/2404 (53%)]\tLoss: 2623.095215\n",
      "====> Epoch: 2 Average loss: 2630.7148\n",
      "====> Test set loss: 2477.6820\n",
      "Train Epoch: 3 [0/2404 (0%)]\tLoss: 2569.455322\n",
      "Train Epoch: 3 [1280/2404 (53%)]\tLoss: 2534.185547\n",
      "====> Epoch: 3 Average loss: 2487.6389\n",
      "====> Test set loss: 2338.7393\n",
      "Train Epoch: 4 [0/2404 (0%)]\tLoss: 2387.044434\n",
      "Train Epoch: 4 [1280/2404 (53%)]\tLoss: 2385.619141\n",
      "====> Epoch: 4 Average loss: 2365.5742\n",
      "====> Test set loss: 2227.5020\n",
      "Train Epoch: 5 [0/2404 (0%)]\tLoss: 2374.028564\n",
      "Train Epoch: 5 [1280/2404 (53%)]\tLoss: 2233.293945\n",
      "====> Epoch: 5 Average loss: 2271.0662\n",
      "====> Test set loss: 2131.6926\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_l1(train_data, model_l1_diag, model_l1_optimizer, epoch, batch_size)\n",
    "        test(test_data, model_l1_diag, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   2,   13,   98,  831, 1299,  180,  521, 1055,    1,    0]), array([1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,\n",
      "       1.e-01, 1.e+00, 1.e+01]))\n"
     ]
    }
   ],
   "source": [
    "bins = [10**(-i) for i in range(10)]\n",
    "bins.reverse()\n",
    "bins += [10]\n",
    "print(np.histogram(model_l1_diag.diag.abs().clone().detach().cpu().numpy(), bins = bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Neuron Loss Train\n",
      "tensor(0.6830, device='cuda:0')\n",
      "Per Neuron Loss Test\n",
      "tensor(0.6499, device='cuda:0')\n",
      "# Non Sparse in Pred test\n",
      "tensor(3907, device='cuda:0')\n",
      "# Non Sparse in Orig test\n",
      "tensor(1105, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "quick_model_summary(model_l1_diag, train_data, test_data, 0.15, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0523, 0.0615, 0.0488,  ..., 0.0624, 0.0702, 0.0525], device='cuda:0',\n",
       "       grad_fn=<StdBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l1_diag(test_data[0:64])[0].std(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0921, 0.1961, 0.1590,  ..., 0.2199, 0.2041, 0.2381], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0:64].std(dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First try Pretrained VAE and then gumble trick with it**\n",
    "\n",
    "**Then try joint training VAE and Gumbel Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain VAE First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae = VAE(input_size, hidden_size, z_size)\n",
    "\n",
    "pretrain_vae.to(device)\n",
    "pretrain_vae_optimizer = torch.optim.Adam(pretrain_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2404 (0%)]\tLoss: 2800.429688\n",
      "Train Epoch: 1 [1280/2404 (53%)]\tLoss: 2697.386475\n",
      "====> Epoch: 1 Average loss: 2714.9719\n",
      "====> Test set loss: 2623.7230\n",
      "Train Epoch: 2 [0/2404 (0%)]\tLoss: 2637.507812\n",
      "Train Epoch: 2 [1280/2404 (53%)]\tLoss: 2553.158936\n",
      "====> Epoch: 2 Average loss: 2556.2984\n",
      "====> Test set loss: 2460.1716\n",
      "Train Epoch: 3 [0/2404 (0%)]\tLoss: 2462.025146\n",
      "Train Epoch: 3 [1280/2404 (53%)]\tLoss: 2350.075928\n",
      "====> Epoch: 3 Average loss: 2381.1461\n",
      "====> Test set loss: 2266.9870\n",
      "Train Epoch: 4 [0/2404 (0%)]\tLoss: 2252.049072\n",
      "Train Epoch: 4 [1280/2404 (53%)]\tLoss: 2157.988770\n",
      "====> Epoch: 4 Average loss: 2211.9644\n",
      "====> Test set loss: 2112.1003\n",
      "Train Epoch: 5 [0/2404 (0%)]\tLoss: 2097.256348\n",
      "Train Epoch: 5 [1280/2404 (53%)]\tLoss: 2128.326904\n",
      "====> Epoch: 5 Average loss: 2115.0678\n",
      "====> Test set loss: 2060.2754\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(train_data, pretrain_vae, pretrain_vae_optimizer, epoch, batch_size)\n",
    "        test(test_data, pretrain_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Neuron Loss Train\n",
      "tensor(0.8806, device='cuda:0')\n",
      "Per Neuron Loss Test\n",
      "tensor(1.0009, device='cuda:0')\n",
      "# Non Sparse in Pred test\n",
      "tensor(2078, device='cuda:0')\n",
      "# Non Sparse in Orig test\n",
      "tensor(1105, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "quick_model_summary(pretrain_vae, train_data, test_data, 0.15, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7510, 0.5357, 0.6021,  ..., 0.1179, 0.1706, 0.1566],\n",
       "        [0.7499, 0.6199, 0.7235,  ..., 0.1133, 0.1100, 0.1681],\n",
       "        [0.5690, 0.6056, 0.5254,  ..., 0.4379, 0.4482, 0.4816],\n",
       "        ...,\n",
       "        [0.5294, 0.5845, 0.5310,  ..., 0.2770, 0.3147, 0.3973],\n",
       "        [0.8161, 0.6126, 0.6848,  ..., 0.1169, 0.1500, 0.1527],\n",
       "        [0.6444, 0.4988, 0.5600,  ..., 0.1190, 0.2083, 0.2042]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data[0:64])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pretrain_vae.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=4000, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (enc_mean): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (enc_logvar): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=4000, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gumbel with the Pre-Trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_with_pre = VAE_Gumbel(input_size, hidden_size, z_size, k = 50)\n",
    "vae_gumbel_with_pre.to(device)\n",
    "vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2404 (0%)]\tLoss: 2888.951904\n",
      "Train Epoch: 1 [1280/2404 (53%)]\tLoss: 2808.739746\n",
      "====> Epoch: 1 Average loss: 2814.0636\n",
      "====> Test set loss: 2631.9332\n",
      "Train Epoch: 2 [0/2404 (0%)]\tLoss: 2719.049072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6591d8fb24ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, \n\u001b[0;32m----> 3\u001b[0;31m                           epoch, pretrain_vae, batch_size)\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_gumbel_with_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mtrain_pre_trained\u001b[0;34m(df, model, optimizer, epoch, pretrained_model, batch_size)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mmu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_latent_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mmu_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0msubset_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubset_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36msample_subset\u001b[0;34m(w, k, t)\u001b[0m\n\u001b[1;32m    136\u001b[0m     '''\n\u001b[1;32m    137\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgumbel_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontinuous_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/Differentiable-Sparse-Subset-Selection/notebooks/utils.py\u001b[0m in \u001b[0;36mcontinuous_topk\u001b[0;34m(w, k, t, separate)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mmax_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0monehot_approx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mkhot_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0monehot_approx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mkhot_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkhot_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, \n",
    "                          epoch, pretrain_vae, batch_size)\n",
    "        test(test_data, vae_gumbel_with_pre, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_model_summary(vae_gumbel_with_pre, train_data, test_data, 0.15, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(input_size, hidden_size, z_size)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(input_size, hidden_size, z_size, k = 50)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_model_summary(joint_vae_gumbel, train_data, test_data, 0.15, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del joint_vanilla_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's actually Graph this.\n",
    "\n",
    "### Try it out at Gumbel sparsity of k = 10, 25, 50, 100, 250\n",
    "\n",
    "### Graph Test MSE Loss\n",
    "\n",
    "## Graph the mean activations at k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_activations(test_data, model, title, file):\n",
    "    preds, _, _ = model(test_data)\n",
    "    \n",
    "    preds[preds < 0.09] = 0\n",
    "    pred_activations = preds.mean(dim = 0)\n",
    "    \n",
    "    test_activations = test_data.mean(dim = 0)\n",
    "    \n",
    "    x = np.arange(input_size) + 1\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, pred_activations.clone().detach().cpu().numpy(), label = 'Average Predictions')\n",
    "    plt.plot(x, test_activations.clone().detach().cpu().numpy(), label = 'Average Test Data')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Average Activation of Feature\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_sparsity(test_data, model, title, file):\n",
    "    preds, _, _ = model(test_data)\n",
    "    \n",
    "    preds[preds < 0.15] = 0\n",
    "    preds[preds >= 0.15] = 1\n",
    "    \n",
    "    pred_count = preds.sum(dim = 0) / len(test_data)\n",
    "    \n",
    "    test_count = test_data.sum(dim = 0) / len(test_data)\n",
    "    \n",
    "    x = np.arange(input_size) + 1\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, pred_count.clone().detach().cpu().numpy(), label = 'Count NonZero Predictions')\n",
    "    plt.plot(x, test_count.clone().detach().cpu().numpy(), label = 'Count NonZero Test Data')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Proportion of Test Set Feature Was not Sparse\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_activations(test_data, model_l1_diag, 'Joint Gumbel vs Test Means', \n",
    "                  BASE_PATH_DATA + 'vae_l1_activations.png')\n",
    "graph_sparsity(test_data, model_l1_diag, 'Joint Gumbel vs Test Sparsity', \n",
    "                  BASE_PATH_DATA + 'vae_l1_sparsity.png')\n",
    "\n",
    "del model_l1_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_activations(test_data, joint_vae_gumbel, 'Joint Gumbel vs Test Means', \n",
    "                  BASE_PATH_DATA + 'joint_gumbel_activations.png')\n",
    "graph_sparsity(test_data, joint_vae_gumbel, 'Joint Gumbel vs Test Sparsity', \n",
    "                  BASE_PATH_DATA + 'joint_gumbel_sparsity.png')\n",
    "\n",
    "del joint_vae_gumbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_activations(test_data, vae_gumbel_with_pre, 'Gumbel Matching Pretrained VAE vs Test Means', \n",
    "                  BASE_PATH_DATA + 'pretrained_gumbel_activations.png')\n",
    "graph_sparsity(test_data, vae_gumbel_with_pre, 'Gumbel Matching Pretrained VAE vs Test Sparsity', \n",
    "                  BASE_PATH_DATA + 'pretrained_gumbel_sparsity.png')\n",
    "\n",
    "del vae_gumbel_with_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_all = [5, 10, 25, 50, 75, 100, 150]#, 250, 500, 1000, 2000, 3000]\n",
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_pre = []\n",
    "losses_joint = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_all:\n",
    "    current_k_pre_losses = []\n",
    "    current_k_joint_losses = []\n",
    "    for trial_i in range(n_trials):\n",
    "        print(\"RUNNING for K {} Trial {}\".format(k, trial_i), flush=True)\n",
    "        vae_gumbel_with_pre = VAE_Gumbel(input_size, hidden_size, z_size, k = k)\n",
    "        vae_gumbel_with_pre.to(device)\n",
    "        vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                        lr=lr, \n",
    "                                                        betas = (b1,b2))\n",
    "    \n",
    "        joint_vanilla_vae = VAE(input_size, hidden_size, z_size)\n",
    "        joint_vanilla_vae.to(device)\n",
    "\n",
    "        joint_vae_gumbel = VAE_Gumbel(input_size, hidden_size, z_size, k = k)\n",
    "        joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "        joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + \n",
    "                                           list(joint_vae_gumbel.parameters()),\n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "    \n",
    "        for epoch in (1, n_epochs + 1):\n",
    "            train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, \n",
    "                              epoch, pretrain_vae, batch_size)\n",
    "            train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    \n",
    "        test_loss_pre = 0\n",
    "        test_loss_joint = 0\n",
    "        \n",
    "        inds = np.arange(test_data.shape[0])\n",
    "        with torch.no_grad():\n",
    "            for i in range(math.ceil(len(test_data)/batch_size)):\n",
    "                batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "                batch_data = test_data[batch_ind, :]\n",
    "                \n",
    "                test_pred_pre = vae_gumbel_with_pre(batch_data)[0]\n",
    "                test_pred_joint = joint_vae_gumbel(batch_data)[0]\n",
    "                \n",
    "                test_pred_pre[test_pred_pre < 0.09] = 0\n",
    "                test_pred_joint[test_pred_joint < 0.09] = 0\n",
    "                \n",
    "                test_loss_pre += F.binary_cross_entropy(test_pred_pre, batch_data, reduction='mean')\n",
    "                test_loss_joint += F.binary_cross_entropy(test_pred_joint, batch_data, reduction='mean')\n",
    "                \n",
    "                del batch_data\n",
    "            \n",
    "        #test_loss_pre /= len(test_data)\n",
    "        #test_loss_joint /= len(test_data)\n",
    "        current_k_pre_losses.append(test_loss_pre.cpu().item())\n",
    "        current_k_joint_losses.append(test_loss_joint.cpu().item())\n",
    "        \n",
    "        # for freeing memory faster\n",
    "        del vae_gumbel_with_pre\n",
    "        del vae_gumbel_with_pre_optimizer\n",
    "        del joint_vanilla_vae\n",
    "        del joint_vae_gumbel\n",
    "        del joint_optimizer\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "    losses_pre.append(np.mean(current_k_pre_losses))\n",
    "    losses_joint.append(np.mean(current_k_joint_losses))\n",
    "    \n",
    "    \n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(k_all, losses_pre, label = 'Average BCE Losses with Gumbel Matching Pretrained')\n",
    "plt.plot(k_all, losses_joint, label = 'Average BCE Losses with Gumbel Joint Training')\n",
    "\n",
    "plt.title(\"Effect on Sparsity on BCE Loss\")\n",
    "plt.xlabel('Sparsity Level (Number of Non-Zero Features)')\n",
    "plt.ylabel('Per Neuron Average BCE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('/scratch/ns3429/sparse-subset/comparing_across_sparsity.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
