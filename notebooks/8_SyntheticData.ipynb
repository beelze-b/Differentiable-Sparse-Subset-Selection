{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know that Gumbel selects things relatively well. Its effects of Zeisel though are a bit muddled because of reconstruction. Let's do a simple synethetic dataset. Half the features are real. Half the features at noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_PATH_DATA = '../data/'\n",
    "BASE_PATH_DATA = '/scratch/ns3429/sparse-subset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "#n_epochs = 50\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 5000\n",
    "z_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=False)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=10, out_features=30, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7641,  0.4002,  0.9787,  2.2409,  1.8676],\n",
       "        [-0.9773,  0.9501, -0.1514, -0.1032,  0.4106],\n",
       "        [ 0.1440,  1.4543,  0.7610,  0.1217,  0.4439],\n",
       "        ...,\n",
       "        [ 0.2501, -1.0168,  0.0459,  0.5006,  1.2243],\n",
       "        [-0.5595,  1.5234, -0.5857,  0.8466, -0.1063],\n",
       "        [ 0.7700,  0.7508, -0.5606, -1.7603,  0.4371]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = data_mapper(latent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(18, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample, half the data is non zero, whereas in zeisel, about 25% if non zero. Easier than Zeisel good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0921e-02, -6.1085e-04, -1.4928e-02,  ..., -1.4309e-02,\n",
       "          1.6859e-02, -1.2177e-02],\n",
       "        [ 7.6496e-03,  1.1971e-02, -2.2414e-02,  ...,  1.0256e-02,\n",
       "         -5.5957e-03,  4.3434e-03],\n",
       "        [ 2.7566e-03,  1.0969e-03,  3.5942e-03,  ...,  6.0039e-03,\n",
       "          8.7524e-04,  7.0365e-03],\n",
       "        ...,\n",
       "        [ 1.8449e-02,  8.3797e-04, -8.9499e-03,  ...,  8.9735e-04,\n",
       "         -1.6982e-03,  7.8153e-03],\n",
       "        [-1.0649e-02, -9.6204e-03, -8.1562e-03,  ..., -2.2612e-04,\n",
       "         -1.4104e-02, -8.2127e-03],\n",
       "        [ 2.1183e-02, -1.1416e-02,  1.8769e-03,  ..., -1.3100e-02,\n",
       "         -6.2333e-03, -4.3646e-05]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_features = torch.empty(N * D).normal_(mean=0,std=0.01).reshape(N, D).to(device)\n",
    "noise_features.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = torch.cat([actual_data, noise_features], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 60])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1866, 0.2313, 0.2054, 0.2209, 0.2323, 0.1899, 0.1801, 0.1969, 0.1133,\n",
       "        0.2353, 0.0925, 0.1310, 0.1725, 0.1902, 0.2294, 0.2275, 0.2082, 0.0530,\n",
       "        0.0980, 0.1738, 0.1728, 0.2156, 0.0460, 0.0932, 0.0255, 0.1816, 0.1587,\n",
       "        0.2263, 0.2125, 0.2393, 0.1326, 0.1439, 0.1281, 0.1421, 0.1297, 0.1413,\n",
       "        0.1492, 0.1272, 0.1233, 0.1420, 0.1422, 0.1363, 0.1256, 0.1288, 0.1377,\n",
       "        0.1438, 0.1337, 0.1331, 0.1258, 0.1346, 0.1507, 0.1223, 0.1429, 0.1343,\n",
       "        0.1348, 0.1361, 0.1388, 0.1391, 0.1426, 0.1383], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.std(dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviatiosn are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0'),\n",
       "indices=tensor([ 738, 4262, 1553, 1484, 2220, 1316, 3892, 1316, 3121, 3883, 1838,  623,\n",
       "        1004, 4856,  689, 2033, 2038, 1316, 4515, 4562, 4668,  616,  894, 4515,\n",
       "        1885, 3892, 4615,  819, 4397, 4293,  713, 2220, 3813, 4659, 4389, 3659,\n",
       "         309, 1804,  495, 4790, 3110, 4671,   36,    0, 1215,  148, 4008, 1317,\n",
       "        2503, 1402, 1580, 2684, 4078, 3334, 1376, 2499, 1301, 3114, 4203, 3183],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.max(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'),\n",
       "indices=tensor([   1,    8,    0,    1,    0,    5,    1,    1,    1,    0,    1,    1,\n",
       "           0,   13,    3,    3,    1,    0,    0,    1,    1,    8,    0,    0,\n",
       "           0,    1,  254,    4,    0,    0,  454, 3677, 1909, 3750, 3638, 4476,\n",
       "         553, 4105,  289, 1150, 2707, 1846, 3579, 3101,  299, 1324, 3277, 4318,\n",
       "        3023,  967, 2932, 3588,  919, 1190,  271, 2937, 3428, 3955, 2719,  198],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.min(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49, device='cuda:0')\n",
      "tensor(42, device='cuda:0')\n",
      "tensor(48, device='cuda:0')\n",
      "tensor(44, device='cuda:0')\n",
      "tensor(44, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 60])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 60])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is all ready. Now time to feed into into a pretraining-matching Gumbel and joint training Gumbel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre train VAE First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae = VAE(2*D, 100, 20)\n",
    "\n",
    "pretrain_vae.to(device)\n",
    "pretrain_vae_optimizer = torch.optim.Adam(pretrain_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))\n",
    "\n",
    "#pretrain_vae_optimizer = torch.optim.SGD(pretrain_vae.parameters(), \n",
    "#                                            lr=lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 42.017113\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 40.903053\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 39.910145\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 39.210461\n",
      "====> Epoch: 1 Average loss: 40.4244\n",
      "====> Test set loss: 38.9524\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 38.888924\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.212513\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 36.419609\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 35.664536\n",
      "====> Epoch: 2 Average loss: 37.3049\n",
      "====> Test set loss: 35.4656\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 35.430317\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 34.606815\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 34.577702\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.209206\n",
      "====> Epoch: 3 Average loss: 34.6850\n",
      "====> Test set loss: 34.1042\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.345818\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 34.207222\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 33.563995\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.908337\n",
      "====> Epoch: 4 Average loss: 33.8812\n",
      "====> Test set loss: 33.6500\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 33.996525\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.835045\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.518723\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.168392\n",
      "====> Epoch: 5 Average loss: 33.5430\n",
      "====> Test set loss: 33.3353\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 33.442101\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.615280\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.352787\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 33.062305\n",
      "====> Epoch: 6 Average loss: 33.3164\n",
      "====> Test set loss: 33.1817\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 33.309429\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 33.084484\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 33.366402\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 33.056320\n",
      "====> Epoch: 7 Average loss: 33.1319\n",
      "====> Test set loss: 33.0265\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 33.327454\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 33.176674\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 33.073765\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 32.693161\n",
      "====> Epoch: 8 Average loss: 33.0185\n",
      "====> Test set loss: 32.9396\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 32.457218\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 32.928253\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 32.993736\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 32.937096\n",
      "====> Epoch: 9 Average loss: 32.9161\n",
      "====> Test set loss: 32.8299\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 32.924587\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 32.558769\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 32.684296\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 33.207237\n",
      "====> Epoch: 10 Average loss: 32.8035\n",
      "====> Test set loss: 32.6680\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 32.653099\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 32.831394\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 32.375183\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 32.626377\n",
      "====> Epoch: 11 Average loss: 32.5824\n",
      "====> Test set loss: 32.3997\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 32.514385\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 32.200542\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 32.589878\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 32.178410\n",
      "====> Epoch: 12 Average loss: 32.3290\n",
      "====> Test set loss: 32.1813\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 32.289268\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 32.206417\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 32.278069\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 32.369549\n",
      "====> Epoch: 13 Average loss: 32.1558\n",
      "====> Test set loss: 32.0547\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 31.900909\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 32.196495\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 32.229164\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 32.197395\n",
      "====> Epoch: 14 Average loss: 32.0321\n",
      "====> Test set loss: 31.9535\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 31.856623\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 31.696789\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 31.695568\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 31.998665\n",
      "====> Epoch: 15 Average loss: 31.9635\n",
      "====> Test set loss: 31.8698\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 31.955343\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 31.983461\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 32.110096\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 32.054447\n",
      "====> Epoch: 16 Average loss: 31.9013\n",
      "====> Test set loss: 31.8082\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 31.771000\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 32.020573\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 31.843025\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 31.751513\n",
      "====> Epoch: 17 Average loss: 31.8380\n",
      "====> Test set loss: 31.7816\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 31.683304\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 31.588379\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 31.705652\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 31.933746\n",
      "====> Epoch: 18 Average loss: 31.7766\n",
      "====> Test set loss: 31.6849\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 32.360500\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 31.724262\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 31.859814\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 31.709867\n",
      "====> Epoch: 19 Average loss: 31.7122\n",
      "====> Test set loss: 31.6376\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 31.779974\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 31.284533\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 31.391958\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 31.349340\n",
      "====> Epoch: 20 Average loss: 31.6303\n",
      "====> Test set loss: 31.5404\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(train_data, pretrain_vae, pretrain_vae_optimizer, epoch, batch_size)\n",
    "        test(test_data, pretrain_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5172, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(pretrain_vae(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually pretty good! %35 percent off when wrong\n",
    "\n",
    "Get 0.49 when nepochs is 50.\n",
    "Get 0.54 when nepochs is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, if the final layer of the data mapper is not ReLU, this reconstruction is usually on point. When some of the features can be sparse, then this becomes troublesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0931, 0.3051, 0.1384, 0.2096, 0.2092, 0.2112, 0.1042, 0.2325, 0.0347,\n",
       "        0.2014, 0.0244, 0.0502, 0.0919, 0.2735, 0.2254, 0.3294, 0.2786, 0.0084,\n",
       "        0.0338, 0.1001, 0.0849, 0.2512, 0.0048, 0.0257, 0.0015, 0.0874, 0.4764,\n",
       "        0.3270, 0.2107, 0.2899], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0974, 0.3121, 0.1461, 0.2162, 0.2220, 0.2156, 0.1141, 0.2397, 0.0427,\n",
       "        0.2098, 0.0319, 0.0692, 0.1002, 0.2707, 0.2366, 0.3450, 0.2797, 0.0256,\n",
       "        0.0445, 0.0980, 0.1043, 0.2610, 0.0198, 0.0355, 0.0178, 0.1067, 0.4767,\n",
       "        0.3174, 0.2263, 0.2916], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1150, 0.0651, 0.1637, 0.1249, 0.0813, 0.0640, 0.1193, 0.1396, 0.0550,\n",
       "        0.1004, 0.0389, 0.0728, 0.0831, 0.1564, 0.2018, 0.1958, 0.0672, 0.0400,\n",
       "        0.0595, 0.1027, 0.1094, 0.1252, 0.0288, 0.0476, 0.0292, 0.1376, 0.0661,\n",
       "        0.1911, 0.0611, 0.1617], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0227, 0.0189, 0.0205, 0.0207, 0.0211, 0.0221, 0.0240, 0.0220, 0.0226,\n",
       "        0.0221, 0.0231, 0.0189, 0.0223, 0.0205, 0.0187, 0.0195, 0.0201, 0.0195,\n",
       "        0.0187, 0.0216, 0.0235, 0.0188, 0.0225, 0.0241, 0.0205, 0.0223, 0.0199,\n",
       "        0.0221, 0.0198, 0.0199], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].std(dim = 0)[D:2*D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_std = pretrain_vae(test_data)[0].std(dim = 0)[:D] / test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5937, 0.2845, 0.7928, 0.5789, 0.3512, 0.3359, 0.6825, 0.7307, 0.5134,\n",
      "        0.4415, 0.3738, 0.5347, 0.4935, 0.8127, 0.8684, 0.8394, 0.3234, 1.0021,\n",
      "        0.5954, 0.5871, 0.6287, 0.5625, 0.5662, 0.4930, 1.7515, 0.7608, 0.4121,\n",
      "        0.8517, 0.2969, 0.6839], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0.6247512102127075\n"
     ]
    }
   ],
   "source": [
    "print(average_std)\n",
    "print(average_std.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get .8 as the mean when nepoch is 50. Get 0.43 as the mean when nepochs is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0260, 0.4043, 0.0297, 0.2576, 0.3167, 0.2183, 0.1417, 0.3313, 0.0390,\n",
       "        0.1503, 0.0259, 0.0783, 0.0397, 0.1553, 0.3596, 0.5376, 0.1995, 0.0185,\n",
       "        0.0231, 0.0407, 0.1605, 0.3890, 0.0124, 0.0281, 0.0093, 0.1379, 0.4134,\n",
       "        0.1563, 0.2425, 0.1784], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1230, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(test_data[samp,:D] - pretrain_vae(test_data)[0][samp, :D]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0455, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[1][:, :D].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7776, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(pretrain_vae(test_data)[2][:, :D]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=60, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (enc_mean): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (enc_logvar): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=60, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good.\n",
    "\n",
    "**Gumbel matching pretrained VAE next**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_with_pre = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "vae_gumbel_with_pre.to(device)\n",
    "vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 130.784912\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 121.390564\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 129.267868\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 117.989792\n",
      "====> Epoch: 1 Average loss: 126.1919\n",
      "====> Test set loss: 39.1105\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 116.514961\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 121.917839\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 111.255531\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 95.695114\n",
      "====> Epoch: 2 Average loss: 109.5175\n",
      "====> Test set loss: 36.7800\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 99.622375\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 100.058678\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 93.236115\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 101.708130\n",
      "====> Epoch: 3 Average loss: 94.6618\n",
      "====> Test set loss: 34.9128\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 95.807892\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 96.661804\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 89.888412\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 77.761230\n",
      "====> Epoch: 4 Average loss: 90.6348\n",
      "====> Test set loss: 34.0960\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 88.567017\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 89.231331\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 92.143478\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 75.551392\n",
      "====> Epoch: 5 Average loss: 85.0320\n",
      "====> Test set loss: 33.6748\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 76.805161\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 67.497597\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 69.265137\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 60.975185\n",
      "====> Epoch: 6 Average loss: 70.3975\n",
      "====> Test set loss: 33.3098\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 61.496159\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 51.532063\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 52.865746\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 55.354939\n",
      "====> Epoch: 7 Average loss: 54.9485\n",
      "====> Test set loss: 32.9794\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 53.701008\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 51.633751\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 45.060623\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 49.310387\n",
      "====> Epoch: 8 Average loss: 50.1609\n",
      "====> Test set loss: 32.6813\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 46.983559\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 47.651196\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 45.054256\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 47.104748\n",
      "====> Epoch: 9 Average loss: 47.6164\n",
      "====> Test set loss: 32.4655\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 45.507896\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 47.334778\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 43.757832\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 44.674210\n",
      "====> Epoch: 10 Average loss: 45.8215\n",
      "====> Test set loss: 32.2971\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 42.853233\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 44.995338\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 43.451370\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 43.608524\n",
      "====> Epoch: 11 Average loss: 44.7431\n",
      "====> Test set loss: 32.1836\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 42.164841\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 43.697994\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 41.391148\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 42.241985\n",
      "====> Epoch: 12 Average loss: 43.9962\n",
      "====> Test set loss: 32.0745\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 43.761005\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 42.539288\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 43.898903\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 44.181370\n",
      "====> Epoch: 13 Average loss: 43.3767\n",
      "====> Test set loss: 31.9876\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 43.539627\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 42.551140\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 41.845863\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 43.745991\n",
      "====> Epoch: 14 Average loss: 42.6998\n",
      "====> Test set loss: 31.9468\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 43.020912\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 40.825523\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 43.408092\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 40.227039\n",
      "====> Epoch: 15 Average loss: 42.4021\n",
      "====> Test set loss: 31.8453\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 42.638771\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 42.158089\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 41.197655\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 41.658890\n",
      "====> Epoch: 16 Average loss: 41.7994\n",
      "====> Test set loss: 31.8061\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 42.927414\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 41.903210\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 40.837509\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 42.501205\n",
      "====> Epoch: 17 Average loss: 41.5041\n",
      "====> Test set loss: 31.7434\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 39.521690\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 39.965504\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 43.502605\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 43.571281\n",
      "====> Epoch: 18 Average loss: 41.3281\n",
      "====> Test set loss: 31.7440\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 39.712669\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 39.640720\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 42.744873\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 42.403976\n",
      "====> Epoch: 19 Average loss: 40.7439\n",
      "====> Test set loss: 31.7066\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 39.756660\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 39.203533\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 40.655067\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 40.476974\n",
      "====> Epoch: 20 Average loss: 40.5974\n",
      "====> Test set loss: 31.6715\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, \n",
    "                      epoch, pretrain_vae, batch_size)\n",
    "    test(test_data, vae_gumbel_with_pre, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5209, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(vae_gumbel_with_pre(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1026, 0.3113, 0.1555, 0.2103, 0.2154, 0.2139, 0.1029, 0.2296, 0.0346,\n",
       "        0.2128, 0.0285, 0.0556, 0.1053, 0.2863, 0.2217, 0.3214, 0.2817, 0.0200,\n",
       "        0.0359, 0.0998, 0.0931, 0.2494, 0.0175, 0.0319, 0.0153, 0.0941, 0.4700,\n",
       "        0.3410, 0.2185, 0.3018], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1153, 0.0518, 0.1646, 0.1163, 0.0842, 0.0573, 0.1136, 0.1312, 0.0339,\n",
       "        0.0973, 0.0276, 0.0546, 0.0667, 0.1537, 0.1882, 0.1878, 0.0449, 0.0208,\n",
       "        0.0360, 0.0927, 0.0970, 0.1194, 0.0200, 0.0306, 0.0193, 0.1121, 0.0552,\n",
       "        0.1921, 0.0659, 0.1561], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0356, 0.3265, 0.0356, 0.2972, 0.3427, 0.2824, 0.1551, 0.3338, 0.0507,\n",
       "        0.1601, 0.0465, 0.1108, 0.0659, 0.1482, 0.4148, 0.5075, 0.3063, 0.0286,\n",
       "        0.0437, 0.0535, 0.1857, 0.3813, 0.0241, 0.0340, 0.0231, 0.2043, 0.4821,\n",
       "        0.1731, 0.2410, 0.1717], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_with_pre.weight_creator(test_data[0:2, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([ 3, 15, 19, 21, 35, 47, 49, 50, 51, 56, 59,  1,  5,  6,  8, 14, 15,\n",
       "        19, 30, 38, 49]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as long as feature index is lesss than 30, then it isn't picking noise\n",
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3 * z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(2*D, 100, 20)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 84.270432\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 81.778725\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 79.940834\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 78.115456\n",
      "====> Epoch: 1 Average loss: 80.8498\n",
      "====> Test set loss: 78.6082\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 77.901146\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 76.610268\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 75.406212\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 73.791656\n",
      "====> Epoch: 2 Average loss: 75.8394\n",
      "====> Test set loss: 75.3927\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 73.369461\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 72.081383\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 70.888252\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 69.977753\n",
      "====> Epoch: 3 Average loss: 71.5442\n",
      "====> Test set loss: 74.9460\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 69.239731\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 69.389435\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 68.216423\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 68.369049\n",
      "====> Epoch: 4 Average loss: 68.7228\n",
      "====> Test set loss: 74.6494\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 67.667618\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 67.035492\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 67.293304\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 66.824974\n",
      "====> Epoch: 5 Average loss: 67.4864\n",
      "====> Test set loss: 75.0293\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 66.697289\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 66.655907\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 67.141121\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 68.063660\n",
      "====> Epoch: 6 Average loss: 66.8799\n",
      "====> Test set loss: 72.3640\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 66.712524\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 66.332222\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 66.332367\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 67.051613\n",
      "====> Epoch: 7 Average loss: 66.5198\n",
      "====> Test set loss: 70.8693\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 65.215546\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 66.364113\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 66.137955\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 66.726509\n",
      "====> Epoch: 8 Average loss: 66.3276\n",
      "====> Test set loss: 70.2999\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 67.311226\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 66.668671\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 65.379501\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 66.836853\n",
      "====> Epoch: 9 Average loss: 66.1376\n",
      "====> Test set loss: 69.8311\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 65.662796\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 65.627083\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 65.630081\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 66.404152\n",
      "====> Epoch: 10 Average loss: 66.0242\n",
      "====> Test set loss: 69.7916\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 66.526512\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 65.810905\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 65.856934\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 65.307800\n",
      "====> Epoch: 11 Average loss: 65.9360\n",
      "====> Test set loss: 69.5051\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 66.791656\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 65.649605\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 64.976891\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 66.364304\n",
      "====> Epoch: 12 Average loss: 65.8537\n",
      "====> Test set loss: 69.1673\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 66.043625\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 66.274529\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 66.197266\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 65.698044\n",
      "====> Epoch: 13 Average loss: 65.7782\n",
      "====> Test set loss: 69.1441\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 66.219948\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 66.480942\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 64.511803\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 65.456650\n",
      "====> Epoch: 14 Average loss: 65.6924\n",
      "====> Test set loss: 69.1444\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 65.545967\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 65.759743\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 65.843903\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 65.239067\n",
      "====> Epoch: 15 Average loss: 65.6371\n",
      "====> Test set loss: 69.8364\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 66.367195\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 66.484467\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 65.457306\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 65.205780\n",
      "====> Epoch: 16 Average loss: 65.5874\n",
      "====> Test set loss: 69.2969\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 65.590111\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 65.696198\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 65.575790\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 65.403862\n",
      "====> Epoch: 17 Average loss: 65.5207\n",
      "====> Test set loss: 69.4861\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 65.304214\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 65.637321\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 65.826515\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 64.886307\n",
      "====> Epoch: 18 Average loss: 65.4670\n",
      "====> Test set loss: 69.6373\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 65.859619\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 64.822205\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 66.078903\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 65.863831\n",
      "====> Epoch: 19 Average loss: 65.4242\n",
      "====> Test set loss: 70.8944\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 65.497437\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 64.895042\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 65.220573\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 65.332169\n",
      "====> Epoch: 20 Average loss: 65.3287\n",
      "====> Test set loss: 70.4097\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5416, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(joint_vae_gumbel(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1045, 0.3075, 0.1511, 0.2136, 0.2214, 0.2187, 0.1068, 0.2332, 0.0491,\n",
       "        0.2182, 0.0362, 0.0662, 0.1080, 0.2849, 0.2242, 0.3268, 0.2868, 0.0284,\n",
       "        0.0435, 0.1078, 0.0941, 0.2522, 0.0245, 0.0395, 0.0227, 0.0989, 0.4723,\n",
       "        0.3333, 0.2250, 0.3063], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0422, 0.0386, 0.0399, 0.0387, 0.0384, 0.0383, 0.0428, 0.0385, 0.0335,\n",
       "        0.0405, 0.0257, 0.0377, 0.0396, 0.0373, 0.0401, 0.0324, 0.0356, 0.0294,\n",
       "        0.0332, 0.0454, 0.0443, 0.0425, 0.0228, 0.0294, 0.0249, 0.0474, 0.0228,\n",
       "        0.0286, 0.0389, 0.0340], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0954, 0.3105, 0.1620, 0.2101, 0.2351, 0.2171, 0.0862, 0.2378, 0.0265,\n",
       "        0.2463, 0.0175, 0.0578, 0.0978, 0.3219, 0.2149, 0.3163, 0.3104, 0.0110,\n",
       "        0.0319, 0.0950, 0.0762, 0.2342, 0.0099, 0.0270, 0.0086, 0.0724, 0.4506,\n",
       "        0.3724, 0.2340, 0.3332], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = joint_vae_gumbel.weight_creator(test_data[0:2, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26, 39, 55, 50, 56, 13, 11, 37, 31, 30],\n",
       "        [ 8, 25, 20, 52, 43, 48, 16, 15, 26, 50]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3 * z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint Training while selecting exactly z_size. Why does it pick the noise variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(2*D, 100, 20)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(2*D, 100, 20, k = z_size)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 85.075401\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 82.677109\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 80.355103\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 78.637482\n",
      "====> Epoch: 1 Average loss: 81.5972\n",
      "====> Test set loss: 83.5908\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 78.531113\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 77.056122\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 75.628159\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 74.188736\n",
      "====> Epoch: 2 Average loss: 76.2796\n",
      "====> Test set loss: 75.9106\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 74.064415\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 73.168182\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 71.832191\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 69.447136\n",
      "====> Epoch: 3 Average loss: 72.1026\n",
      "====> Test set loss: 72.9315\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 69.868683\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 69.277580\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 69.040825\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 67.638107\n",
      "====> Epoch: 4 Average loss: 68.9180\n",
      "====> Test set loss: 72.0876\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 67.737846\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 67.487602\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 67.879440\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 67.943062\n",
      "====> Epoch: 5 Average loss: 67.5563\n",
      "====> Test set loss: 71.8436\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 67.024315\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 66.915352\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 66.804558\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 66.638489\n",
      "====> Epoch: 6 Average loss: 66.9143\n",
      "====> Test set loss: 70.9081\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 66.732658\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 66.467094\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 67.609161\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 66.511520\n",
      "====> Epoch: 7 Average loss: 66.5269\n",
      "====> Test set loss: 70.0472\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 66.483131\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 65.836609\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 66.204964\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 65.495667\n",
      "====> Epoch: 8 Average loss: 66.3147\n",
      "====> Test set loss: 70.5378\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 65.992371\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 66.808403\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 65.483543\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 66.380592\n",
      "====> Epoch: 9 Average loss: 66.1673\n",
      "====> Test set loss: 69.6297\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 65.714027\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 65.473679\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 65.093407\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 66.034981\n",
      "====> Epoch: 10 Average loss: 66.0504\n",
      "====> Test set loss: 69.0215\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 65.972176\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 66.543007\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 65.342834\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 65.451935\n",
      "====> Epoch: 11 Average loss: 65.9466\n",
      "====> Test set loss: 68.6673\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 65.065460\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 65.991844\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 64.863564\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 65.819412\n",
      "====> Epoch: 12 Average loss: 65.8798\n",
      "====> Test set loss: 68.7583\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 65.330734\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 65.666733\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 66.062912\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 65.106659\n",
      "====> Epoch: 13 Average loss: 65.8044\n",
      "====> Test set loss: 68.4594\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 65.557014\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 65.244080\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 65.469025\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 65.810341\n",
      "====> Epoch: 14 Average loss: 65.7616\n",
      "====> Test set loss: 68.4906\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 66.893570\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 65.132568\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 66.951965\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 66.236679\n",
      "====> Epoch: 15 Average loss: 65.6961\n",
      "====> Test set loss: 68.7406\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 66.500366\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 66.509315\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 65.050423\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 65.449379\n",
      "====> Epoch: 16 Average loss: 65.6341\n",
      "====> Test set loss: 68.0772\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 65.686272\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 65.654694\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 65.673904\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 66.241180\n",
      "====> Epoch: 17 Average loss: 65.6016\n",
      "====> Test set loss: 68.1017\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 65.667946\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 66.049652\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 65.645195\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 65.286415\n",
      "====> Epoch: 18 Average loss: 65.5392\n",
      "====> Test set loss: 68.2587\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 66.081413\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 65.179588\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 65.362411\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 67.099304\n",
      "====> Epoch: 19 Average loss: 65.4872\n",
      "====> Test set loss: 69.3336\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 65.214989\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 64.759575\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 64.195358\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 65.305588\n",
      "====> Epoch: 20 Average loss: 65.4355\n",
      "====> Test set loss: 68.4410\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5430, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(joint_vae_gumbel(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1043, 0.3059, 0.1469, 0.2165, 0.2183, 0.2168, 0.1126, 0.2374, 0.0493,\n",
       "        0.2107, 0.0356, 0.0629, 0.1037, 0.2789, 0.2302, 0.3307, 0.2845, 0.0304,\n",
       "        0.0463, 0.1113, 0.0959, 0.2545, 0.0247, 0.0398, 0.0259, 0.0993, 0.4844,\n",
       "        0.3294, 0.2177, 0.2941], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0421, 0.0380, 0.0389, 0.0380, 0.0371, 0.0375, 0.0419, 0.0377, 0.0326,\n",
       "        0.0397, 0.0250, 0.0369, 0.0385, 0.0363, 0.0385, 0.0311, 0.0349, 0.0291,\n",
       "        0.0321, 0.0444, 0.0433, 0.0411, 0.0229, 0.0289, 0.0246, 0.0463, 0.0226,\n",
       "        0.0292, 0.0382, 0.0326], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0551, 0.2485, 0.1082, 0.1835, 0.1943, 0.1613, 0.0786, 0.1836, 0.0109,\n",
       "        0.1750, 0.0067, 0.0196, 0.0446, 0.2755, 0.2051, 0.3167, 0.2483, 0.0037,\n",
       "        0.0090, 0.0602, 0.0459, 0.2150, 0.0034, 0.0155, 0.0029, 0.0388, 0.4901,\n",
       "        0.3103, 0.1910, 0.2466], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = joint_vae_gumbel.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52, 41, 36, 58, 10],\n",
       "        [29, 35, 12,  4, 28]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching pre trained is actually better here than joint training.\n",
    "The gumbel trick greatly reduces the ability to make predictions. \n",
    "Notice that the standard deviations are not as high as in the original data. Not being able to use a model looking at the full data as an anchor definitely hurts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
