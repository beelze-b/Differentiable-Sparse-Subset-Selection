{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know that Gumbel selects things relatively well. Its effects of Zeisel though are a bit muddled because of reconstruction. Let's do a simple synethetic dataset. Half the features are real. Half the features at noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_PATH_DATA = '../data/'\n",
    "BASE_PATH_DATA = '/scratch/ns3429/sparse-subset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 10\n",
    "#n_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 5000\n",
    "z_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=False)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=10, out_features=30, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7641,  0.4002,  0.9787,  2.2409,  1.8676],\n",
       "        [-0.9773,  0.9501, -0.1514, -0.1032,  0.4106],\n",
       "        [ 0.1440,  1.4543,  0.7610,  0.1217,  0.4439],\n",
       "        ...,\n",
       "        [ 0.2501, -1.0168,  0.0459,  0.5006,  1.2243],\n",
       "        [-0.5595,  1.5234, -0.5857,  0.8466, -0.1063],\n",
       "        [ 0.7700,  0.7508, -0.5606, -1.7603,  0.4371]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = data_mapper(latent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(18, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample, half the data is non zero, whereas in zeisel, about 25% if non zero. Easier than Zeisel good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0921e-02, -6.1085e-04, -1.4928e-02,  ..., -1.4309e-02,\n",
       "          1.6859e-02, -1.2177e-02],\n",
       "        [ 7.6496e-03,  1.1971e-02, -2.2414e-02,  ...,  1.0256e-02,\n",
       "         -5.5957e-03,  4.3434e-03],\n",
       "        [ 2.7566e-03,  1.0969e-03,  3.5942e-03,  ...,  6.0039e-03,\n",
       "          8.7524e-04,  7.0365e-03],\n",
       "        ...,\n",
       "        [ 1.8449e-02,  8.3797e-04, -8.9499e-03,  ...,  8.9735e-04,\n",
       "         -1.6982e-03,  7.8153e-03],\n",
       "        [-1.0649e-02, -9.6204e-03, -8.1562e-03,  ..., -2.2612e-04,\n",
       "         -1.4104e-02, -8.2127e-03],\n",
       "        [ 2.1183e-02, -1.1416e-02,  1.8769e-03,  ..., -1.3100e-02,\n",
       "         -6.2333e-03, -4.3646e-05]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_features = torch.empty(N * D).normal_(mean=0,std=0.01).reshape(N, D).to(device)\n",
    "noise_features.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = torch.cat([actual_data, noise_features], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 60])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1866, 0.2313, 0.2054, 0.2209, 0.2323, 0.1899, 0.1801, 0.1969, 0.1133,\n",
       "        0.2353, 0.0925, 0.1310, 0.1725, 0.1902, 0.2294, 0.2275, 0.2082, 0.0530,\n",
       "        0.0980, 0.1738, 0.1728, 0.2156, 0.0460, 0.0932, 0.0255, 0.1816, 0.1587,\n",
       "        0.2263, 0.2125, 0.2393, 0.1326, 0.1439, 0.1281, 0.1421, 0.1297, 0.1413,\n",
       "        0.1492, 0.1272, 0.1233, 0.1420, 0.1422, 0.1363, 0.1256, 0.1288, 0.1377,\n",
       "        0.1438, 0.1337, 0.1331, 0.1258, 0.1346, 0.1507, 0.1223, 0.1429, 0.1343,\n",
       "        0.1348, 0.1361, 0.1388, 0.1391, 0.1426, 0.1383], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.std(dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviatiosn are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0'),\n",
       "indices=tensor([ 738, 4262, 1553, 1484, 2220, 1316, 3892, 1316, 3121, 3883, 1838,  623,\n",
       "        1004, 4856,  689, 2033, 2038, 1316, 4515, 4562, 4668,  616,  894, 4515,\n",
       "        1885, 3892, 4615,  819, 4397, 4293,  713, 2220, 3813, 4659, 4389, 3659,\n",
       "         309, 1804,  495, 4790, 3110, 4671,   36,    0, 1215,  148, 4008, 1317,\n",
       "        2503, 1402, 1580, 2684, 4078, 3334, 1376, 2499, 1301, 3114, 4203, 3183],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.max(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'),\n",
       "indices=tensor([   1,    8,    0,    1,    0,    5,    1,    1,    1,    0,    1,    1,\n",
       "           0,   13,    3,    3,    1,    0,    0,    1,    1,    8,    0,    0,\n",
       "           0,    1,  254,    4,    0,    0,  454, 3677, 1909, 3750, 3638, 4476,\n",
       "         553, 4105,  289, 1150, 2707, 1846, 3579, 3101,  299, 1324, 3277, 4318,\n",
       "        3023,  967, 2932, 3588,  919, 1190,  271, 2937, 3428, 3955, 2719,  198],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.min(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49, device='cuda:0')\n",
      "tensor(42, device='cuda:0')\n",
      "tensor(48, device='cuda:0')\n",
      "tensor(44, device='cuda:0')\n",
      "tensor(44, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 60])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 60])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_correlations(dataset):\n",
    "    cor_df = np.zeros((dataset.shape[1], dataset.shape[1]))\n",
    "    for row in np.arange(dataset.shape[1]):\n",
    "        for col in np.arange(dataset.shape[1]):\n",
    "            cor_df[row, col] = pearsonr(dataset[row], dataset[col])[0]\n",
    "    return cor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_data_clone = actual_data.clone().detach().cpu().numpy()\n",
    "\n",
    "pairwise_cor_df = pairwise_correlations(actual_data_clone)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(pairwise_cor_df)\n",
    "\n",
    "correlates = []\n",
    "for i in range(2 * D):\n",
    "    one = np.sum(np.abs(pairwise_cor_df[i, :D]))\n",
    "    two = np.sum(np.abs(pairwise_cor_df[i, D:2*D]))\n",
    "    #correlates.append([one, two])\n",
    "    correlates.append(one > two)\n",
    "    \n",
    "# correlation of that feature with the first 30 features and the second 30 features\n",
    "print(np.sum(correlates[:D]))\n",
    "\n",
    "print(np.sum(correlates[D:2*D]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now run all of the notebook or just the last part focusing on gumbel vs gradients on truncated (or not) losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is all ready. Now time to feed into into a pretraining-matching Gumbel and joint training Gumbel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre train VAE First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae = VAE(2*D, 100, 20)\n",
    "\n",
    "pretrain_vae.to(device)\n",
    "pretrain_vae_optimizer = torch.optim.Adam(pretrain_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))\n",
    "\n",
    "#pretrain_vae_optimizer = torch.optim.SGD(pretrain_vae.parameters(), \n",
    "#                                            lr=lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 42.017113\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 40.903053\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 39.910145\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 39.210461\n",
      "====> Epoch: 1 Average loss: 40.4244\n",
      "====> Test set loss: 38.9524\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 38.888924\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.212513\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 36.419609\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 35.664536\n",
      "====> Epoch: 2 Average loss: 37.3049\n",
      "====> Test set loss: 35.4656\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 35.430317\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 34.606815\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 34.577702\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.209206\n",
      "====> Epoch: 3 Average loss: 34.6850\n",
      "====> Test set loss: 34.1042\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.345818\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 34.207222\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 33.563995\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.908337\n",
      "====> Epoch: 4 Average loss: 33.8812\n",
      "====> Test set loss: 33.6500\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 33.996525\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.835045\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.518723\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.168392\n",
      "====> Epoch: 5 Average loss: 33.5430\n",
      "====> Test set loss: 33.3353\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 33.442101\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.615280\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.352787\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 33.062305\n",
      "====> Epoch: 6 Average loss: 33.3164\n",
      "====> Test set loss: 33.1817\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 33.309429\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 33.084484\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 33.366402\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 33.056320\n",
      "====> Epoch: 7 Average loss: 33.1319\n",
      "====> Test set loss: 33.0265\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 33.327454\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 33.176674\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 33.073765\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 32.693161\n",
      "====> Epoch: 8 Average loss: 33.0185\n",
      "====> Test set loss: 32.9396\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 32.457218\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 32.928253\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 32.993736\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 32.937096\n",
      "====> Epoch: 9 Average loss: 32.9161\n",
      "====> Test set loss: 32.8299\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 32.924587\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 32.558769\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 32.684296\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 33.207237\n",
      "====> Epoch: 10 Average loss: 32.8035\n",
      "====> Test set loss: 32.6680\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 32.653099\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 32.831394\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 32.375183\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 32.626377\n",
      "====> Epoch: 11 Average loss: 32.5824\n",
      "====> Test set loss: 32.3997\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 32.514385\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 32.200542\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 32.589878\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 32.178410\n",
      "====> Epoch: 12 Average loss: 32.3290\n",
      "====> Test set loss: 32.1813\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 32.289268\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 32.206417\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 32.278069\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 32.369549\n",
      "====> Epoch: 13 Average loss: 32.1558\n",
      "====> Test set loss: 32.0547\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 31.900909\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 32.196495\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 32.229164\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 32.197395\n",
      "====> Epoch: 14 Average loss: 32.0321\n",
      "====> Test set loss: 31.9535\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 31.856623\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 31.696789\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 31.695568\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 31.998665\n",
      "====> Epoch: 15 Average loss: 31.9635\n",
      "====> Test set loss: 31.8698\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 31.955343\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 31.983461\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 32.110096\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 32.054447\n",
      "====> Epoch: 16 Average loss: 31.9013\n",
      "====> Test set loss: 31.8082\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 31.771000\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 32.020573\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 31.843025\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 31.751513\n",
      "====> Epoch: 17 Average loss: 31.8380\n",
      "====> Test set loss: 31.7816\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 31.683304\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 31.588379\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 31.705652\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 31.933746\n",
      "====> Epoch: 18 Average loss: 31.7766\n",
      "====> Test set loss: 31.6849\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 32.360500\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 31.724262\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 31.859814\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 31.709867\n",
      "====> Epoch: 19 Average loss: 31.7122\n",
      "====> Test set loss: 31.6376\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 31.779974\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 31.284533\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 31.391958\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 31.349340\n",
      "====> Epoch: 20 Average loss: 31.6303\n",
      "====> Test set loss: 31.5404\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 31.590551\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 31.640583\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 31.818018\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 31.553604\n",
      "====> Epoch: 21 Average loss: 31.5221\n",
      "====> Test set loss: 31.4279\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 31.209724\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 31.501251\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 31.478521\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 31.363119\n",
      "====> Epoch: 22 Average loss: 31.3792\n",
      "====> Test set loss: 31.2837\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 31.283752\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 31.079319\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 31.267811\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 31.373390\n",
      "====> Epoch: 23 Average loss: 31.2691\n",
      "====> Test set loss: 31.2361\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 31.072739\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 31.460453\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 31.270405\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 30.968897\n",
      "====> Epoch: 24 Average loss: 31.1697\n",
      "====> Test set loss: 31.1025\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 31.283909\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 31.004129\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 31.003883\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 31.388237\n",
      "====> Epoch: 25 Average loss: 31.0785\n",
      "====> Test set loss: 30.9833\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 30.887918\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 30.966131\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 30.969601\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 30.970596\n",
      "====> Epoch: 26 Average loss: 30.9724\n",
      "====> Test set loss: 30.9033\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 30.640602\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 30.771666\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 31.104567\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 30.618370\n",
      "====> Epoch: 27 Average loss: 30.9081\n",
      "====> Test set loss: 30.8173\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 30.784786\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 30.941294\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 30.863577\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 30.868454\n",
      "====> Epoch: 28 Average loss: 30.8511\n",
      "====> Test set loss: 30.7931\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 30.984713\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 30.895269\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 30.978951\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 30.716114\n",
      "====> Epoch: 29 Average loss: 30.8103\n",
      "====> Test set loss: 30.7490\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 31.057421\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 30.876495\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 30.588932\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 30.662325\n",
      "====> Epoch: 30 Average loss: 30.7786\n",
      "====> Test set loss: 30.7362\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 30.691622\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 30.873983\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 30.858854\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 30.906868\n",
      "====> Epoch: 31 Average loss: 30.7436\n",
      "====> Test set loss: 30.6999\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 30.614374\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 30.584618\n",
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 30.891748\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 30.780199\n",
      "====> Epoch: 32 Average loss: 30.7141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 30.6768\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 30.802919\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 30.749544\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 30.829163\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 30.446085\n",
      "====> Epoch: 33 Average loss: 30.6890\n",
      "====> Test set loss: 30.6682\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 30.434799\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 30.970924\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 30.403496\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 30.874573\n",
      "====> Epoch: 34 Average loss: 30.6707\n",
      "====> Test set loss: 30.6344\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 30.790586\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 30.480667\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 30.827261\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 30.903517\n",
      "====> Epoch: 35 Average loss: 30.6488\n",
      "====> Test set loss: 30.6218\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 31.043398\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 30.573771\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 30.547482\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 30.535990\n",
      "====> Epoch: 36 Average loss: 30.6306\n",
      "====> Test set loss: 30.6092\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 30.713840\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 31.007141\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 30.307425\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 30.344015\n",
      "====> Epoch: 37 Average loss: 30.6262\n",
      "====> Test set loss: 30.5888\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 30.955111\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 30.717663\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 30.759192\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 30.585281\n",
      "====> Epoch: 38 Average loss: 30.5836\n",
      "====> Test set loss: 30.5827\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 30.699099\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 30.553802\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 30.305653\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 30.460131\n",
      "====> Epoch: 39 Average loss: 30.5795\n",
      "====> Test set loss: 30.5595\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 30.779570\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 30.416681\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 30.851614\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 30.504482\n",
      "====> Epoch: 40 Average loss: 30.5620\n",
      "====> Test set loss: 30.5409\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 30.623724\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 30.480110\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 30.459265\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 30.428608\n",
      "====> Epoch: 41 Average loss: 30.5471\n",
      "====> Test set loss: 30.5488\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 30.495504\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 30.636826\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 30.621056\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 30.647179\n",
      "====> Epoch: 42 Average loss: 30.5442\n",
      "====> Test set loss: 30.4980\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 30.441555\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 30.395248\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 30.599918\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 30.662346\n",
      "====> Epoch: 43 Average loss: 30.5185\n",
      "====> Test set loss: 30.5054\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 30.723736\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 30.521111\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 30.387514\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 30.585745\n",
      "====> Epoch: 44 Average loss: 30.5106\n",
      "====> Test set loss: 30.4933\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 30.558941\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 30.572298\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 30.252615\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 30.509411\n",
      "====> Epoch: 45 Average loss: 30.5017\n",
      "====> Test set loss: 30.4653\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 30.593023\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 30.848335\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 30.790195\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 30.339924\n",
      "====> Epoch: 46 Average loss: 30.4843\n",
      "====> Test set loss: 30.4524\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 30.619297\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 30.143131\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 30.599688\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 30.458723\n",
      "====> Epoch: 47 Average loss: 30.4707\n",
      "====> Test set loss: 30.4806\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 30.644178\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 30.505157\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 30.275278\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 30.209518\n",
      "====> Epoch: 48 Average loss: 30.4628\n",
      "====> Test set loss: 30.4370\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 30.555466\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 30.528658\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 30.316772\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 30.543491\n",
      "====> Epoch: 49 Average loss: 30.4437\n",
      "====> Test set loss: 30.4201\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 30.503532\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 30.464394\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 30.484758\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 30.143312\n",
      "====> Epoch: 50 Average loss: 30.4296\n",
      "====> Test set loss: 30.3976\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(train_data, pretrain_vae, pretrain_vae_optimizer, epoch, batch_size)\n",
    "    test(test_data, pretrain_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.4964, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(pretrain_vae(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually pretty good! %35 percent off when wrong\n",
    "\n",
    "Get 0.49 when nepochs is 50.\n",
    "Get 0.54 when nepochs is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, if the final layer of the data mapper is not ReLU, this reconstruction is usually on point. When some of the features can be sparse, then this becomes troublesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0931, 0.3051, 0.1384, 0.2096, 0.2092, 0.2112, 0.1042, 0.2325, 0.0347,\n",
       "        0.2014, 0.0244, 0.0502, 0.0919, 0.2735, 0.2254, 0.3294, 0.2786, 0.0084,\n",
       "        0.0338, 0.1001, 0.0849, 0.2512, 0.0048, 0.0257, 0.0015, 0.0874, 0.4764,\n",
       "        0.3270, 0.2107, 0.2899], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1014, 0.3082, 0.1481, 0.2059, 0.2035, 0.2026, 0.1036, 0.2276, 0.0334,\n",
       "        0.1933, 0.0268, 0.0544, 0.0878, 0.2829, 0.2261, 0.3300, 0.2708, 0.0094,\n",
       "        0.0353, 0.0994, 0.0870, 0.2518, 0.0070, 0.0294, 0.0036, 0.0922, 0.4750,\n",
       "        0.3314, 0.2135, 0.2963], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1635, 0.2089, 0.1991, 0.1973, 0.2046, 0.0988, 0.1592, 0.1667, 0.0784,\n",
       "        0.2134, 0.0471, 0.0845, 0.1464, 0.1706, 0.2144, 0.2154, 0.1740, 0.0129,\n",
       "        0.0778, 0.1471, 0.1372, 0.1853, 0.0119, 0.0618, 0.0055, 0.1587, 0.1384,\n",
       "        0.2182, 0.1869, 0.2146], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0192, 0.0166, 0.0169, 0.0182, 0.0186, 0.0176, 0.0177, 0.0193, 0.0181,\n",
       "        0.0182, 0.0176, 0.0181, 0.0175, 0.0176, 0.0170, 0.0169, 0.0160, 0.0168,\n",
       "        0.0176, 0.0168, 0.0176, 0.0161, 0.0185, 0.0188, 0.0188, 0.0182, 0.0179,\n",
       "        0.0173, 0.0182, 0.0180], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].std(dim = 0)[D:2*D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_std = pretrain_vae(test_data)[0].std(dim = 0)[:D] / test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8577, 0.8759, 0.9402, 0.8815, 0.8857, 0.5150, 0.8801, 0.8438, 0.7950,\n",
      "        0.9072, 0.5083, 0.6143, 0.8757, 0.8771, 0.9157, 0.9198, 0.8350, 0.3095,\n",
      "        0.8366, 0.8784, 0.7831, 0.8066, 0.2013, 0.6737, 0.3344, 0.8627, 0.8543,\n",
      "        0.9643, 0.8786, 0.9155], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0.7742371559143066\n"
     ]
    }
   ],
   "source": [
    "print(average_std)\n",
    "print(average_std.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get .8 as the mean when nepoch is 50. Get 0.43 as the mean when nepochs is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0007, 0.4000, 0.0045, 0.1049, 0.6208, 0.3453, 0.0412, 0.3219, 0.0022,\n",
       "        0.2507, 0.0014, 0.0100, 0.0417, 0.0886, 0.3184, 0.5396, 0.2331, 0.0083,\n",
       "        0.0024, 0.0026, 0.0709, 0.5700, 0.0041, 0.0025, 0.0013, 0.0598, 0.3389,\n",
       "        0.0866, 0.5517, 0.3539], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(test_data[samp,:D] - pretrain_vae(test_data)[0][samp, :D]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0107, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[1][:, :D].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6380, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(pretrain_vae(test_data)[2][:, :D]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=60, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (enc_mean): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (enc_logvar): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=60, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good.\n",
    "\n",
    "**Gumbel matching pretrained VAE next**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_with_pre = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "vae_gumbel_with_pre.to(device)\n",
    "vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 135.722412\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 126.609497\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 121.512253\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 116.164474\n",
      "====> Epoch: 1 Average loss: 125.7550\n",
      "====> Test set loss: 39.4420\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 118.806061\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 115.479691\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 123.038879\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 118.022873\n",
      "====> Epoch: 2 Average loss: 120.1893\n",
      "====> Test set loss: 37.3728\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 109.040268\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 116.864220\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 106.289795\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 99.098221\n",
      "====> Epoch: 3 Average loss: 106.5753\n",
      "====> Test set loss: 35.5293\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 92.867889\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 92.626755\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 90.445953\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 83.077820\n",
      "====> Epoch: 4 Average loss: 92.0391\n",
      "====> Test set loss: 34.4310\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 94.296074\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 77.714569\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 81.084869\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 74.171280\n",
      "====> Epoch: 5 Average loss: 82.9036\n",
      "====> Test set loss: 33.8310\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 78.075912\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 70.341141\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 63.874229\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 61.171684\n",
      "====> Epoch: 6 Average loss: 65.8625\n",
      "====> Test set loss: 33.4171\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 57.562172\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 58.226929\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 54.463192\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 50.588814\n",
      "====> Epoch: 7 Average loss: 55.8087\n",
      "====> Test set loss: 33.0276\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 55.725613\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 51.121552\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 50.626991\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 49.758743\n",
      "====> Epoch: 8 Average loss: 52.0710\n",
      "====> Test set loss: 32.7353\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 50.725620\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 50.645775\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 48.536812\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 50.556717\n",
      "====> Epoch: 9 Average loss: 49.6258\n",
      "====> Test set loss: 32.4263\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 47.926109\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 47.804886\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 47.524033\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 45.503014\n",
      "====> Epoch: 10 Average loss: 47.7907\n",
      "====> Test set loss: 32.1936\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 46.048843\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 46.897743\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 44.097408\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 43.540649\n",
      "====> Epoch: 11 Average loss: 46.2466\n",
      "====> Test set loss: 31.9357\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 45.598862\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 46.120262\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 44.730377\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 44.540695\n",
      "====> Epoch: 12 Average loss: 45.0687\n",
      "====> Test set loss: 31.8322\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 45.227455\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 45.561817\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 42.888672\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 42.212193\n",
      "====> Epoch: 13 Average loss: 44.2510\n",
      "====> Test set loss: 31.6805\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 44.285442\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 42.699551\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 44.450256\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 42.885391\n",
      "====> Epoch: 14 Average loss: 43.4981\n",
      "====> Test set loss: 31.5295\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 43.120880\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 43.721817\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 41.357368\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 41.464832\n",
      "====> Epoch: 15 Average loss: 42.7544\n",
      "====> Test set loss: 31.4147\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 40.528797\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 43.701683\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 44.578201\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 41.733902\n",
      "====> Epoch: 16 Average loss: 42.2686\n",
      "====> Test set loss: 31.3458\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 42.929054\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 42.429470\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 42.658516\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 40.454048\n",
      "====> Epoch: 17 Average loss: 42.0751\n",
      "====> Test set loss: 31.2650\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 41.296978\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 41.057640\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 40.952259\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 41.607834\n",
      "====> Epoch: 18 Average loss: 41.6210\n",
      "====> Test set loss: 31.1831\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 40.142597\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 40.493942\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 41.663361\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 40.313290\n",
      "====> Epoch: 19 Average loss: 41.1337\n",
      "====> Test set loss: 31.1396\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 38.343391\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 43.186989\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 38.848911\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 40.579254\n",
      "====> Epoch: 20 Average loss: 40.6926\n",
      "====> Test set loss: 31.0904\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 39.104298\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 40.035809\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 41.526787\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 39.590034\n",
      "====> Epoch: 21 Average loss: 40.5537\n",
      "====> Test set loss: 31.0424\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 39.277164\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 41.596649\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 39.822006\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 38.502644\n",
      "====> Epoch: 22 Average loss: 40.3633\n",
      "====> Test set loss: 30.9810\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 41.492134\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 39.686409\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 39.374912\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 39.570984\n",
      "====> Epoch: 23 Average loss: 40.0200\n",
      "====> Test set loss: 30.9478\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 41.066231\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 39.722153\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 39.362839\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 41.070858\n",
      "====> Epoch: 24 Average loss: 40.0437\n",
      "====> Test set loss: 30.9370\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 39.159885\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 38.207764\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 38.474796\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 41.347359\n",
      "====> Epoch: 25 Average loss: 39.6715\n",
      "====> Test set loss: 30.8875\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 39.023853\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 39.097809\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 41.555843\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 37.815792\n",
      "====> Epoch: 26 Average loss: 39.3586\n",
      "====> Test set loss: 30.8734\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 39.684742\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 37.628872\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 39.488312\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 38.496597\n",
      "====> Epoch: 27 Average loss: 39.3501\n",
      "====> Test set loss: 30.8401\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 38.156860\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 38.214558\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 38.184345\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 39.237236\n",
      "====> Epoch: 28 Average loss: 39.0730\n",
      "====> Test set loss: 30.7946\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 38.855862\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 39.434158\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 39.349262\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 38.685936\n",
      "====> Epoch: 29 Average loss: 38.8034\n",
      "====> Test set loss: 30.7809\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 38.423027\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 38.675560\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 38.860107\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 38.933361\n",
      "====> Epoch: 30 Average loss: 38.9031\n",
      "====> Test set loss: 30.7613\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 38.892719\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 37.938801\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 38.100388\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 39.568184\n",
      "====> Epoch: 31 Average loss: 38.6412\n",
      "====> Test set loss: 30.7411\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 38.578812\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 37.562054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 38.562012\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 38.535431\n",
      "====> Epoch: 32 Average loss: 38.7111\n",
      "====> Test set loss: 30.7497\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 39.352974\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 38.365799\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 40.016819\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 36.582668\n",
      "====> Epoch: 33 Average loss: 38.2257\n",
      "====> Test set loss: 30.7124\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 36.885632\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 38.281082\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 38.087807\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 37.061802\n",
      "====> Epoch: 34 Average loss: 38.1512\n",
      "====> Test set loss: 30.7391\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 37.405006\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 36.905785\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 37.311352\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 38.416206\n",
      "====> Epoch: 35 Average loss: 38.2980\n",
      "====> Test set loss: 30.6733\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 39.994431\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 39.165375\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 38.501377\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 36.666950\n",
      "====> Epoch: 36 Average loss: 38.2152\n",
      "====> Test set loss: 30.6773\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 37.391823\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 37.541138\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 38.123650\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 37.897511\n",
      "====> Epoch: 37 Average loss: 37.9125\n",
      "====> Test set loss: 30.6877\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 37.122612\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 37.899662\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 37.728035\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 37.005970\n",
      "====> Epoch: 38 Average loss: 37.7708\n",
      "====> Test set loss: 30.6447\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 38.822205\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 36.942570\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 36.673691\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 37.845844\n",
      "====> Epoch: 39 Average loss: 37.6848\n",
      "====> Test set loss: 30.6441\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 36.944633\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 38.330536\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 37.176762\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 37.522594\n",
      "====> Epoch: 40 Average loss: 37.7043\n",
      "====> Test set loss: 30.6297\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 38.149426\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 36.964828\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 38.864342\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 38.696426\n",
      "====> Epoch: 41 Average loss: 37.6811\n",
      "====> Test set loss: 30.6402\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 37.739174\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 38.629017\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 38.088554\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 37.424843\n",
      "====> Epoch: 42 Average loss: 37.4285\n",
      "====> Test set loss: 30.6431\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 36.234150\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 36.521694\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 38.001583\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 36.447544\n",
      "====> Epoch: 43 Average loss: 37.5232\n",
      "====> Test set loss: 30.5873\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 35.822144\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 36.768387\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 37.996017\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 36.809685\n",
      "====> Epoch: 44 Average loss: 37.3715\n",
      "====> Test set loss: 30.5720\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 37.069199\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 36.494930\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 36.162689\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 36.900597\n",
      "====> Epoch: 45 Average loss: 37.1270\n",
      "====> Test set loss: 30.5939\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 37.495930\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 36.550056\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 36.974342\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 37.575371\n",
      "====> Epoch: 46 Average loss: 37.2622\n",
      "====> Test set loss: 30.5732\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 37.007774\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 37.243793\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 37.070442\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 36.741886\n",
      "====> Epoch: 47 Average loss: 37.1619\n",
      "====> Test set loss: 30.5707\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 37.652676\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 38.723778\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 37.901093\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 36.425674\n",
      "====> Epoch: 48 Average loss: 37.1602\n",
      "====> Test set loss: 30.5591\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 37.737724\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 37.379749\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 35.841042\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 37.430756\n",
      "====> Epoch: 49 Average loss: 36.9142\n",
      "====> Test set loss: 30.5727\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 37.110191\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 37.940578\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 37.048847\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 37.101440\n",
      "====> Epoch: 50 Average loss: 37.0909\n",
      "====> Test set loss: 30.5599\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, \n",
    "                      epoch, pretrain_vae, batch_size)\n",
    "    test(test_data, vae_gumbel_with_pre, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.4989, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(vae_gumbel_with_pre(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0952, 0.2989, 0.1417, 0.2104, 0.2064, 0.2060, 0.0998, 0.2277, 0.0326,\n",
       "        0.2021, 0.0250, 0.0541, 0.0915, 0.2805, 0.2235, 0.3298, 0.2769, 0.0084,\n",
       "        0.0316, 0.0938, 0.0890, 0.2452, 0.0058, 0.0260, 0.0032, 0.0875, 0.4759,\n",
       "        0.3291, 0.2102, 0.2966], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1582, 0.1962, 0.1938, 0.1979, 0.1960, 0.0950, 0.1551, 0.1640, 0.0684,\n",
       "        0.2129, 0.0405, 0.0839, 0.1462, 0.1689, 0.2100, 0.2093, 0.1674, 0.0097,\n",
       "        0.0663, 0.1370, 0.1413, 0.1723, 0.0072, 0.0467, 0.0036, 0.1531, 0.1370,\n",
       "        0.2105, 0.1771, 0.2142], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0036, 0.3848, 0.0125, 0.0872, 0.4108, 0.2515, 0.0508, 0.2489, 0.0047,\n",
       "        0.1727, 0.0030, 0.0104, 0.0376, 0.1713, 0.3161, 0.4981, 0.2082, 0.0111,\n",
       "        0.0056, 0.0140, 0.0380, 0.4384, 0.0068, 0.0056, 0.0024, 0.0459, 0.3856,\n",
       "        0.1690, 0.3979, 0.3343], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_with_pre.weight_creator(test_data[0:2, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 10, 54,  3,  1, 19,  7, 28, 58, 30, 20, 27, 23, 53, 21],\n",
       "        [31, 54, 14,  4, 25, 28,  6,  1,  3, 32,  7, 46, 21, 16, 27]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as long as feature index is lesss than 30, then it isn't picking noise\n",
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3 * z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(2*D, 100, 20)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 85.120331\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 83.020958\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 80.954147\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 79.361855\n",
      "====> Epoch: 1 Average loss: 81.9689\n",
      "====> Test set loss: 79.4539\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 79.080437\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 77.663467\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 76.020126\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 74.630486\n",
      "====> Epoch: 2 Average loss: 76.8986\n",
      "====> Test set loss: 75.8125\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 74.629433\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 73.278839\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 70.789421\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 70.161156\n",
      "====> Epoch: 3 Average loss: 72.2921\n",
      "====> Test set loss: 77.5574\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 70.520943\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 69.573349\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 68.896667\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 68.662308\n",
      "====> Epoch: 4 Average loss: 69.1299\n",
      "====> Test set loss: 76.7028\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 68.870155\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 67.649879\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 68.348282\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 67.160683\n",
      "====> Epoch: 5 Average loss: 67.7588\n",
      "====> Test set loss: 75.5308\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 67.019279\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 67.460571\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 67.243752\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 66.848442\n",
      "====> Epoch: 6 Average loss: 67.0532\n",
      "====> Test set loss: 74.2383\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 67.079002\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 66.801254\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 66.463577\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 66.573029\n",
      "====> Epoch: 7 Average loss: 66.6890\n",
      "====> Test set loss: 73.8677\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 66.033325\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 67.008614\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 67.191246\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 66.821388\n",
      "====> Epoch: 8 Average loss: 66.4315\n",
      "====> Test set loss: 71.9676\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 67.077827\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 66.302277\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 65.598946\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 66.065842\n",
      "====> Epoch: 9 Average loss: 66.2325\n",
      "====> Test set loss: 70.7316\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 65.642868\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 65.643806\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 65.742523\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 65.993820\n",
      "====> Epoch: 10 Average loss: 66.1159\n",
      "====> Test set loss: 70.4100\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 66.209656\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 64.953316\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 65.943794\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 66.302399\n",
      "====> Epoch: 11 Average loss: 66.0022\n",
      "====> Test set loss: 70.3888\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 66.858665\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 65.585823\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 66.158653\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 65.960205\n",
      "====> Epoch: 12 Average loss: 65.9310\n",
      "====> Test set loss: 70.8123\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 65.825790\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 66.025703\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 65.350441\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 65.198959\n",
      "====> Epoch: 13 Average loss: 65.8425\n",
      "====> Test set loss: 69.6681\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 65.330353\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 66.463913\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 65.913666\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 64.938507\n",
      "====> Epoch: 14 Average loss: 65.7635\n",
      "====> Test set loss: 70.0538\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 65.412621\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 65.030045\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 66.328560\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 65.469902\n",
      "====> Epoch: 15 Average loss: 65.7169\n",
      "====> Test set loss: 69.6493\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 65.872261\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 66.430664\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 65.589653\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 66.154572\n",
      "====> Epoch: 16 Average loss: 65.6784\n",
      "====> Test set loss: 70.0340\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 65.568878\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 64.930687\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 65.702370\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 66.113861\n",
      "====> Epoch: 17 Average loss: 65.6184\n",
      "====> Test set loss: 70.4173\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 65.870804\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 65.819801\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 64.452682\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 65.038269\n",
      "====> Epoch: 18 Average loss: 65.5786\n",
      "====> Test set loss: 70.6464\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 65.427666\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 66.409622\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 65.421684\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 65.626007\n",
      "====> Epoch: 19 Average loss: 65.4993\n",
      "====> Test set loss: 71.7177\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 65.730927\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 66.581512\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 65.286354\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 65.407127\n",
      "====> Epoch: 20 Average loss: 65.4497\n",
      "====> Test set loss: 70.9496\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 65.244766\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 65.323105\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 66.003639\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 65.625351\n",
      "====> Epoch: 21 Average loss: 65.4036\n",
      "====> Test set loss: 70.9505\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 64.755600\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 65.410507\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 64.434845\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 64.885323\n",
      "====> Epoch: 22 Average loss: 65.3242\n",
      "====> Test set loss: 75.2253\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 65.269173\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 66.075523\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 65.243851\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 65.600380\n",
      "====> Epoch: 23 Average loss: 65.2668\n",
      "====> Test set loss: 73.9586\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 65.012154\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 66.047707\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 64.880890\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 64.166130\n",
      "====> Epoch: 24 Average loss: 65.2261\n",
      "====> Test set loss: 73.5859\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 64.968445\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 65.726173\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 65.008385\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 65.235855\n",
      "====> Epoch: 25 Average loss: 65.1555\n",
      "====> Test set loss: 73.8823\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 65.461761\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 65.247643\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 66.449425\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 65.314026\n",
      "====> Epoch: 26 Average loss: 65.1168\n",
      "====> Test set loss: 77.4398\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 65.809090\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 65.458664\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 66.002693\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 64.983391\n",
      "====> Epoch: 27 Average loss: 65.0553\n",
      "====> Test set loss: 72.7579\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 65.870384\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 64.665024\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 65.199409\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 64.315048\n",
      "====> Epoch: 28 Average loss: 64.9983\n",
      "====> Test set loss: 73.8617\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 65.110031\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 65.458908\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 66.157150\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 64.014267\n",
      "====> Epoch: 29 Average loss: 64.9655\n",
      "====> Test set loss: 74.9844\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 64.800980\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 64.387314\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 64.484421\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 64.682220\n",
      "====> Epoch: 30 Average loss: 64.8800\n",
      "====> Test set loss: 76.2075\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 65.204781\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 65.658867\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 63.803402\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 64.515213\n",
      "====> Epoch: 31 Average loss: 64.8072\n",
      "====> Test set loss: 78.8404\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 65.716110\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 64.623779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 64.888695\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 64.616302\n",
      "====> Epoch: 32 Average loss: 64.7184\n",
      "====> Test set loss: 80.2254\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 65.033531\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 64.216110\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 65.966454\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 64.526108\n",
      "====> Epoch: 33 Average loss: 64.6671\n",
      "====> Test set loss: 78.1952\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 64.402298\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 64.677193\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 65.303062\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 65.423096\n",
      "====> Epoch: 34 Average loss: 64.5143\n",
      "====> Test set loss: 83.8939\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 64.876678\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 65.037285\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 64.742767\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 64.346825\n",
      "====> Epoch: 35 Average loss: 64.4184\n",
      "====> Test set loss: 91.9585\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 64.564316\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 64.665680\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 64.275558\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 64.832420\n",
      "====> Epoch: 36 Average loss: 64.3278\n",
      "====> Test set loss: 85.7973\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 64.367378\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 64.693970\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 64.921509\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 63.906216\n",
      "====> Epoch: 37 Average loss: 64.2733\n",
      "====> Test set loss: 88.1185\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 64.951561\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 63.682293\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 64.466873\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 64.627014\n",
      "====> Epoch: 38 Average loss: 64.2021\n",
      "====> Test set loss: 85.9474\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 64.832993\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 63.775391\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 64.050255\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 64.514023\n",
      "====> Epoch: 39 Average loss: 64.1281\n",
      "====> Test set loss: 86.8896\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 63.587822\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 64.476250\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 63.999409\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 63.912823\n",
      "====> Epoch: 40 Average loss: 64.0640\n",
      "====> Test set loss: 84.7211\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 63.294216\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 63.779594\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 63.684685\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 63.488869\n",
      "====> Epoch: 41 Average loss: 64.0101\n",
      "====> Test set loss: 83.2684\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 63.409225\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 63.597775\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 63.812870\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 63.821693\n",
      "====> Epoch: 42 Average loss: 63.9834\n",
      "====> Test set loss: 90.7477\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 63.088356\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 63.797413\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 62.764374\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 63.860630\n",
      "====> Epoch: 43 Average loss: 63.9208\n",
      "====> Test set loss: 84.8181\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 63.711357\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 64.113892\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 63.317085\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 63.268475\n",
      "====> Epoch: 44 Average loss: 63.8979\n",
      "====> Test set loss: 86.8267\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 63.329037\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 64.768196\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 63.941944\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 63.889118\n",
      "====> Epoch: 45 Average loss: 63.9246\n",
      "====> Test set loss: 85.3709\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 64.034927\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 64.065956\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 63.607323\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 64.095634\n",
      "====> Epoch: 46 Average loss: 63.8591\n",
      "====> Test set loss: 85.2010\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 63.148525\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 63.351086\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 63.889854\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 63.952694\n",
      "====> Epoch: 47 Average loss: 63.8268\n",
      "====> Test set loss: 85.2323\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 64.006744\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 62.834538\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 63.809444\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 63.265884\n",
      "====> Epoch: 48 Average loss: 63.8268\n",
      "====> Test set loss: 86.3727\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 63.999229\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 64.446915\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 63.765244\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 63.517433\n",
      "====> Epoch: 49 Average loss: 63.7913\n",
      "====> Test set loss: 83.5573\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 64.116997\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 64.088387\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 64.132721\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 64.036415\n",
      "====> Epoch: 50 Average loss: 63.7846\n",
      "====> Test set loss: 82.7646\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5260, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(joint_vae_gumbel(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0967, 0.3074, 0.1439, 0.2103, 0.2154, 0.2160, 0.1038, 0.2318, 0.0368,\n",
       "        0.2081, 0.0256, 0.0530, 0.0988, 0.2806, 0.2221, 0.3266, 0.2796, 0.0111,\n",
       "        0.0358, 0.1023, 0.0869, 0.2511, 0.0086, 0.0275, 0.0058, 0.0881, 0.4742,\n",
       "        0.3314, 0.2171, 0.2956], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0352, 0.0423, 0.0907, 0.1022, 0.0288, 0.0402, 0.1274, 0.1182, 0.0396,\n",
       "        0.1329, 0.0187, 0.0539, 0.0739, 0.1255, 0.1855, 0.1602, 0.0281, 0.0109,\n",
       "        0.0186, 0.0299, 0.0880, 0.1090, 0.0096, 0.0145, 0.0059, 0.1265, 0.0483,\n",
       "        0.1183, 0.0780, 0.1779], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1005, 0.3697, 0.0888, 0.2939, 0.2604, 0.2632, 0.1520, 0.3305, 0.0683,\n",
       "        0.1272, 0.0525, 0.1012, 0.0634, 0.2336, 0.3295, 0.4207, 0.3195, 0.0291,\n",
       "        0.0738, 0.1620, 0.1530, 0.3479, 0.0185, 0.0526, 0.0151, 0.1193, 0.4787,\n",
       "        0.2654, 0.1946, 0.1838], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = joint_vae_gumbel.weight_creator(test_data[0:2, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57, 56, 40,  8, 13, 10,  7,  6, 24, 37, 50, 20, 42,  9, 25],\n",
       "        [ 7, 21, 25, 20, 15, 11,  3,  1,  6, 50, 13,  8, 10, 39, 33]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3 * z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint Training while selecting exactly z_size. Why does it pick the noise variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(2*D, 100, 20)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(2*D, 100, 20, k = z_size)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 84.478943\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 81.877838\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 79.880386\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 78.186386\n",
      "====> Epoch: 1 Average loss: 80.9338\n",
      "====> Test set loss: 78.4697\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 78.088425\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 76.408539\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 74.774490\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 74.107933\n",
      "====> Epoch: 2 Average loss: 75.8456\n",
      "====> Test set loss: 74.7870\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 73.678490\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 72.635223\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 70.574219\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 69.879906\n",
      "====> Epoch: 3 Average loss: 71.5512\n",
      "====> Test set loss: 74.8424\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 69.224770\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 68.924301\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 68.320763\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 68.044418\n",
      "====> Epoch: 4 Average loss: 68.7043\n",
      "====> Test set loss: 72.9656\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 67.914696\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 67.806984\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 67.002571\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 67.578979\n",
      "====> Epoch: 5 Average loss: 67.4974\n",
      "====> Test set loss: 71.4309\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 67.280563\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 67.348824\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 66.944008\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 67.180283\n",
      "====> Epoch: 6 Average loss: 66.9083\n",
      "====> Test set loss: 70.7739\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 65.981758\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 66.341148\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 66.138199\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 66.812286\n",
      "====> Epoch: 7 Average loss: 66.5465\n",
      "====> Test set loss: 70.2124\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 66.022270\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 67.211060\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 66.227013\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 66.185097\n",
      "====> Epoch: 8 Average loss: 66.3118\n",
      "====> Test set loss: 69.6274\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 65.742493\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 66.378281\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 65.968155\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 65.922966\n",
      "====> Epoch: 9 Average loss: 66.1446\n",
      "====> Test set loss: 69.2217\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 65.379486\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 65.609177\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 67.131668\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 65.232025\n",
      "====> Epoch: 10 Average loss: 66.0487\n",
      "====> Test set loss: 69.0091\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 67.432198\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 64.827316\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 65.900887\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 65.574120\n",
      "====> Epoch: 11 Average loss: 65.9425\n",
      "====> Test set loss: 68.6901\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 65.381592\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 64.768715\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 65.140259\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 65.357735\n",
      "====> Epoch: 12 Average loss: 65.8417\n",
      "====> Test set loss: 68.3949\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 65.972977\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 65.717117\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 65.733727\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 65.015907\n",
      "====> Epoch: 13 Average loss: 65.7615\n",
      "====> Test set loss: 68.9049\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 65.773338\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 65.938942\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 65.851608\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 65.518074\n",
      "====> Epoch: 14 Average loss: 65.6976\n",
      "====> Test set loss: 68.5675\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 66.120232\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 65.523125\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 65.932838\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 65.209900\n",
      "====> Epoch: 15 Average loss: 65.6352\n",
      "====> Test set loss: 68.2836\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 65.543846\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 66.358543\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 64.224350\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 65.815659\n",
      "====> Epoch: 16 Average loss: 65.5707\n",
      "====> Test set loss: 68.5546\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 65.528656\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 66.203041\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 65.456360\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 64.610939\n",
      "====> Epoch: 17 Average loss: 65.5175\n",
      "====> Test set loss: 68.3558\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 64.985321\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 64.965126\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 65.162666\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 65.465118\n",
      "====> Epoch: 18 Average loss: 65.4510\n",
      "====> Test set loss: 68.4074\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 65.648163\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 64.753036\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 64.667885\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 65.555771\n",
      "====> Epoch: 19 Average loss: 65.3893\n",
      "====> Test set loss: 67.8695\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 65.501755\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 65.051888\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 65.106125\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 66.933014\n",
      "====> Epoch: 20 Average loss: 65.3504\n",
      "====> Test set loss: 67.7665\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 65.529152\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 65.201950\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 65.250946\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 65.862167\n",
      "====> Epoch: 21 Average loss: 65.3273\n",
      "====> Test set loss: 67.5446\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 64.886925\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 66.026924\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 65.426308\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 65.249657\n",
      "====> Epoch: 22 Average loss: 65.2664\n",
      "====> Test set loss: 67.7729\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 65.796982\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 65.205658\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 64.447769\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 65.636322\n",
      "====> Epoch: 23 Average loss: 65.2285\n",
      "====> Test set loss: 67.4143\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 64.102974\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 65.561470\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 65.507790\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 65.357574\n",
      "====> Epoch: 24 Average loss: 65.1740\n",
      "====> Test set loss: 67.2423\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 64.595322\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 64.666801\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 65.735298\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 63.946606\n",
      "====> Epoch: 25 Average loss: 65.1500\n",
      "====> Test set loss: 67.4877\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 65.474503\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 65.126686\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 65.470306\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 64.871918\n",
      "====> Epoch: 26 Average loss: 65.1288\n",
      "====> Test set loss: 67.6765\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 64.271362\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 65.913055\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 65.243416\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 64.288712\n",
      "====> Epoch: 27 Average loss: 65.0853\n",
      "====> Test set loss: 67.5824\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 64.796532\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 64.478195\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 65.644585\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 64.471390\n",
      "====> Epoch: 28 Average loss: 65.0546\n",
      "====> Test set loss: 67.2934\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 65.218544\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 64.529701\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 65.150551\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 65.310928\n",
      "====> Epoch: 29 Average loss: 65.0296\n",
      "====> Test set loss: 67.7171\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 64.929924\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 65.032532\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 65.912483\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 65.904732\n",
      "====> Epoch: 30 Average loss: 64.9711\n",
      "====> Test set loss: 67.3148\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 64.569695\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 64.725883\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 64.408600\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 64.970154\n",
      "====> Epoch: 31 Average loss: 64.9526\n",
      "====> Test set loss: 67.9725\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 65.791122\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 64.368843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 64.756683\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 64.941551\n",
      "====> Epoch: 32 Average loss: 64.9359\n",
      "====> Test set loss: 67.5003\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 65.290535\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 65.672798\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 64.623581\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 63.564552\n",
      "====> Epoch: 33 Average loss: 64.8824\n",
      "====> Test set loss: 67.2242\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 64.408577\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 65.233421\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 64.797005\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 65.064407\n",
      "====> Epoch: 34 Average loss: 64.8511\n",
      "====> Test set loss: 67.1424\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 64.721779\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 64.455750\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 64.802505\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 64.632957\n",
      "====> Epoch: 35 Average loss: 64.8258\n",
      "====> Test set loss: 67.1752\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 64.781677\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 64.585129\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 64.589554\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 64.417587\n",
      "====> Epoch: 36 Average loss: 64.8135\n",
      "====> Test set loss: 67.0715\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 64.745354\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 65.094963\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 64.356293\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 64.338692\n",
      "====> Epoch: 37 Average loss: 64.7941\n",
      "====> Test set loss: 67.1288\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 65.894745\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 64.607933\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 64.172043\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 65.162254\n",
      "====> Epoch: 38 Average loss: 64.7812\n",
      "====> Test set loss: 67.0610\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 64.631165\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 64.150810\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 63.933880\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 64.382927\n",
      "====> Epoch: 39 Average loss: 64.7655\n",
      "====> Test set loss: 67.1198\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 64.140305\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 63.624210\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 64.744072\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 64.182205\n",
      "====> Epoch: 40 Average loss: 64.7381\n",
      "====> Test set loss: 67.5884\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 64.358002\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 63.901726\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 64.144417\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 64.553474\n",
      "====> Epoch: 41 Average loss: 64.7302\n",
      "====> Test set loss: 66.8312\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 64.351852\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 64.564690\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 64.992592\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 64.746719\n",
      "====> Epoch: 42 Average loss: 64.7273\n",
      "====> Test set loss: 67.1401\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 64.520912\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 64.396019\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 64.618958\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 64.277046\n",
      "====> Epoch: 43 Average loss: 64.7092\n",
      "====> Test set loss: 67.3040\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 64.174622\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 64.628265\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 64.204422\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 65.086990\n",
      "====> Epoch: 44 Average loss: 64.6874\n",
      "====> Test set loss: 67.5176\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 65.388062\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 63.921562\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 64.596359\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 63.833035\n",
      "====> Epoch: 45 Average loss: 64.6876\n",
      "====> Test set loss: 67.4682\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 64.628731\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 64.001060\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 64.416573\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 64.865402\n",
      "====> Epoch: 46 Average loss: 64.6696\n",
      "====> Test set loss: 68.3524\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 65.166176\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 65.174835\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 64.427483\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 64.784927\n",
      "====> Epoch: 47 Average loss: 64.6618\n",
      "====> Test set loss: 68.8743\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 65.083313\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 64.402618\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 64.252785\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 65.795982\n",
      "====> Epoch: 48 Average loss: 64.6425\n",
      "====> Test set loss: 68.9186\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 64.413895\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 65.418488\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 64.650650\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 64.624695\n",
      "====> Epoch: 49 Average loss: 64.6217\n",
      "====> Test set loss: 69.6174\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 65.455841\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 64.447174\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 65.077614\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 64.651657\n",
      "====> Epoch: 50 Average loss: 64.6142\n",
      "====> Test set loss: 70.3460\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5364, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(joint_vae_gumbel(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0975, 0.3006, 0.1469, 0.2061, 0.2143, 0.2091, 0.0950, 0.2251, 0.0322,\n",
       "        0.2164, 0.0268, 0.0494, 0.0996, 0.2896, 0.2137, 0.3202, 0.2812, 0.0122,\n",
       "        0.0336, 0.1001, 0.0814, 0.2440, 0.0090, 0.0279, 0.0081, 0.0785, 0.4761,\n",
       "        0.3386, 0.2208, 0.3097], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0408, 0.0346, 0.0405, 0.0356, 0.0378, 0.0496, 0.0487, 0.0248,\n",
       "        0.0397, 0.0207, 0.0279, 0.0306, 0.0362, 0.0594, 0.0502, 0.0309, 0.0132,\n",
       "        0.0227, 0.0398, 0.0363, 0.0528, 0.0102, 0.0206, 0.0090, 0.0450, 0.0176,\n",
       "        0.0300, 0.0341, 0.0467], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0759, 0.3064, 0.1190, 0.1617, 0.1704, 0.1844, 0.0581, 0.2103, 0.0106,\n",
       "        0.2191, 0.0066, 0.0216, 0.0811, 0.2912, 0.1632, 0.2801, 0.2655, 0.0029,\n",
       "        0.0134, 0.0689, 0.0450, 0.1810, 0.0010, 0.0119, 0.0015, 0.0405, 0.4814,\n",
       "        0.3562, 0.1910, 0.3094], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = joint_vae_gumbel.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57, 35, 40, 55,  6],\n",
       "        [ 8, 59,  5,  6, 38],\n",
       "        [50, 43,  6, 41, 54],\n",
       "        [59,  6, 25, 21, 50],\n",
       "        [30, 25,  8,  3, 11],\n",
       "        [13, 57, 55, 49, 34],\n",
       "        [52, 50, 33, 55, 44],\n",
       "        [57, 13, 50, 17,  2],\n",
       "        [46, 31, 35, 13, 49],\n",
       "        [ 6,  8, 38, 26, 59]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching pre trained is actually better here than joint training.\n",
    "The gumbel trick greatly reduces the ability to make predictions. \n",
    "Notice that the standard deviations are not as high as in the original data. Not being able to use a model looking at the full data as an anchor definitely hurts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Loss only over select features\n",
    "What happens if we limit the calculation of the loss to just the first few non-noisy stuff?\n",
    "\n",
    "Not doing joint training here because the calculation of loss is hidden inside utils and cannot modify the indexing so easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_truncated_with_gradients(df, model, optimizer, epoch, batch_size, Dim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    gradients = torch.zeros(df.shape[1]).to(device)\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone().to(device)\n",
    "        \n",
    "        \n",
    "        # need to do this twice because deriative with respect to input not implemented in BCE\n",
    "        # so need to switch them up\n",
    "        optimizer.zero_grad()\n",
    "        batch_data.requires_grad_(True)\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        # why clone detach here?\n",
    "        # still want gradient with respect to input, but BCE gradient with respect to target is not defined\n",
    "        # plus we only want to see how input affects mu_x, not the target\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim].clone().detach(), mu_x[:, :Dim], \n",
    "                                             mu_latent, logvar_latent) \n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gradients += torch.sqrt(batch_data.grad ** 2).sum(dim = 0)\n",
    "        # no step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # do not calculate with respect to \n",
    "        batch_data.requires_grad_(False)\n",
    "        mu_x.requires_grad_(True)\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim], mu_x[:, :Dim], mu_latent, logvar_latent) \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    return gradients\n",
    "    \n",
    "# match pre trained model\n",
    "def train_pre_trained_truncated(df, model, optimizer, epoch, pretrained_model, batch_size, D):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone()\n",
    "        \n",
    "        batch_data.requires_grad_(True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        with torch.no_grad():\n",
    "            _, mu_latent_2, logvar_latent_2 = pretrained_model(batch_data)\n",
    "        \n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :D], mu_x[:, :D], mu_latent, logvar_latent)\n",
    "        loss += 10*F.mse_loss(mu_latent, mu_latent_2, reduction = 'sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just train a gumbel without matching or joint training to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 20.898481\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 19.928968\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.049250\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 18.290642\n",
      "====> Epoch: 1 Average loss: 19.4542\n",
      "====> Test set loss: 39.1692\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 18.124523\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 17.402191\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 16.026787\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 15.008844\n",
      "====> Epoch: 2 Average loss: 16.6336\n",
      "====> Test set loss: 36.2676\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 14.873109\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 14.141089\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 13.461892\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 13.495072\n",
      "====> Epoch: 3 Average loss: 13.9418\n",
      "====> Test set loss: 35.0281\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 13.267741\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 12.905434\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 13.171424\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 12.867325\n",
      "====> Epoch: 4 Average loss: 13.1130\n",
      "====> Test set loss: 34.5379\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 13.128362\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 12.697511\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 13.054204\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 12.477242\n",
      "====> Epoch: 5 Average loss: 12.7828\n",
      "====> Test set loss: 34.2909\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 12.706622\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 12.238554\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.837653\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 12.341600\n",
      "====> Epoch: 6 Average loss: 12.5861\n",
      "====> Test set loss: 34.0988\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 12.763276\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.913714\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.146635\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 12.417777\n",
      "====> Epoch: 7 Average loss: 12.4853\n",
      "====> Test set loss: 33.9990\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.525644\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.667449\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.169458\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.394552\n",
      "====> Epoch: 8 Average loss: 12.4046\n",
      "====> Test set loss: 33.9118\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.584126\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.377527\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 12.044291\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 12.676594\n",
      "====> Epoch: 9 Average loss: 12.3281\n",
      "====> Test set loss: 33.8716\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.375000\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 12.248496\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 11.752567\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 12.320887\n",
      "====> Epoch: 10 Average loss: 12.2886\n",
      "====> Test set loss: 33.7976\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c74fb5550>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1092, 0.3102, 0.1590, 0.2244, 0.2203, 0.2235, 0.1158, 0.2418, 0.0548,\n",
       "        0.2233, 0.0514, 0.0799, 0.1109, 0.2903, 0.2281, 0.3313, 0.2916, 0.0392,\n",
       "        0.0631, 0.1157, 0.1028, 0.2585, 0.0370, 0.0495, 0.0345, 0.1025, 0.4735,\n",
       "        0.3372, 0.2318, 0.3076], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0473, 0.0384, 0.0520, 0.0631, 0.0463, 0.0494, 0.0489, 0.0506, 0.0409,\n",
       "        0.0569, 0.0389, 0.0513, 0.0500, 0.0375, 0.0457, 0.0416, 0.0484, 0.0369,\n",
       "        0.0451, 0.0478, 0.0505, 0.0509, 0.0311, 0.0371, 0.0308, 0.0505, 0.0220,\n",
       "        0.0297, 0.0537, 0.0394], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.0001).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c48488650>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[44, 46, 27, 33, 36, 14, 59, 49, 53, 54, 24,  0, 12, 10,  4],\n",
       "        [48, 10, 20, 46, 25, 14, 13, 12, 50, 22, 21, 28, 54, 32, 52],\n",
       "        [12, 16, 36, 31, 37, 24, 48, 17, 59, 14, 33,  6, 55, 52,  9],\n",
       "        [49, 41, 42, 43, 44,  0, 47, 12, 23, 56, 54, 28, 53, 58, 57],\n",
       "        [40,  9, 54, 20, 46, 59, 28, 53, 21, 58,  8,  7,  2, 30, 27],\n",
       "        [10, 22, 40, 25, 39, 49, 27, 28, 42, 58, 30,  8, 56, 57, 11],\n",
       "        [36, 28, 19, 18, 23,  0, 47, 24, 27, 21,  8,  6, 37, 53, 54],\n",
       "        [33,  9, 44, 15, 14, 40, 51, 39, 45,  7, 30, 32, 27, 37, 19],\n",
       "        [25, 39, 27, 11, 52, 33, 55, 22, 24, 31,  4, 23,  2, 49, 51],\n",
       "        [ 0, 46, 31, 32, 36, 25, 45, 48, 50, 55, 26, 29, 12,  8, 16]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3*z_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(72.9997)\n",
      "tensor(77.0003)\n"
     ]
    }
   ],
   "source": [
    "print(subset_indices[:, :D].sum())\n",
    "\n",
    "print(subset_indices[:, D:2*D].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(72, device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(w.argsort(descending= True)[:, :3*z_size] < 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you run it on all the data? And not truncate the loss to the first D features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 42.028778\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 40.915230\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 39.910862\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 39.394970\n",
      "====> Epoch: 1 Average loss: 40.4695\n",
      "====> Test set loss: 39.0649\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 39.088829\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.240669\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 37.056469\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 35.996536\n",
      "====> Epoch: 2 Average loss: 37.6893\n",
      "====> Test set loss: 35.8604\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 35.930138\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 34.903442\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 34.834145\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.325855\n",
      "====> Epoch: 3 Average loss: 34.9034\n",
      "====> Test set loss: 34.1504\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.267342\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 34.246933\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 33.465240\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.505730\n",
      "====> Epoch: 4 Average loss: 33.9496\n",
      "====> Test set loss: 33.6882\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 33.564644\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.386166\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.632290\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.638016\n",
      "====> Epoch: 5 Average loss: 33.5579\n",
      "====> Test set loss: 33.4723\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 33.485138\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.585220\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.102207\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 33.257530\n",
      "====> Epoch: 6 Average loss: 33.3877\n",
      "====> Test set loss: 33.3086\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 33.015835\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 33.281929\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 33.381119\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 32.970177\n",
      "====> Epoch: 7 Average loss: 33.2691\n",
      "====> Test set loss: 33.2034\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 33.040359\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 33.359299\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 32.975746\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 33.223404\n",
      "====> Epoch: 8 Average loss: 33.1554\n",
      "====> Test set loss: 33.0677\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 32.876152\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 33.120911\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 33.185116\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 33.158360\n",
      "====> Epoch: 9 Average loss: 33.0980\n",
      "====> Test set loss: 33.0538\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 33.006538\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 33.618176\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 33.208805\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 32.846165\n",
      "====> Epoch: 10 Average loss: 33.0552\n",
      "====> Test set loss: 32.9834\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = 2*D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c48355f10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1117, 0.3124, 0.1546, 0.2224, 0.2263, 0.2244, 0.1178, 0.2419, 0.0613,\n",
       "        0.2224, 0.0488, 0.0724, 0.1090, 0.2857, 0.2340, 0.3341, 0.2930, 0.0407,\n",
       "        0.0566, 0.1165, 0.1001, 0.2556, 0.0359, 0.0624, 0.0372, 0.1026, 0.4772,\n",
       "        0.3331, 0.2251, 0.3030], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0505, 0.0445, 0.0496, 0.0526, 0.0504, 0.0454, 0.0481, 0.0426, 0.0387,\n",
       "        0.0542, 0.0390, 0.0442, 0.0484, 0.0392, 0.0436, 0.0365, 0.0444, 0.0337,\n",
       "        0.0407, 0.0490, 0.0481, 0.0379, 0.0336, 0.0435, 0.0325, 0.0502, 0.0217,\n",
       "        0.0353, 0.0468, 0.0361], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_truncated(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.0001).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c48485790>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35, 18, 59, 44, 40, 31, 30,  1, 21,  0, 10,  9,  4,  3, 11],\n",
       "        [12, 31, 39, 23, 41, 43, 46, 47, 14, 33, 51, 52, 54, 57,  4],\n",
       "        [10,  9, 23, 24, 42, 26, 51, 27, 53, 19, 33,  7, 40, 39,  2],\n",
       "        [50, 51, 18, 42, 14, 40, 12, 11, 20, 27, 56, 36, 37, 22, 53],\n",
       "        [17, 51, 18, 42, 15, 14, 27, 11, 21, 40, 57,  4, 31, 33, 30],\n",
       "        [57, 50, 37, 28, 40, 54, 55, 32, 58,  3,  2,  6,  5,  7,  4],\n",
       "        [33, 51, 19, 18,  0, 41, 14, 11, 23, 28,  4, 45, 58, 57, 30],\n",
       "        [22, 28, 46, 16, 49, 12, 52, 36, 10, 33, 21,  2,  8, 31, 39],\n",
       "        [12, 29, 53, 51, 49, 46, 42, 32, 24, 13,  0,  2,  7,  3,  4],\n",
       "        [16, 19, 18, 59, 24, 49, 42, 52, 20, 58,  4,  2, 39, 36, 37]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3*z_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(75.9792)\n",
      "tensor(74.0208)\n"
     ]
    }
   ],
   "source": [
    "print(subset_indices[:, :D].sum())\n",
    "\n",
    "print(subset_indices[:, D:2*D].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80, device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(w.argsort(descending= True)[:, :3*z_size] < 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does a normal VAE do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to reconstruct first 30 features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = VAE(2*D, 100, 20)\n",
    "\n",
    "vanilla_vae.to(device)\n",
    "vanilla_vae_optimizer = torch.optim.Adam(vanilla_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 21.213346\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 20.276854\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.344076\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 18.498720\n",
      "====> Epoch: 1 Average loss: 19.7915\n",
      "====> Test set loss: 39.3484\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 18.456263\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 17.164484\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 15.890133\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 14.501512\n",
      "====> Epoch: 2 Average loss: 16.4995\n",
      "====> Test set loss: 36.2507\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 14.773250\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 13.784710\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 13.811372\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 13.806759\n",
      "====> Epoch: 3 Average loss: 13.9482\n",
      "====> Test set loss: 35.1174\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 13.142475\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 13.138114\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 12.924956\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 12.976542\n",
      "====> Epoch: 4 Average loss: 13.1579\n",
      "====> Test set loss: 34.6179\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 12.756412\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 12.345743\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 13.026613\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 12.524945\n",
      "====> Epoch: 5 Average loss: 12.7881\n",
      "====> Test set loss: 34.2605\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 12.716744\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 12.429401\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.454115\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 12.009907\n",
      "====> Epoch: 6 Average loss: 12.5590\n",
      "====> Test set loss: 34.1010\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 12.698843\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.421752\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.483682\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 11.963717\n",
      "====> Epoch: 7 Average loss: 12.3879\n",
      "====> Test set loss: 33.8407\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.339183\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.550796\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.321168\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.450256\n",
      "====> Epoch: 8 Average loss: 12.2676\n",
      "====> Test set loss: 33.7756\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.633069\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.031480\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 11.850620\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 12.413431\n",
      "====> Epoch: 9 Average loss: 12.1889\n",
      "====> Test set loss: 33.6582\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.027069\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 11.862024\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 12.223186\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 11.617983\n",
      "====> Epoch: 10 Average loss: 12.0970\n",
      "====> Test set loss: 33.5823\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vanilla_vae, \n",
    "                                         vanilla_vae_optimizer, epoch, batch_size, Dim = D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vanilla_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c482e0750>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1093, 0.3065, 0.1559, 0.2122, 0.2189, 0.2161, 0.1051, 0.2298, 0.0646,\n",
       "        0.2223, 0.0487, 0.0725, 0.1107, 0.2994, 0.2200, 0.3288, 0.2781, 0.0436,\n",
       "        0.0481, 0.1081, 0.0969, 0.2486, 0.0358, 0.0561, 0.0365, 0.0938, 0.4796,\n",
       "        0.3421, 0.2245, 0.3166], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0733, 0.0475, 0.0694, 0.0722, 0.0665, 0.0721, 0.0719, 0.0769, 0.0611,\n",
       "        0.0558, 0.0497, 0.0675, 0.0667, 0.0465, 0.0771, 0.0640, 0.0520, 0.0466,\n",
       "        0.0468, 0.0687, 0.0650, 0.0743, 0.0426, 0.0530, 0.0434, 0.0701, 0.0274,\n",
       "        0.0452, 0.0483, 0.0362], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = VAE(2*D, 100, 20)\n",
    "\n",
    "vanilla_vae.to(device)\n",
    "vanilla_vae_optimizer = torch.optim.Adam(vanilla_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 41.973228\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 41.085449\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 40.187717\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 39.287830\n",
      "====> Epoch: 1 Average loss: 40.5867\n",
      "====> Test set loss: 39.2181\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 39.292076\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.425259\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 37.164680\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 36.029900\n",
      "====> Epoch: 2 Average loss: 37.7599\n",
      "====> Test set loss: 36.0856\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 36.332581\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 35.325539\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 34.929386\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.419472\n",
      "====> Epoch: 3 Average loss: 35.1425\n",
      "====> Test set loss: 34.4717\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.692886\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 34.117592\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 33.713638\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.654694\n",
      "====> Epoch: 4 Average loss: 34.0944\n",
      "====> Test set loss: 33.8044\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 33.901157\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.830902\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.720039\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.133400\n",
      "====> Epoch: 5 Average loss: 33.6367\n",
      "====> Test set loss: 33.4560\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 33.693241\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.343418\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.421474\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 33.178448\n",
      "====> Epoch: 6 Average loss: 33.3628\n",
      "====> Test set loss: 33.2471\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 33.007824\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 33.203304\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 32.846340\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 33.542393\n",
      "====> Epoch: 7 Average loss: 33.1676\n",
      "====> Test set loss: 33.0630\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 33.325821\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 32.398952\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 33.350544\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 33.014492\n",
      "====> Epoch: 8 Average loss: 33.0612\n",
      "====> Test set loss: 32.9765\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 32.790531\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 32.727760\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 33.234215\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 32.502037\n",
      "====> Epoch: 9 Average loss: 32.9719\n",
      "====> Test set loss: 32.8750\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 32.742924\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 32.846191\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 33.125343\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 32.853436\n",
      "====> Epoch: 10 Average loss: 32.8908\n",
      "====> Test set loss: 32.8300\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vanilla_vae, \n",
    "                                         vanilla_vae_optimizer, epoch, batch_size, Dim = 2*D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vanilla_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c481df590>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1091, 0.3042, 0.1509, 0.2060, 0.2198, 0.2101, 0.1092, 0.2246, 0.0496,\n",
       "        0.2215, 0.0523, 0.0658, 0.1053, 0.2918, 0.2162, 0.3244, 0.2777, 0.0364,\n",
       "        0.0611, 0.1036, 0.0885, 0.2473, 0.0406, 0.0635, 0.0381, 0.0854, 0.4745,\n",
       "        0.3367, 0.2317, 0.3168], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0736, 0.0476, 0.0689, 0.0689, 0.0612, 0.0657, 0.0744, 0.0722, 0.0609,\n",
       "        0.0672, 0.0495, 0.0546, 0.0617, 0.0408, 0.0722, 0.0540, 0.0538, 0.0410,\n",
       "        0.0621, 0.0677, 0.0719, 0.0687, 0.0469, 0.0589, 0.0429, 0.0712, 0.0297,\n",
       "        0.0361, 0.0523, 0.0437], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are a few epochs are good for selecting of features as the Gumbel trick. Maybe even better.\n",
    "In fact, a Vanilla VAE is just as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
