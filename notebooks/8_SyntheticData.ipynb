{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know that Gumbel selects things relatively well. Its effects of Zeisel though are a bit muddled because of reconstruction. Let's do a simple synethetic dataset. Half the features are real. Half the features at noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_PATH_DATA = '../data/'\n",
    "BASE_PATH_DATA = '/scratch/ns3429/sparse-subset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 50\n",
    "#n_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 5000\n",
    "z_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=False)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=10, out_features=30, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7641,  0.4002,  0.9787,  2.2409,  1.8676],\n",
       "        [-0.9773,  0.9501, -0.1514, -0.1032,  0.4106],\n",
       "        [ 0.1440,  1.4543,  0.7610,  0.1217,  0.4439],\n",
       "        ...,\n",
       "        [ 0.2501, -1.0168,  0.0459,  0.5006,  1.2243],\n",
       "        [-0.5595,  1.5234, -0.5857,  0.8466, -0.1063],\n",
       "        [ 0.7700,  0.7508, -0.5606, -1.7603,  0.4371]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = data_mapper(latent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19)\n",
      "tensor(12)\n",
      "tensor(18)\n",
      "tensor(14)\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample, half the data is non zero, whereas in zeisel, about 25% if non zero. Easier than Zeisel good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0921e-02, -6.1085e-04, -1.4928e-02,  ..., -1.4309e-02,\n",
       "          1.6859e-02, -1.2177e-02],\n",
       "        [ 7.6496e-03,  1.1971e-02, -2.2414e-02,  ...,  1.0256e-02,\n",
       "         -5.5957e-03,  4.3434e-03],\n",
       "        [ 2.7566e-03,  1.0969e-03,  3.5942e-03,  ...,  6.0039e-03,\n",
       "          8.7524e-04,  7.0365e-03],\n",
       "        ...,\n",
       "        [ 1.8449e-02,  8.3797e-04, -8.9499e-03,  ...,  8.9735e-04,\n",
       "         -1.6982e-03,  7.8153e-03],\n",
       "        [-1.0649e-02, -9.6204e-03, -8.1562e-03,  ..., -2.2612e-04,\n",
       "         -1.4104e-02, -8.2127e-03],\n",
       "        [ 2.1183e-02, -1.1416e-02,  1.8769e-03,  ..., -1.3100e-02,\n",
       "         -6.2333e-03, -4.3646e-05]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_features = torch.empty(N * D).normal_(mean=0,std=0.01).reshape(N, D).to(device)\n",
    "noise_features.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = torch.cat([actual_data, noise_features], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 60])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1866, 0.2313, 0.2054, 0.2209, 0.2323, 0.1899, 0.1801, 0.1969, 0.1133,\n",
       "        0.2353, 0.0925, 0.1310, 0.1725, 0.1902, 0.2294, 0.2275, 0.2082, 0.0530,\n",
       "        0.0980, 0.1738, 0.1728, 0.2156, 0.0460, 0.0932, 0.0255, 0.1816, 0.1587,\n",
       "        0.2263, 0.2125, 0.2393, 0.1326, 0.1439, 0.1281, 0.1421, 0.1297, 0.1413,\n",
       "        0.1492, 0.1272, 0.1233, 0.1420, 0.1422, 0.1363, 0.1256, 0.1288, 0.1377,\n",
       "        0.1438, 0.1337, 0.1331, 0.1258, 0.1346, 0.1507, 0.1223, 0.1429, 0.1343,\n",
       "        0.1348, 0.1361, 0.1388, 0.1391, 0.1426, 0.1383])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.std(dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviatiosn are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "indices=tensor([ 738, 4262, 1553, 1484, 2220, 1316, 3892, 1316, 3121, 3883, 1838,  623,\n",
       "        1004, 4856,  689, 2033, 2038, 1316, 4515, 4562, 4668,  616,  894, 4515,\n",
       "        1885, 3892, 4615,  819, 4397, 4293,  713, 2220, 3813, 4659, 4389, 3659,\n",
       "         309, 1804,  495, 4790, 3110, 4671,   36,    0, 1215,  148, 4008, 1317,\n",
       "        2503, 1402, 1580, 2684, 4078, 3334, 1376, 2499, 1301, 3114, 4203, 3183]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.max(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "indices=tensor([4997, 4996, 4997, 4999, 4998, 4998, 4999, 4998, 4999, 4998, 4999, 4999,\n",
       "        4998, 4992, 4999, 4990, 4998, 4999, 4999, 4997, 4999, 4998, 4999, 4999,\n",
       "        4999, 4999,  254, 4997, 4997, 4997,  454, 3677, 1909, 3750, 3638, 4476,\n",
       "         553, 4105,  289, 1150, 2707, 1846, 3579, 3101,  299, 1324, 3277, 4318,\n",
       "        3023,  967, 2932, 3588,  919, 1190,  271, 2937, 3428, 3955, 2719,  198]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.min(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49)\n",
      "tensor(42)\n",
      "tensor(48)\n",
      "tensor(44)\n",
      "tensor(44)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 60])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 60])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_correlations(dataset):\n",
    "    cor_df = np.zeros((dataset.shape[1], dataset.shape[1]))\n",
    "    for row in np.arange(dataset.shape[1]):\n",
    "        for col in np.arange(dataset.shape[1]):\n",
    "            cor_df[row, col] = pearsonr(dataset[row], dataset[col])[0]\n",
    "    return cor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_data_clone = actual_data.clone().detach().cpu().numpy()\n",
    "\n",
    "pairwise_cor_df = pairwise_correlations(actual_data_clone)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(pairwise_cor_df)\n",
    "\n",
    "correlates = []\n",
    "for i in range(2 * D):\n",
    "    one = np.sum(np.abs(pairwise_cor_df[i, :D]))\n",
    "    two = np.sum(np.abs(pairwise_cor_df[i, D:2*D]))\n",
    "    #correlates.append([one, two])\n",
    "    correlates.append(one > two)\n",
    "    \n",
    "# correlation of that feature with the first 30 features and the second 30 features\n",
    "print(np.sum(correlates[:D]))\n",
    "\n",
    "print(np.sum(correlates[D:2*D]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is all ready. Now time to feed into into a pretraining-matching Gumbel and joint training Gumbel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre train VAE First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae = VAE(2*D, 100, 20)\n",
    "\n",
    "pretrain_vae.to(device)\n",
    "pretrain_vae_optimizer = torch.optim.Adam(pretrain_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))\n",
    "\n",
    "#pretrain_vae_optimizer = torch.optim.SGD(pretrain_vae.parameters(), \n",
    "#                                            lr=lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 42.017113\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 40.903053\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 39.910145\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 39.210461\n",
      "====> Epoch: 1 Average loss: 40.4244\n",
      "====> Test set loss: 38.9524\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 38.888924\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.212513\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 36.419609\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 35.664536\n",
      "====> Epoch: 2 Average loss: 37.3049\n",
      "====> Test set loss: 35.4656\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 35.430317\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 34.606815\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 34.577702\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.209206\n",
      "====> Epoch: 3 Average loss: 34.6850\n",
      "====> Test set loss: 34.1042\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.345818\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 34.207222\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 33.563995\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.908337\n",
      "====> Epoch: 4 Average loss: 33.8812\n",
      "====> Test set loss: 33.6500\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 33.996525\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.835045\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.518723\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.168392\n",
      "====> Epoch: 5 Average loss: 33.5430\n",
      "====> Test set loss: 33.3353\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 33.442101\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.615280\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.352787\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 33.062305\n",
      "====> Epoch: 6 Average loss: 33.3164\n",
      "====> Test set loss: 33.1817\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 33.309429\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 33.084484\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 33.366402\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 33.056320\n",
      "====> Epoch: 7 Average loss: 33.1319\n",
      "====> Test set loss: 33.0265\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 33.327454\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 33.176674\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 33.073765\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 32.693161\n",
      "====> Epoch: 8 Average loss: 33.0185\n",
      "====> Test set loss: 32.9396\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 32.457218\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 32.928253\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 32.993736\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 32.937096\n",
      "====> Epoch: 9 Average loss: 32.9161\n",
      "====> Test set loss: 32.8299\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 32.924587\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 32.558769\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 32.684296\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 33.207237\n",
      "====> Epoch: 10 Average loss: 32.8035\n",
      "====> Test set loss: 32.6680\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 32.653099\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 32.831394\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 32.375183\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 32.626377\n",
      "====> Epoch: 11 Average loss: 32.5824\n",
      "====> Test set loss: 32.3997\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 32.514385\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 32.200542\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 32.589878\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 32.178410\n",
      "====> Epoch: 12 Average loss: 32.3290\n",
      "====> Test set loss: 32.1813\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 32.289268\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 32.206417\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 32.278069\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 32.369549\n",
      "====> Epoch: 13 Average loss: 32.1558\n",
      "====> Test set loss: 32.0547\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 31.900909\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 32.196495\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 32.229164\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 32.197395\n",
      "====> Epoch: 14 Average loss: 32.0321\n",
      "====> Test set loss: 31.9535\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 31.856623\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 31.696789\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 31.695568\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 31.998665\n",
      "====> Epoch: 15 Average loss: 31.9635\n",
      "====> Test set loss: 31.8698\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 31.955343\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 31.983461\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 32.110096\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 32.054447\n",
      "====> Epoch: 16 Average loss: 31.9013\n",
      "====> Test set loss: 31.8082\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 31.771000\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 32.020573\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 31.843025\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 31.751513\n",
      "====> Epoch: 17 Average loss: 31.8380\n",
      "====> Test set loss: 31.7816\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 31.683304\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 31.588379\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 31.705652\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 31.933746\n",
      "====> Epoch: 18 Average loss: 31.7766\n",
      "====> Test set loss: 31.6849\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 32.360500\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 31.724262\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 31.859814\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 31.709867\n",
      "====> Epoch: 19 Average loss: 31.7122\n",
      "====> Test set loss: 31.6376\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 31.779974\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 31.284533\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 31.391958\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 31.349340\n",
      "====> Epoch: 20 Average loss: 31.6303\n",
      "====> Test set loss: 31.5404\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 31.590551\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 31.640583\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 31.818018\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 31.553604\n",
      "====> Epoch: 21 Average loss: 31.5221\n",
      "====> Test set loss: 31.4279\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 31.209724\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 31.501251\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 31.478521\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 31.363119\n",
      "====> Epoch: 22 Average loss: 31.3792\n",
      "====> Test set loss: 31.2837\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 31.283752\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 31.079319\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 31.267811\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 31.373390\n",
      "====> Epoch: 23 Average loss: 31.2691\n",
      "====> Test set loss: 31.2361\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 31.072739\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 31.460453\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 31.270405\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 30.968897\n",
      "====> Epoch: 24 Average loss: 31.1697\n",
      "====> Test set loss: 31.1025\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 31.283909\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 31.004129\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 31.003883\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 31.388237\n",
      "====> Epoch: 25 Average loss: 31.0785\n",
      "====> Test set loss: 30.9833\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 30.887918\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 30.966131\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 30.969601\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 30.970596\n",
      "====> Epoch: 26 Average loss: 30.9724\n",
      "====> Test set loss: 30.9033\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 30.640602\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 30.771666\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 31.104567\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 30.618370\n",
      "====> Epoch: 27 Average loss: 30.9081\n",
      "====> Test set loss: 30.8173\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 30.784786\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 30.941294\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 30.863577\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 30.868454\n",
      "====> Epoch: 28 Average loss: 30.8511\n",
      "====> Test set loss: 30.7931\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 30.984713\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 30.895269\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 30.978951\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 30.716114\n",
      "====> Epoch: 29 Average loss: 30.8103\n",
      "====> Test set loss: 30.7490\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 31.057421\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 30.876495\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 30.588932\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 30.662325\n",
      "====> Epoch: 30 Average loss: 30.7786\n",
      "====> Test set loss: 30.7362\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 30.691622\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 30.873983\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 30.858854\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 30.906868\n",
      "====> Epoch: 31 Average loss: 30.7436\n",
      "====> Test set loss: 30.6999\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 30.614374\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 30.584618\n",
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 30.891748\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 30.780199\n",
      "====> Epoch: 32 Average loss: 30.7141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 30.6768\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 30.802919\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 30.749544\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 30.829163\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 30.446085\n",
      "====> Epoch: 33 Average loss: 30.6890\n",
      "====> Test set loss: 30.6682\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 30.434799\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 30.970924\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 30.403496\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 30.874573\n",
      "====> Epoch: 34 Average loss: 30.6707\n",
      "====> Test set loss: 30.6344\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 30.790586\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 30.480667\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 30.827261\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 30.903517\n",
      "====> Epoch: 35 Average loss: 30.6488\n",
      "====> Test set loss: 30.6218\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 31.043398\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 30.573771\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 30.547482\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 30.535990\n",
      "====> Epoch: 36 Average loss: 30.6306\n",
      "====> Test set loss: 30.6092\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 30.713840\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 31.007141\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 30.307425\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 30.344015\n",
      "====> Epoch: 37 Average loss: 30.6262\n",
      "====> Test set loss: 30.5888\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 30.955111\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 30.717663\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 30.759192\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 30.585281\n",
      "====> Epoch: 38 Average loss: 30.5836\n",
      "====> Test set loss: 30.5827\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 30.699099\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 30.553802\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 30.305653\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 30.460131\n",
      "====> Epoch: 39 Average loss: 30.5795\n",
      "====> Test set loss: 30.5595\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 30.779570\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 30.416681\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 30.851614\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 30.504482\n",
      "====> Epoch: 40 Average loss: 30.5620\n",
      "====> Test set loss: 30.5409\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 30.623724\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 30.480110\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 30.459265\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 30.428608\n",
      "====> Epoch: 41 Average loss: 30.5471\n",
      "====> Test set loss: 30.5488\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 30.495504\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 30.636826\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 30.621056\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 30.647179\n",
      "====> Epoch: 42 Average loss: 30.5442\n",
      "====> Test set loss: 30.4980\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 30.441555\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 30.395248\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 30.599918\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 30.662346\n",
      "====> Epoch: 43 Average loss: 30.5185\n",
      "====> Test set loss: 30.5054\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 30.723736\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 30.521111\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 30.387514\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 30.585745\n",
      "====> Epoch: 44 Average loss: 30.5106\n",
      "====> Test set loss: 30.4933\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 30.558941\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 30.572298\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 30.252615\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 30.509411\n",
      "====> Epoch: 45 Average loss: 30.5017\n",
      "====> Test set loss: 30.4653\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 30.593023\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 30.848335\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 30.790195\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 30.339924\n",
      "====> Epoch: 46 Average loss: 30.4843\n",
      "====> Test set loss: 30.4524\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 30.619297\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 30.143131\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 30.599688\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 30.458723\n",
      "====> Epoch: 47 Average loss: 30.4707\n",
      "====> Test set loss: 30.4806\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 30.644178\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 30.505157\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 30.275278\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 30.209518\n",
      "====> Epoch: 48 Average loss: 30.4628\n",
      "====> Test set loss: 30.4370\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 30.555466\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 30.528658\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 30.316772\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 30.543491\n",
      "====> Epoch: 49 Average loss: 30.4437\n",
      "====> Test set loss: 30.4201\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 30.503532\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 30.464394\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 30.484758\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 30.143312\n",
      "====> Epoch: 50 Average loss: 30.4296\n",
      "====> Test set loss: 30.3976\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(train_data, pretrain_vae, pretrain_vae_optimizer, epoch, batch_size)\n",
    "    test(test_data, pretrain_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.4964, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(pretrain_vae(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually pretty good! %35 percent off when wrong\n",
    "\n",
    "Get 0.49 when nepochs is 50.\n",
    "Get 0.54 when nepochs is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, if the final layer of the data mapper is not ReLU, this reconstruction is usually on point. When some of the features can be sparse, then this becomes troublesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0931, 0.3051, 0.1384, 0.2096, 0.2092, 0.2112, 0.1042, 0.2325, 0.0347,\n",
       "        0.2014, 0.0244, 0.0502, 0.0919, 0.2735, 0.2254, 0.3294, 0.2786, 0.0084,\n",
       "        0.0338, 0.1001, 0.0849, 0.2512, 0.0048, 0.0257, 0.0015, 0.0874, 0.4764,\n",
       "        0.3270, 0.2107, 0.2899], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1014, 0.3082, 0.1481, 0.2059, 0.2035, 0.2026, 0.1036, 0.2276, 0.0334,\n",
       "        0.1933, 0.0268, 0.0544, 0.0878, 0.2829, 0.2261, 0.3300, 0.2708, 0.0094,\n",
       "        0.0353, 0.0994, 0.0870, 0.2518, 0.0070, 0.0294, 0.0036, 0.0922, 0.4750,\n",
       "        0.3314, 0.2135, 0.2963], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1635, 0.2089, 0.1991, 0.1973, 0.2046, 0.0988, 0.1592, 0.1667, 0.0784,\n",
       "        0.2134, 0.0471, 0.0845, 0.1464, 0.1706, 0.2144, 0.2154, 0.1740, 0.0129,\n",
       "        0.0778, 0.1471, 0.1372, 0.1853, 0.0119, 0.0618, 0.0055, 0.1587, 0.1384,\n",
       "        0.2182, 0.1869, 0.2146], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0192, 0.0166, 0.0169, 0.0182, 0.0186, 0.0176, 0.0177, 0.0193, 0.0181,\n",
       "        0.0182, 0.0176, 0.0181, 0.0175, 0.0176, 0.0170, 0.0169, 0.0160, 0.0168,\n",
       "        0.0176, 0.0168, 0.0176, 0.0161, 0.0185, 0.0188, 0.0188, 0.0182, 0.0179,\n",
       "        0.0173, 0.0182, 0.0180], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0].std(dim = 0)[D:2*D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_std = pretrain_vae(test_data)[0].std(dim = 0)[:D] / test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8577, 0.8759, 0.9402, 0.8815, 0.8857, 0.5150, 0.8801, 0.8438, 0.7950,\n",
      "        0.9072, 0.5083, 0.6143, 0.8757, 0.8771, 0.9157, 0.9198, 0.8350, 0.3095,\n",
      "        0.8366, 0.8784, 0.7831, 0.8066, 0.2013, 0.6737, 0.3344, 0.8627, 0.8543,\n",
      "        0.9643, 0.8786, 0.9155], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0.7742371559143066\n"
     ]
    }
   ],
   "source": [
    "print(average_std)\n",
    "print(average_std.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get .8 as the mean when nepoch is 50. Get 0.43 as the mean when nepochs is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0007, 0.4000, 0.0045, 0.1049, 0.6208, 0.3453, 0.0412, 0.3219, 0.0022,\n",
       "        0.2507, 0.0014, 0.0100, 0.0417, 0.0886, 0.3184, 0.5396, 0.2331, 0.0083,\n",
       "        0.0024, 0.0026, 0.0709, 0.5700, 0.0041, 0.0025, 0.0013, 0.0598, 0.3389,\n",
       "        0.0866, 0.5517, 0.3539], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(test_data[samp,:D] - pretrain_vae(test_data)[0][samp, :D]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0107, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae(test_data)[1][:, :D].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6380, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(pretrain_vae(test_data)[2][:, :D]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=60, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (enc_mean): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (enc_logvar): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=60, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good.\n",
    "\n",
    "**Gumbel matching pretrained VAE next**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_with_pre = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "vae_gumbel_with_pre.to(device)\n",
    "vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 135.722412\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 126.609497\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 121.512253\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 116.164474\n",
      "====> Epoch: 1 Average loss: 125.7550\n",
      "====> Test set loss: 39.4420\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 118.806061\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 115.479691\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 123.038879\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 118.022873\n",
      "====> Epoch: 2 Average loss: 120.1893\n",
      "====> Test set loss: 37.3728\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 109.040268\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 116.864220\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 106.289795\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 99.098221\n",
      "====> Epoch: 3 Average loss: 106.5753\n",
      "====> Test set loss: 35.5293\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 92.867889\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 92.626755\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 90.445953\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 83.077820\n",
      "====> Epoch: 4 Average loss: 92.0391\n",
      "====> Test set loss: 34.4310\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 94.296074\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 77.714569\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 81.084869\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 74.171280\n",
      "====> Epoch: 5 Average loss: 82.9036\n",
      "====> Test set loss: 33.8310\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 78.075912\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 70.341141\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 63.874229\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 61.171684\n",
      "====> Epoch: 6 Average loss: 65.8625\n",
      "====> Test set loss: 33.4171\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 57.562172\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 58.226929\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 54.463192\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 50.588814\n",
      "====> Epoch: 7 Average loss: 55.8087\n",
      "====> Test set loss: 33.0276\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 55.725613\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 51.121552\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 50.626991\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 49.758743\n",
      "====> Epoch: 8 Average loss: 52.0710\n",
      "====> Test set loss: 32.7353\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 50.725620\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 50.645775\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 48.536812\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 50.556717\n",
      "====> Epoch: 9 Average loss: 49.6258\n",
      "====> Test set loss: 32.4263\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 47.926109\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 47.804886\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 47.524033\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 45.503014\n",
      "====> Epoch: 10 Average loss: 47.7907\n",
      "====> Test set loss: 32.1936\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 46.048843\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 46.897743\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 44.097408\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 43.540649\n",
      "====> Epoch: 11 Average loss: 46.2466\n",
      "====> Test set loss: 31.9357\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 45.598862\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 46.120262\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 44.730377\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 44.540695\n",
      "====> Epoch: 12 Average loss: 45.0687\n",
      "====> Test set loss: 31.8322\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 45.227455\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 45.561817\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 42.888672\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 42.212193\n",
      "====> Epoch: 13 Average loss: 44.2510\n",
      "====> Test set loss: 31.6805\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 44.285442\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 42.699551\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 44.450256\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 42.885391\n",
      "====> Epoch: 14 Average loss: 43.4981\n",
      "====> Test set loss: 31.5295\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 43.120880\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 43.721817\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 41.357368\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 41.464832\n",
      "====> Epoch: 15 Average loss: 42.7544\n",
      "====> Test set loss: 31.4147\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 40.528797\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 43.701683\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 44.578201\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 41.733902\n",
      "====> Epoch: 16 Average loss: 42.2686\n",
      "====> Test set loss: 31.3458\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 42.929054\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 42.429470\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 42.658516\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 40.454048\n",
      "====> Epoch: 17 Average loss: 42.0751\n",
      "====> Test set loss: 31.2650\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 41.296978\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 41.057640\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 40.952259\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 41.607834\n",
      "====> Epoch: 18 Average loss: 41.6210\n",
      "====> Test set loss: 31.1831\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 40.142597\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 40.493942\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 41.663361\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 40.313290\n",
      "====> Epoch: 19 Average loss: 41.1337\n",
      "====> Test set loss: 31.1396\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 38.343391\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 43.186989\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 38.848911\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 40.579254\n",
      "====> Epoch: 20 Average loss: 40.6926\n",
      "====> Test set loss: 31.0904\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 39.104298\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 40.035809\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 41.526787\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 39.590034\n",
      "====> Epoch: 21 Average loss: 40.5537\n",
      "====> Test set loss: 31.0424\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 39.277164\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 41.596649\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 39.822006\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 38.502644\n",
      "====> Epoch: 22 Average loss: 40.3633\n",
      "====> Test set loss: 30.9810\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 41.492134\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 39.686409\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 39.374912\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 39.570984\n",
      "====> Epoch: 23 Average loss: 40.0200\n",
      "====> Test set loss: 30.9478\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 41.066231\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 39.722153\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 39.362839\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 41.070858\n",
      "====> Epoch: 24 Average loss: 40.0437\n",
      "====> Test set loss: 30.9370\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 39.159885\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 38.207764\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 38.474796\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 41.347359\n",
      "====> Epoch: 25 Average loss: 39.6715\n",
      "====> Test set loss: 30.8875\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 39.023853\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 39.097809\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 41.555843\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 37.815792\n",
      "====> Epoch: 26 Average loss: 39.3586\n",
      "====> Test set loss: 30.8734\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 39.684742\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 37.628872\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 39.488312\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 38.496597\n",
      "====> Epoch: 27 Average loss: 39.3501\n",
      "====> Test set loss: 30.8401\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 38.156860\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 38.214558\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 38.184345\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 39.237236\n",
      "====> Epoch: 28 Average loss: 39.0730\n",
      "====> Test set loss: 30.7946\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 38.855862\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 39.434158\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 39.349262\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 38.685936\n",
      "====> Epoch: 29 Average loss: 38.8034\n",
      "====> Test set loss: 30.7809\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 38.423027\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 38.675560\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 38.860107\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 38.933361\n",
      "====> Epoch: 30 Average loss: 38.9031\n",
      "====> Test set loss: 30.7613\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 38.892719\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 37.938801\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 38.100388\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 39.568184\n",
      "====> Epoch: 31 Average loss: 38.6412\n",
      "====> Test set loss: 30.7411\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 38.578812\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 37.562054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 38.562012\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 38.535431\n",
      "====> Epoch: 32 Average loss: 38.7111\n",
      "====> Test set loss: 30.7497\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 39.352974\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 38.365799\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 40.016819\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 36.582668\n",
      "====> Epoch: 33 Average loss: 38.2257\n",
      "====> Test set loss: 30.7124\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 36.885632\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 38.281082\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 38.087807\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 37.061802\n",
      "====> Epoch: 34 Average loss: 38.1512\n",
      "====> Test set loss: 30.7391\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 37.405006\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 36.905785\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 37.311352\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 38.416206\n",
      "====> Epoch: 35 Average loss: 38.2980\n",
      "====> Test set loss: 30.6733\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 39.994431\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 39.165375\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 38.501377\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 36.666950\n",
      "====> Epoch: 36 Average loss: 38.2152\n",
      "====> Test set loss: 30.6773\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 37.391823\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 37.541138\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 38.123650\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 37.897511\n",
      "====> Epoch: 37 Average loss: 37.9125\n",
      "====> Test set loss: 30.6877\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 37.122612\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 37.899662\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 37.728035\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 37.005970\n",
      "====> Epoch: 38 Average loss: 37.7708\n",
      "====> Test set loss: 30.6447\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 38.822205\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 36.942570\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 36.673691\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 37.845844\n",
      "====> Epoch: 39 Average loss: 37.6848\n",
      "====> Test set loss: 30.6441\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 36.944633\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 38.330536\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 37.176762\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 37.522594\n",
      "====> Epoch: 40 Average loss: 37.7043\n",
      "====> Test set loss: 30.6297\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 38.149426\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 36.964828\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 38.864342\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 38.696426\n",
      "====> Epoch: 41 Average loss: 37.6811\n",
      "====> Test set loss: 30.6402\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 37.739174\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 38.629017\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 38.088554\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 37.424843\n",
      "====> Epoch: 42 Average loss: 37.4285\n",
      "====> Test set loss: 30.6431\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 36.234150\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 36.521694\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 38.001583\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 36.447544\n",
      "====> Epoch: 43 Average loss: 37.5232\n",
      "====> Test set loss: 30.5873\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 35.822144\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 36.768387\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 37.996017\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 36.809685\n",
      "====> Epoch: 44 Average loss: 37.3715\n",
      "====> Test set loss: 30.5720\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 37.069199\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 36.494930\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 36.162689\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 36.900597\n",
      "====> Epoch: 45 Average loss: 37.1270\n",
      "====> Test set loss: 30.5939\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 37.495930\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 36.550056\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 36.974342\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 37.575371\n",
      "====> Epoch: 46 Average loss: 37.2622\n",
      "====> Test set loss: 30.5732\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 37.007774\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 37.243793\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 37.070442\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 36.741886\n",
      "====> Epoch: 47 Average loss: 37.1619\n",
      "====> Test set loss: 30.5707\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 37.652676\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 38.723778\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 37.901093\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 36.425674\n",
      "====> Epoch: 48 Average loss: 37.1602\n",
      "====> Test set loss: 30.5591\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 37.737724\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 37.379749\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 35.841042\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 37.430756\n",
      "====> Epoch: 49 Average loss: 36.9142\n",
      "====> Test set loss: 30.5727\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 37.110191\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 37.940578\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 37.048847\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 37.101440\n",
      "====> Epoch: 50 Average loss: 37.0909\n",
      "====> Test set loss: 30.5599\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, \n",
    "                      epoch, pretrain_vae, batch_size)\n",
    "    test(test_data, vae_gumbel_with_pre, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.4989, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(vae_gumbel_with_pre(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0952, 0.2989, 0.1417, 0.2104, 0.2064, 0.2060, 0.0998, 0.2277, 0.0326,\n",
       "        0.2021, 0.0250, 0.0541, 0.0915, 0.2805, 0.2235, 0.3298, 0.2769, 0.0084,\n",
       "        0.0316, 0.0938, 0.0890, 0.2452, 0.0058, 0.0260, 0.0032, 0.0875, 0.4759,\n",
       "        0.3291, 0.2102, 0.2966], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1582, 0.1962, 0.1938, 0.1979, 0.1960, 0.0950, 0.1551, 0.1640, 0.0684,\n",
       "        0.2129, 0.0405, 0.0839, 0.1462, 0.1689, 0.2100, 0.2093, 0.1674, 0.0097,\n",
       "        0.0663, 0.1370, 0.1413, 0.1723, 0.0072, 0.0467, 0.0036, 0.1531, 0.1370,\n",
       "        0.2105, 0.1771, 0.2142], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0036, 0.3848, 0.0125, 0.0872, 0.4108, 0.2515, 0.0508, 0.2489, 0.0047,\n",
       "        0.1727, 0.0030, 0.0104, 0.0376, 0.1713, 0.3161, 0.4981, 0.2082, 0.0111,\n",
       "        0.0056, 0.0140, 0.0380, 0.4384, 0.0068, 0.0056, 0.0024, 0.0459, 0.3856,\n",
       "        0.1690, 0.3979, 0.3343], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel_with_pre(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_with_pre.weight_creator(test_data[0:2, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 10, 54,  3,  1, 19,  7, 28, 58, 30, 20, 27, 23, 53, 21],\n",
       "        [31, 54, 14,  4, 25, 28,  6,  1,  3, 32,  7, 46, 21, 16, 27]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as long as feature index is lesss than 30, then it isn't picking noise\n",
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3 * z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(2*D, 100, 20)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 85.120331\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 83.020958\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 80.954147\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 79.361855\n",
      "====> Epoch: 1 Average loss: 81.9689\n",
      "====> Test set loss: 79.4539\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 79.080437\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 77.663467\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 76.020126\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 74.630486\n",
      "====> Epoch: 2 Average loss: 76.8986\n",
      "====> Test set loss: 75.8125\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 74.629433\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 73.278839\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 70.789421\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 70.161156\n",
      "====> Epoch: 3 Average loss: 72.2921\n",
      "====> Test set loss: 77.5574\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 70.520943\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 69.573349\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 68.896667\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 68.662308\n",
      "====> Epoch: 4 Average loss: 69.1299\n",
      "====> Test set loss: 76.7028\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 68.870155\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 67.649879\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 68.348282\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 67.160683\n",
      "====> Epoch: 5 Average loss: 67.7588\n",
      "====> Test set loss: 75.5308\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 67.019279\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 67.460571\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 67.243752\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 66.848442\n",
      "====> Epoch: 6 Average loss: 67.0532\n",
      "====> Test set loss: 74.2383\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 67.079002\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 66.801254\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 66.463577\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 66.573029\n",
      "====> Epoch: 7 Average loss: 66.6890\n",
      "====> Test set loss: 73.8677\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 66.033325\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 67.008614\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 67.191246\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 66.821388\n",
      "====> Epoch: 8 Average loss: 66.4315\n",
      "====> Test set loss: 71.9676\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 67.077827\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 66.302277\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 65.598946\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 66.065842\n",
      "====> Epoch: 9 Average loss: 66.2325\n",
      "====> Test set loss: 70.7316\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 65.642868\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 65.643806\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 65.742523\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 65.993820\n",
      "====> Epoch: 10 Average loss: 66.1159\n",
      "====> Test set loss: 70.4100\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 66.209656\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 64.953316\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 65.943794\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 66.302399\n",
      "====> Epoch: 11 Average loss: 66.0022\n",
      "====> Test set loss: 70.3888\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 66.858665\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 65.585823\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 66.158653\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 65.960205\n",
      "====> Epoch: 12 Average loss: 65.9310\n",
      "====> Test set loss: 70.8123\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 65.825790\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 66.025703\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 65.350441\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 65.198959\n",
      "====> Epoch: 13 Average loss: 65.8425\n",
      "====> Test set loss: 69.6681\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 65.330353\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 66.463913\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 65.913666\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 64.938507\n",
      "====> Epoch: 14 Average loss: 65.7635\n",
      "====> Test set loss: 70.0538\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 65.412621\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 65.030045\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 66.328560\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 65.469902\n",
      "====> Epoch: 15 Average loss: 65.7169\n",
      "====> Test set loss: 69.6493\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 65.872261\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 66.430664\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 65.589653\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 66.154572\n",
      "====> Epoch: 16 Average loss: 65.6784\n",
      "====> Test set loss: 70.0340\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 65.568878\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 64.930687\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 65.702370\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 66.113861\n",
      "====> Epoch: 17 Average loss: 65.6184\n",
      "====> Test set loss: 70.4173\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 65.870804\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 65.819801\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 64.452682\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 65.038269\n",
      "====> Epoch: 18 Average loss: 65.5786\n",
      "====> Test set loss: 70.6464\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 65.427666\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 66.409622\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 65.421684\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 65.626007\n",
      "====> Epoch: 19 Average loss: 65.4993\n",
      "====> Test set loss: 71.7177\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 65.730927\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 66.581512\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 65.286354\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 65.407127\n",
      "====> Epoch: 20 Average loss: 65.4497\n",
      "====> Test set loss: 70.9496\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 65.244766\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 65.323105\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 66.003639\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 65.625351\n",
      "====> Epoch: 21 Average loss: 65.4036\n",
      "====> Test set loss: 70.9505\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 64.755600\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 65.410507\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 64.434845\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 64.885323\n",
      "====> Epoch: 22 Average loss: 65.3242\n",
      "====> Test set loss: 75.2253\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 65.269173\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 66.075523\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 65.243851\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 65.600380\n",
      "====> Epoch: 23 Average loss: 65.2668\n",
      "====> Test set loss: 73.9586\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 65.012154\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 66.047707\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 64.880890\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 64.166130\n",
      "====> Epoch: 24 Average loss: 65.2261\n",
      "====> Test set loss: 73.5859\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 64.968445\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 65.726173\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 65.008385\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 65.235855\n",
      "====> Epoch: 25 Average loss: 65.1555\n",
      "====> Test set loss: 73.8823\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 65.461761\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 65.247643\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 66.449425\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 65.314026\n",
      "====> Epoch: 26 Average loss: 65.1168\n",
      "====> Test set loss: 77.4398\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 65.809090\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 65.458664\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 66.002693\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 64.983391\n",
      "====> Epoch: 27 Average loss: 65.0553\n",
      "====> Test set loss: 72.7579\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 65.870384\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 64.665024\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 65.199409\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 64.315048\n",
      "====> Epoch: 28 Average loss: 64.9983\n",
      "====> Test set loss: 73.8617\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 65.110031\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 65.458908\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 66.157150\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 64.014267\n",
      "====> Epoch: 29 Average loss: 64.9655\n",
      "====> Test set loss: 74.9844\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 64.800980\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 64.387314\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 64.484421\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 64.682220\n",
      "====> Epoch: 30 Average loss: 64.8800\n",
      "====> Test set loss: 76.2075\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 65.204781\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 65.658867\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 63.803402\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 64.515213\n",
      "====> Epoch: 31 Average loss: 64.8072\n",
      "====> Test set loss: 78.8404\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 65.716110\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 64.623779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 64.888695\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 64.616302\n",
      "====> Epoch: 32 Average loss: 64.7184\n",
      "====> Test set loss: 80.2254\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 65.033531\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 64.216110\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 65.966454\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 64.526108\n",
      "====> Epoch: 33 Average loss: 64.6671\n",
      "====> Test set loss: 78.1952\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 64.402298\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 64.677193\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 65.303062\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 65.423096\n",
      "====> Epoch: 34 Average loss: 64.5143\n",
      "====> Test set loss: 83.8939\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 64.876678\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 65.037285\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 64.742767\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 64.346825\n",
      "====> Epoch: 35 Average loss: 64.4184\n",
      "====> Test set loss: 91.9585\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 64.564316\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 64.665680\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 64.275558\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 64.832420\n",
      "====> Epoch: 36 Average loss: 64.3278\n",
      "====> Test set loss: 85.7973\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 64.367378\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 64.693970\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 64.921509\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 63.906216\n",
      "====> Epoch: 37 Average loss: 64.2733\n",
      "====> Test set loss: 88.1185\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 64.951561\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 63.682293\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 64.466873\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 64.627014\n",
      "====> Epoch: 38 Average loss: 64.2021\n",
      "====> Test set loss: 85.9474\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 64.832993\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 63.775391\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 64.050255\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 64.514023\n",
      "====> Epoch: 39 Average loss: 64.1281\n",
      "====> Test set loss: 86.8896\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 63.587822\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 64.476250\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 63.999409\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 63.912823\n",
      "====> Epoch: 40 Average loss: 64.0640\n",
      "====> Test set loss: 84.7211\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 63.294216\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 63.779594\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 63.684685\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 63.488869\n",
      "====> Epoch: 41 Average loss: 64.0101\n",
      "====> Test set loss: 83.2684\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 63.409225\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 63.597775\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 63.812870\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 63.821693\n",
      "====> Epoch: 42 Average loss: 63.9834\n",
      "====> Test set loss: 90.7477\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 63.088356\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 63.797413\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 62.764374\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 63.860630\n",
      "====> Epoch: 43 Average loss: 63.9208\n",
      "====> Test set loss: 84.8181\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 63.711357\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 64.113892\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 63.317085\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 63.268475\n",
      "====> Epoch: 44 Average loss: 63.8979\n",
      "====> Test set loss: 86.8267\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 63.329037\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 64.768196\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 63.941944\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 63.889118\n",
      "====> Epoch: 45 Average loss: 63.9246\n",
      "====> Test set loss: 85.3709\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 64.034927\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 64.065956\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 63.607323\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 64.095634\n",
      "====> Epoch: 46 Average loss: 63.8591\n",
      "====> Test set loss: 85.2010\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 63.148525\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 63.351086\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 63.889854\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 63.952694\n",
      "====> Epoch: 47 Average loss: 63.8268\n",
      "====> Test set loss: 85.2323\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 64.006744\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 62.834538\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 63.809444\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 63.265884\n",
      "====> Epoch: 48 Average loss: 63.8268\n",
      "====> Test set loss: 86.3727\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 63.999229\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 64.446915\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 63.765244\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 63.517433\n",
      "====> Epoch: 49 Average loss: 63.7913\n",
      "====> Test set loss: 83.5573\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 64.116997\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 64.088387\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 64.132721\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 64.036415\n",
      "====> Epoch: 50 Average loss: 63.7846\n",
      "====> Test set loss: 82.7646\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5260, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(joint_vae_gumbel(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0967, 0.3074, 0.1439, 0.2103, 0.2154, 0.2160, 0.1038, 0.2318, 0.0368,\n",
       "        0.2081, 0.0256, 0.0530, 0.0988, 0.2806, 0.2221, 0.3266, 0.2796, 0.0111,\n",
       "        0.0358, 0.1023, 0.0869, 0.2511, 0.0086, 0.0275, 0.0058, 0.0881, 0.4742,\n",
       "        0.3314, 0.2171, 0.2956], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0352, 0.0423, 0.0907, 0.1022, 0.0288, 0.0402, 0.1274, 0.1182, 0.0396,\n",
       "        0.1329, 0.0187, 0.0539, 0.0739, 0.1255, 0.1855, 0.1602, 0.0281, 0.0109,\n",
       "        0.0186, 0.0299, 0.0880, 0.1090, 0.0096, 0.0145, 0.0059, 0.1265, 0.0483,\n",
       "        0.1183, 0.0780, 0.1779], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1005, 0.3697, 0.0888, 0.2939, 0.2604, 0.2632, 0.1520, 0.3305, 0.0683,\n",
       "        0.1272, 0.0525, 0.1012, 0.0634, 0.2336, 0.3295, 0.4207, 0.3195, 0.0291,\n",
       "        0.0738, 0.1620, 0.1530, 0.3479, 0.0185, 0.0526, 0.0151, 0.1193, 0.4787,\n",
       "        0.2654, 0.1946, 0.1838], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = joint_vae_gumbel.weight_creator(test_data[0:2, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57, 56, 40,  8, 13, 10,  7,  6, 24, 37, 50, 20, 42,  9, 25],\n",
       "        [ 7, 21, 25, 20, 15, 11,  3,  1,  6, 50, 13,  8, 10, 39, 33]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3 * z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint Training while selecting exactly z_size. Why does it pick the noise variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(2*D, 100, 20)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(2*D, 100, 20, k = z_size)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 84.478943\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 81.877838\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 79.880386\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 78.186386\n",
      "====> Epoch: 1 Average loss: 80.9338\n",
      "====> Test set loss: 78.4697\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 78.088425\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 76.408539\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 74.774490\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 74.107933\n",
      "====> Epoch: 2 Average loss: 75.8456\n",
      "====> Test set loss: 74.7870\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 73.678490\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 72.635223\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 70.574219\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 69.879906\n",
      "====> Epoch: 3 Average loss: 71.5512\n",
      "====> Test set loss: 74.8424\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 69.224770\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 68.924301\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 68.320763\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 68.044418\n",
      "====> Epoch: 4 Average loss: 68.7043\n",
      "====> Test set loss: 72.9656\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 67.914696\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 67.806984\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 67.002571\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 67.578979\n",
      "====> Epoch: 5 Average loss: 67.4974\n",
      "====> Test set loss: 71.4309\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 67.280563\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 67.348824\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 66.944008\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 67.180283\n",
      "====> Epoch: 6 Average loss: 66.9083\n",
      "====> Test set loss: 70.7739\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 65.981758\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 66.341148\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 66.138199\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 66.812286\n",
      "====> Epoch: 7 Average loss: 66.5465\n",
      "====> Test set loss: 70.2124\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 66.022270\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 67.211060\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 66.227013\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 66.185097\n",
      "====> Epoch: 8 Average loss: 66.3118\n",
      "====> Test set loss: 69.6274\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 65.742493\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 66.378281\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 65.968155\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 65.922966\n",
      "====> Epoch: 9 Average loss: 66.1446\n",
      "====> Test set loss: 69.2217\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 65.379486\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 65.609177\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 67.131668\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 65.232025\n",
      "====> Epoch: 10 Average loss: 66.0487\n",
      "====> Test set loss: 69.0091\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 67.432198\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 64.827316\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 65.900887\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 65.574120\n",
      "====> Epoch: 11 Average loss: 65.9425\n",
      "====> Test set loss: 68.6901\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 65.381592\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 64.768715\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 65.140259\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 65.357735\n",
      "====> Epoch: 12 Average loss: 65.8417\n",
      "====> Test set loss: 68.3949\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 65.972977\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 65.717117\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 65.733727\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 65.015907\n",
      "====> Epoch: 13 Average loss: 65.7615\n",
      "====> Test set loss: 68.9049\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 65.773338\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 65.938942\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 65.851608\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 65.518074\n",
      "====> Epoch: 14 Average loss: 65.6976\n",
      "====> Test set loss: 68.5675\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 66.120232\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 65.523125\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 65.932838\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 65.209900\n",
      "====> Epoch: 15 Average loss: 65.6352\n",
      "====> Test set loss: 68.2836\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 65.543846\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 66.358543\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 64.224350\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 65.815659\n",
      "====> Epoch: 16 Average loss: 65.5707\n",
      "====> Test set loss: 68.5546\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 65.528656\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 66.203041\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 65.456360\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 64.610939\n",
      "====> Epoch: 17 Average loss: 65.5175\n",
      "====> Test set loss: 68.3558\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 64.985321\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 64.965126\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 65.162666\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 65.465118\n",
      "====> Epoch: 18 Average loss: 65.4510\n",
      "====> Test set loss: 68.4074\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 65.648163\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 64.753036\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 64.667885\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 65.555771\n",
      "====> Epoch: 19 Average loss: 65.3893\n",
      "====> Test set loss: 67.8695\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 65.501755\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 65.051888\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 65.106125\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 66.933014\n",
      "====> Epoch: 20 Average loss: 65.3504\n",
      "====> Test set loss: 67.7665\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 65.529152\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 65.201950\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 65.250946\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 65.862167\n",
      "====> Epoch: 21 Average loss: 65.3273\n",
      "====> Test set loss: 67.5446\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 64.886925\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 66.026924\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 65.426308\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 65.249657\n",
      "====> Epoch: 22 Average loss: 65.2664\n",
      "====> Test set loss: 67.7729\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 65.796982\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 65.205658\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 64.447769\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 65.636322\n",
      "====> Epoch: 23 Average loss: 65.2285\n",
      "====> Test set loss: 67.4143\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 64.102974\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 65.561470\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 65.507790\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 65.357574\n",
      "====> Epoch: 24 Average loss: 65.1740\n",
      "====> Test set loss: 67.2423\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 64.595322\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 64.666801\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 65.735298\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 63.946606\n",
      "====> Epoch: 25 Average loss: 65.1500\n",
      "====> Test set loss: 67.4877\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 65.474503\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 65.126686\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 65.470306\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 64.871918\n",
      "====> Epoch: 26 Average loss: 65.1288\n",
      "====> Test set loss: 67.6765\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 64.271362\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 65.913055\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 65.243416\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 64.288712\n",
      "====> Epoch: 27 Average loss: 65.0853\n",
      "====> Test set loss: 67.5824\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 64.796532\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 64.478195\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 65.644585\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 64.471390\n",
      "====> Epoch: 28 Average loss: 65.0546\n",
      "====> Test set loss: 67.2934\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 65.218544\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 64.529701\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 65.150551\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 65.310928\n",
      "====> Epoch: 29 Average loss: 65.0296\n",
      "====> Test set loss: 67.7171\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 64.929924\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 65.032532\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 65.912483\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 65.904732\n",
      "====> Epoch: 30 Average loss: 64.9711\n",
      "====> Test set loss: 67.3148\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 64.569695\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 64.725883\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 64.408600\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 64.970154\n",
      "====> Epoch: 31 Average loss: 64.9526\n",
      "====> Test set loss: 67.9725\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 65.791122\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 64.368843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 64.756683\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 64.941551\n",
      "====> Epoch: 32 Average loss: 64.9359\n",
      "====> Test set loss: 67.5003\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 65.290535\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 65.672798\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 64.623581\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 63.564552\n",
      "====> Epoch: 33 Average loss: 64.8824\n",
      "====> Test set loss: 67.2242\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 64.408577\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 65.233421\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 64.797005\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 65.064407\n",
      "====> Epoch: 34 Average loss: 64.8511\n",
      "====> Test set loss: 67.1424\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 64.721779\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 64.455750\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 64.802505\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 64.632957\n",
      "====> Epoch: 35 Average loss: 64.8258\n",
      "====> Test set loss: 67.1752\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 64.781677\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 64.585129\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 64.589554\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 64.417587\n",
      "====> Epoch: 36 Average loss: 64.8135\n",
      "====> Test set loss: 67.0715\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 64.745354\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 65.094963\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 64.356293\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 64.338692\n",
      "====> Epoch: 37 Average loss: 64.7941\n",
      "====> Test set loss: 67.1288\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 65.894745\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 64.607933\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 64.172043\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 65.162254\n",
      "====> Epoch: 38 Average loss: 64.7812\n",
      "====> Test set loss: 67.0610\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 64.631165\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 64.150810\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 63.933880\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 64.382927\n",
      "====> Epoch: 39 Average loss: 64.7655\n",
      "====> Test set loss: 67.1198\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 64.140305\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 63.624210\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 64.744072\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 64.182205\n",
      "====> Epoch: 40 Average loss: 64.7381\n",
      "====> Test set loss: 67.5884\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 64.358002\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 63.901726\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 64.144417\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 64.553474\n",
      "====> Epoch: 41 Average loss: 64.7302\n",
      "====> Test set loss: 66.8312\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 64.351852\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 64.564690\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 64.992592\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 64.746719\n",
      "====> Epoch: 42 Average loss: 64.7273\n",
      "====> Test set loss: 67.1401\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 64.520912\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 64.396019\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 64.618958\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 64.277046\n",
      "====> Epoch: 43 Average loss: 64.7092\n",
      "====> Test set loss: 67.3040\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 64.174622\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 64.628265\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 64.204422\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 65.086990\n",
      "====> Epoch: 44 Average loss: 64.6874\n",
      "====> Test set loss: 67.5176\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 65.388062\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 63.921562\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 64.596359\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 63.833035\n",
      "====> Epoch: 45 Average loss: 64.6876\n",
      "====> Test set loss: 67.4682\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 64.628731\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 64.001060\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 64.416573\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 64.865402\n",
      "====> Epoch: 46 Average loss: 64.6696\n",
      "====> Test set loss: 68.3524\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 65.166176\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 65.174835\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 64.427483\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 64.784927\n",
      "====> Epoch: 47 Average loss: 64.6618\n",
      "====> Test set loss: 68.8743\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 65.083313\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 64.402618\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 64.252785\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 65.795982\n",
      "====> Epoch: 48 Average loss: 64.6425\n",
      "====> Test set loss: 68.9186\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 64.413895\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 65.418488\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 64.650650\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 64.624695\n",
      "====> Epoch: 49 Average loss: 64.6217\n",
      "====> Test set loss: 69.6174\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 65.455841\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 64.447174\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 65.077614\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 64.651657\n",
      "====> Epoch: 50 Average loss: 64.6142\n",
      "====> Test set loss: 70.3460\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch, batch_size)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss\n",
      "tensor(0.5364, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Test Loss\")\n",
    "    print(F.binary_cross_entropy(joint_vae_gumbel(test_data)[0], test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0975, 0.3006, 0.1469, 0.2061, 0.2143, 0.2091, 0.0950, 0.2251, 0.0322,\n",
       "        0.2164, 0.0268, 0.0494, 0.0996, 0.2896, 0.2137, 0.3202, 0.2812, 0.0122,\n",
       "        0.0336, 0.1001, 0.0814, 0.2440, 0.0090, 0.0279, 0.0081, 0.0785, 0.4761,\n",
       "        0.3386, 0.2208, 0.3097], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0408, 0.0346, 0.0405, 0.0356, 0.0378, 0.0496, 0.0487, 0.0248,\n",
       "        0.0397, 0.0207, 0.0279, 0.0306, 0.0362, 0.0594, 0.0502, 0.0309, 0.0132,\n",
       "        0.0227, 0.0398, 0.0363, 0.0528, 0.0102, 0.0206, 0.0090, 0.0450, 0.0176,\n",
       "        0.0300, 0.0341, 0.0467], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5286, 0.0000, 0.0231, 0.6963, 0.3966, 0.0103, 0.2939, 0.0000,\n",
       "        0.3152, 0.0000, 0.0000, 0.0000, 0.0617, 0.4150, 0.6262, 0.2110, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.6829, 0.0000, 0.0000, 0.0000, 0.0000, 0.3516,\n",
       "        0.0000, 0.6491, 0.4560], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[samp,:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0759, 0.3064, 0.1190, 0.1617, 0.1704, 0.1844, 0.0581, 0.2103, 0.0106,\n",
       "        0.2191, 0.0066, 0.0216, 0.0811, 0.2912, 0.1632, 0.2801, 0.2655, 0.0029,\n",
       "        0.0134, 0.0689, 0.0450, 0.1810, 0.0010, 0.0119, 0.0015, 0.0405, 0.4814,\n",
       "        0.3562, 0.1910, 0.3094], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_vae_gumbel(test_data)[0][samp, :D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = joint_vae_gumbel.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57, 35, 40, 55,  6],\n",
       "        [ 8, 59,  5,  6, 38],\n",
       "        [50, 43,  6, 41, 54],\n",
       "        [59,  6, 25, 21, 50],\n",
       "        [30, 25,  8,  3, 11],\n",
       "        [13, 57, 55, 49, 34],\n",
       "        [52, 50, 33, 55, 44],\n",
       "        [57, 13, 50, 17,  2],\n",
       "        [46, 31, 35, 13, 49],\n",
       "        [ 6,  8, 38, 26, 59]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching pre trained is actually better here than joint training.\n",
    "The gumbel trick greatly reduces the ability to make predictions. \n",
    "Notice that the standard deviations are not as high as in the original data. Not being able to use a model looking at the full data as an anchor definitely hurts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Loss only over select features\n",
    "What happens if we limit the calculation of the loss to just the first few non-noisy stuff?\n",
    "\n",
    "Not doing joint training here because the calculation of loss is hidden inside utils and cannot modify the indexing so easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_truncated_with_gradients(df, model, optimizer, epoch, batch_size, Dim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    gradients = torch.zeros(df.shape[1])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone()\n",
    "        \n",
    "        \n",
    "        # need to do this twice because deriative with respect to input not implemented in BCE\n",
    "        # so need to switch them up\n",
    "        optimizer.zero_grad()\n",
    "        batch_data.requires_grad_(True)\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        # why clone detach here?\n",
    "        # still want gradient with respect to input, but BCE gradient with respect to target is not defined\n",
    "        # plus we only want to see how input affects mu_x, not the target\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim].clone().detach(), mu_x[:, :Dim], \n",
    "                                             mu_latent, logvar_latent) \n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gradients += torch.sqrt(batch_data.grad ** 2).sum(dim = 0)\n",
    "        # no step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # do not calculate with respect to \n",
    "        batch_data.requires_grad_(False)\n",
    "        mu_x.requires_grad_(True)\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim], mu_x[:, :Dim], mu_latent, logvar_latent) \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    return gradients\n",
    "    \n",
    "# match pre trained model\n",
    "def train_pre_trained_truncated(df, model, optimizer, epoch, pretrained_model, batch_size, D):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone()\n",
    "        \n",
    "        batch_data.requires_grad_(True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        with torch.no_grad():\n",
    "            _, mu_latent_2, logvar_latent_2 = pretrained_model(batch_data)\n",
    "        \n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :D], mu_x[:, :D], mu_latent, logvar_latent)\n",
    "        loss += 10*F.mse_loss(mu_latent, mu_latent_2, reduction = 'sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just train a gumbel without matching or joint training to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 21.103443\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 20.206802\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.229645\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 18.402851\n",
      "====> Epoch: 1 Average loss: 19.6576\n",
      "====> Test set loss: 39.3199\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 18.287004\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 17.456120\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 16.238079\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 14.711233\n",
      "====> Epoch: 2 Average loss: 16.6542\n",
      "====> Test set loss: 36.1960\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 14.447031\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 14.325089\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 13.524355\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 13.353219\n",
      "====> Epoch: 3 Average loss: 13.9206\n",
      "====> Test set loss: 34.8945\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 13.340278\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 13.343408\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 13.332027\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 13.038354\n",
      "====> Epoch: 4 Average loss: 13.1266\n",
      "====> Test set loss: 34.4031\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 12.855056\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 12.829342\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 12.551483\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 13.091331\n",
      "====> Epoch: 5 Average loss: 12.7904\n",
      "====> Test set loss: 34.1688\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 12.391103\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 12.582399\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.901088\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 12.256439\n",
      "====> Epoch: 6 Average loss: 12.5841\n",
      "====> Test set loss: 33.9693\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 13.006126\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.468227\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.469522\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 12.610761\n",
      "====> Epoch: 7 Average loss: 12.4586\n",
      "====> Test set loss: 33.8757\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.597450\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.299882\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.646302\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.130644\n",
      "====> Epoch: 8 Average loss: 12.3401\n",
      "====> Test set loss: 33.7700\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.435438\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.620152\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 12.264089\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 12.088250\n",
      "====> Epoch: 9 Average loss: 12.2575\n",
      "====> Test set loss: 33.7043\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.307338\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 12.223151\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 12.538885\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 12.747277\n",
      "====> Epoch: 10 Average loss: 12.1937\n",
      "====> Test set loss: 33.6568\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 12.583730\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 12.135897\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 11.526429\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 12.098818\n",
      "====> Epoch: 11 Average loss: 12.1216\n",
      "====> Test set loss: 33.5679\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 12.035725\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 11.951980\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 12.121884\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 11.836975\n",
      "====> Epoch: 12 Average loss: 12.0720\n",
      "====> Test set loss: 33.5534\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 12.605581\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 12.252321\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 12.051279\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 12.047996\n",
      "====> Epoch: 13 Average loss: 12.0256\n",
      "====> Test set loss: 33.4376\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 12.069750\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 12.160933\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 11.941130\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 11.954514\n",
      "====> Epoch: 14 Average loss: 11.9880\n",
      "====> Test set loss: 33.3794\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 11.892143\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 11.909328\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 11.844833\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 11.919641\n",
      "====> Epoch: 15 Average loss: 11.9207\n",
      "====> Test set loss: 33.3618\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 11.690919\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 11.596912\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 12.126680\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 11.717098\n",
      "====> Epoch: 16 Average loss: 11.8540\n",
      "====> Test set loss: 33.2103\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 12.052111\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 11.495130\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 11.507313\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 11.571838\n",
      "====> Epoch: 17 Average loss: 11.7411\n",
      "====> Test set loss: 33.1029\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 11.786332\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 11.614669\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 11.786354\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 11.726648\n",
      "====> Epoch: 18 Average loss: 11.5814\n",
      "====> Test set loss: 32.9990\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 11.857553\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 11.453579\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 11.059422\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 11.275011\n",
      "====> Epoch: 19 Average loss: 11.4826\n",
      "====> Test set loss: 32.8745\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1])\n",
    "for epoch in range(1, 20):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c5ff5a890>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22, 56, 46, 47, 53, 54, 55, 45, 49, 14, 31, 15, 48, 51, 12],\n",
       "        [15,  7, 47, 33, 25,  8,  1,  2, 24,  3, 32, 57,  4,  9, 18],\n",
       "        [46, 41, 55, 12, 59, 49, 34, 52, 53, 50, 30, 45, 57, 38, 56],\n",
       "        [ 8, 51, 36,  4, 33, 26, 25,  3, 14, 15, 39, 13, 29, 59, 19],\n",
       "        [51, 47, 36, 13, 22, 25, 15, 42,  9, 10, 14, 24, 33, 35,  3],\n",
       "        [54, 50, 39, 40, 52, 59, 34, 49, 37, 33, 46, 56, 55, 58, 53],\n",
       "        [33, 24, 34, 39, 45, 15, 26, 59, 16,  3, 58, 41,  4, 40, 23],\n",
       "        [33, 38, 54, 57, 27, 50, 59, 53, 29, 44, 48, 41, 45, 34, 39],\n",
       "        [41, 25,  7, 33, 27, 12, 14,  3, 26, 15, 37, 53, 38, 55, 52],\n",
       "        [36,  7,  8, 22, 33, 51, 46,  2, 23,  6, 14, 24, 25,  0, 40]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3*z_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you run it on all the data? And not truncate the loss to the first D features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(2*D, 100, 20, k = 3*z_size)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 41.741440\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 40.700535\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 39.856743\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 38.967194\n",
      "====> Epoch: 1 Average loss: 40.2913\n",
      "====> Test set loss: 38.9028\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 38.917130\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.058872\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 37.138599\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 36.115650\n",
      "====> Epoch: 2 Average loss: 37.4833\n",
      "====> Test set loss: 35.8502\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 36.252213\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 35.301136\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 34.480026\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.640163\n",
      "====> Epoch: 3 Average loss: 34.9531\n",
      "====> Test set loss: 34.3158\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.440681\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 34.273869\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 33.371895\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.925152\n",
      "====> Epoch: 4 Average loss: 33.9968\n",
      "====> Test set loss: 33.7479\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 33.729958\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.192104\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.539833\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.593903\n",
      "====> Epoch: 5 Average loss: 33.6002\n",
      "====> Test set loss: 33.4509\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 33.425434\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.298710\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.170731\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 32.936794\n",
      "====> Epoch: 6 Average loss: 33.3637\n",
      "====> Test set loss: 33.2351\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 33.167400\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 33.655766\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 33.171200\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 33.089798\n",
      "====> Epoch: 7 Average loss: 33.2200\n",
      "====> Test set loss: 33.1444\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 33.181454\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 33.480404\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 33.088215\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 33.568649\n",
      "====> Epoch: 8 Average loss: 33.1166\n",
      "====> Test set loss: 33.0672\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 32.850342\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 33.468853\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 32.945618\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 33.045372\n",
      "====> Epoch: 9 Average loss: 33.0099\n",
      "====> Test set loss: 32.9608\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 32.706181\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 32.691048\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 32.852058\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 32.849815\n",
      "====> Epoch: 10 Average loss: 32.9355\n",
      "====> Test set loss: 32.8547\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 33.121086\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 32.738068\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 33.255402\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 32.958492\n",
      "====> Epoch: 11 Average loss: 32.8679\n",
      "====> Test set loss: 32.8095\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 32.827538\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 33.055080\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 32.890793\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 32.438034\n",
      "====> Epoch: 12 Average loss: 32.8002\n",
      "====> Test set loss: 32.7461\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 33.209148\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 32.677425\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 32.535019\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 32.575535\n",
      "====> Epoch: 13 Average loss: 32.7390\n",
      "====> Test set loss: 32.6771\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 32.585953\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 32.684311\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 32.755898\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 32.607388\n",
      "====> Epoch: 14 Average loss: 32.6789\n",
      "====> Test set loss: 32.6017\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 33.136864\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 32.154800\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 32.874466\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 32.657951\n",
      "====> Epoch: 15 Average loss: 32.6162\n",
      "====> Test set loss: 32.5034\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 32.977375\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 32.370331\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 32.254951\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 32.422253\n",
      "====> Epoch: 16 Average loss: 32.4981\n",
      "====> Test set loss: 32.3946\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 32.210880\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 31.992201\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 32.314640\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 32.325581\n",
      "====> Epoch: 17 Average loss: 32.3844\n",
      "====> Test set loss: 32.2760\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 32.473579\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 32.125526\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 32.083111\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 31.907349\n",
      "====> Epoch: 18 Average loss: 32.2741\n",
      "====> Test set loss: 32.1650\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 32.596954\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 31.987713\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 31.724171\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 31.790878\n",
      "====> Epoch: 19 Average loss: 32.1903\n",
      "====> Test set loss: 32.1322\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1])\n",
    "for epoch in range(1, 20):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, 2*D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c5c19bd10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50,  3,  1, 27,  4, 14, 35, 29, 57, 11, 42, 41, 58, 38, 31],\n",
       "        [ 7, 12, 25, 41, 48,  6, 29, 14, 31, 37, 20, 38, 18, 24, 15],\n",
       "        [55, 30, 27, 59, 45, 56, 49, 35, 29, 15,  6, 36, 51,  9, 57],\n",
       "        [59, 48, 51, 14, 11, 25,  8, 45, 46, 34, 21, 30,  4,  2, 38],\n",
       "        [54, 15, 46,  0, 11,  6, 14, 25, 24, 21, 58, 12,  8, 17,  3],\n",
       "        [58, 52, 42, 50, 40, 55, 57, 27, 14, 41, 49, 37, 44, 59, 12],\n",
       "        [38,  6,  8, 48, 50, 59, 43,  7, 29, 26, 28, 46, 31, 47, 13],\n",
       "        [41, 35, 43, 56, 32, 26, 55, 29, 49, 59, 44, 51, 25, 52, 27],\n",
       "        [38,  6,  2, 27, 13,  1, 59, 35, 49, 21, 26, 48, 41, 14, 58],\n",
       "        [15, 21, 11, 34,  6,  8, 29, 25, 12, 55, 41, 51, 14, 17,  7]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(subset_indices, dim = 1, descending = True)[:, :3*z_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does a normal VAE do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to reconstruct first 30 features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = VAE(2*D, 100, 20)\n",
    "\n",
    "vanilla_vae.to(device)\n",
    "vanilla_vae_optimizer = torch.optim.Adam(vanilla_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 21.305817\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 20.217251\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.262314\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 18.368513\n",
      "====> Epoch: 1 Average loss: 19.7475\n",
      "====> Test set loss: 39.3018\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 18.102858\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 17.050991\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 15.593099\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 14.756241\n",
      "====> Epoch: 2 Average loss: 16.4262\n",
      "====> Test set loss: 36.1944\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 15.161070\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 13.857596\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 13.620502\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 13.551487\n",
      "====> Epoch: 3 Average loss: 13.9867\n",
      "====> Test set loss: 34.9108\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 13.556039\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 13.099985\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 13.147767\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 12.718060\n",
      "====> Epoch: 4 Average loss: 13.1675\n",
      "====> Test set loss: 34.3934\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 13.057909\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 12.951824\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 12.840936\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 12.606261\n",
      "====> Epoch: 5 Average loss: 12.7916\n",
      "====> Test set loss: 34.0871\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 12.854706\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 12.484241\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.104684\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 12.549754\n",
      "====> Epoch: 6 Average loss: 12.5493\n",
      "====> Test set loss: 33.8928\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 12.162035\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.481037\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.806499\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 12.735730\n",
      "====> Epoch: 7 Average loss: 12.3800\n",
      "====> Test set loss: 33.7390\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.273914\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.635716\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.351833\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.457133\n",
      "====> Epoch: 8 Average loss: 12.2646\n",
      "====> Test set loss: 33.6469\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.160340\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.496351\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 12.345266\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 12.476279\n",
      "====> Epoch: 9 Average loss: 12.1860\n",
      "====> Test set loss: 33.5760\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.364115\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 11.441708\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 12.588407\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 12.262075\n",
      "====> Epoch: 10 Average loss: 12.1183\n",
      "====> Test set loss: 33.4738\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 11.918593\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 12.007713\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 12.150164\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 11.907038\n",
      "====> Epoch: 11 Average loss: 12.0498\n",
      "====> Test set loss: 33.4442\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 11.842986\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 12.504944\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 11.491516\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 11.807207\n",
      "====> Epoch: 12 Average loss: 12.0059\n",
      "====> Test set loss: 33.3463\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 11.912459\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 12.243884\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 12.375504\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 11.509920\n",
      "====> Epoch: 13 Average loss: 11.9376\n",
      "====> Test set loss: 33.2614\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 11.881425\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 12.172265\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 11.570065\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 11.693096\n",
      "====> Epoch: 14 Average loss: 11.8359\n",
      "====> Test set loss: 33.1870\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 12.046391\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 11.687422\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 11.638872\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 11.532894\n",
      "====> Epoch: 15 Average loss: 11.6936\n",
      "====> Test set loss: 33.0123\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 11.544255\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 11.149473\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 11.794995\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 11.174697\n",
      "====> Epoch: 16 Average loss: 11.5004\n",
      "====> Test set loss: 32.8621\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 11.316328\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 11.675257\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 11.085747\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 10.940674\n",
      "====> Epoch: 17 Average loss: 11.3488\n",
      "====> Test set loss: 32.7179\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 11.323971\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 11.493971\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 11.246624\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 11.271722\n",
      "====> Epoch: 18 Average loss: 11.2135\n",
      "====> Test set loss: 32.6227\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 11.371073\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 11.299996\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 11.331904\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 11.015661\n",
      "====> Epoch: 19 Average loss: 11.1242\n",
      "====> Test set loss: 32.5546\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1])\n",
    "for epoch in range(1, 20):\n",
    "    grads=train_truncated_with_gradients(train_data, vanilla_vae, \n",
    "                                         vanilla_vae_optimizer, epoch, batch_size, Dim = D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vanilla_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c5fe496d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0979, 0.3085, 0.1449, 0.2156, 0.2186, 0.2130, 0.1177, 0.2351, 0.0472,\n",
       "        0.2119, 0.0334, 0.0639, 0.1105, 0.2820, 0.2370, 0.3378, 0.2777, 0.0218,\n",
       "        0.0429, 0.0983, 0.1002, 0.2566, 0.0204, 0.0363, 0.0248, 0.1031, 0.4758,\n",
       "        0.3246, 0.2225, 0.2987], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0579, 0.0559, 0.1085, 0.1091, 0.0537, 0.0600, 0.1252, 0.1215, 0.0574,\n",
       "        0.1510, 0.0409, 0.0655, 0.0851, 0.1341, 0.1951, 0.1774, 0.0542, 0.0305,\n",
       "        0.0500, 0.0591, 0.0875, 0.1167, 0.0308, 0.0407, 0.0338, 0.1189, 0.0534,\n",
       "        0.1344, 0.0915, 0.1886], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = VAE(2*D, 100, 20)\n",
    "\n",
    "vanilla_vae.to(device)\n",
    "vanilla_vae_optimizer = torch.optim.Adam(vanilla_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 42.336414\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 41.271889\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 40.370758\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 39.458405\n",
      "====> Epoch: 1 Average loss: 40.8466\n",
      "====> Test set loss: 39.3785\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 39.301117\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 38.128567\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 36.831554\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 36.009228\n",
      "====> Epoch: 2 Average loss: 37.5384\n",
      "====> Test set loss: 35.6281\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 36.001575\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 34.555130\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 34.854897\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 34.220776\n",
      "====> Epoch: 3 Average loss: 34.8348\n",
      "====> Test set loss: 34.2587\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 34.541420\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 33.951736\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 33.747082\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 33.662735\n",
      "====> Epoch: 4 Average loss: 33.9751\n",
      "====> Test set loss: 33.7372\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 34.025387\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 33.504284\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 33.781029\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 33.392941\n",
      "====> Epoch: 5 Average loss: 33.6041\n",
      "====> Test set loss: 33.4761\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 33.609634\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 33.652809\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 33.354446\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 33.336044\n",
      "====> Epoch: 6 Average loss: 33.3707\n",
      "====> Test set loss: 33.2578\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 32.989056\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 32.915840\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 33.119797\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 33.286308\n",
      "====> Epoch: 7 Average loss: 33.2156\n",
      "====> Test set loss: 33.1117\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 32.984386\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 33.078285\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 33.265011\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 33.184483\n",
      "====> Epoch: 8 Average loss: 33.1161\n",
      "====> Test set loss: 33.0253\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 33.161655\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 32.782619\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 32.739365\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 33.058495\n",
      "====> Epoch: 9 Average loss: 33.0322\n",
      "====> Test set loss: 32.9454\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 32.915535\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 32.938900\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 32.955681\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 32.870586\n",
      "====> Epoch: 10 Average loss: 32.9690\n",
      "====> Test set loss: 32.9112\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 33.128059\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 32.776230\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 32.967430\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 33.267872\n",
      "====> Epoch: 11 Average loss: 32.9024\n",
      "====> Test set loss: 32.8705\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 33.270527\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 33.031815\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 33.099899\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 33.045017\n",
      "====> Epoch: 12 Average loss: 32.8494\n",
      "====> Test set loss: 32.7915\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 33.128239\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 32.922569\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 32.625931\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 33.078568\n",
      "====> Epoch: 13 Average loss: 32.7871\n",
      "====> Test set loss: 32.7209\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 32.475311\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 32.935646\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 32.545357\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 32.393219\n",
      "====> Epoch: 14 Average loss: 32.6867\n",
      "====> Test set loss: 32.6229\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 32.529652\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 32.752712\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 32.364719\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 32.398014\n",
      "====> Epoch: 15 Average loss: 32.4837\n",
      "====> Test set loss: 32.3142\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 32.303123\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 32.266071\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 32.459091\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 31.658182\n",
      "====> Epoch: 16 Average loss: 32.2595\n",
      "====> Test set loss: 32.1569\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 32.426109\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 32.143883\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 31.826620\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 32.220314\n",
      "====> Epoch: 17 Average loss: 32.0955\n",
      "====> Test set loss: 32.0225\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 31.976044\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 32.169003\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 32.048458\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 31.883194\n",
      "====> Epoch: 18 Average loss: 32.0005\n",
      "====> Test set loss: 31.9429\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 31.967304\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 32.020027\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 32.046829\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 31.512016\n",
      "====> Epoch: 19 Average loss: 31.9448\n",
      "====> Test set loss: 31.8526\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1])\n",
    "for epoch in range(1, 20):\n",
    "    grads=train_truncated_with_gradients(train_data, vanilla_vae, \n",
    "                                         vanilla_vae_optimizer, epoch, batch_size, Dim = 2*D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    test(test_data, vanilla_vae, epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c70248d10>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.3031, 0.1394, 0.2099, 0.2063, 0.2098, 0.1076, 0.2316, 0.0298,\n",
       "        0.1968, 0.0261, 0.0561, 0.0848, 0.2727, 0.2320, 0.3355, 0.2739, 0.0056,\n",
       "        0.0301, 0.0953, 0.0889, 0.2483, 0.0063, 0.0231, 0.0009, 0.0924, 0.4782,\n",
       "        0.3251, 0.2092, 0.2881])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1010, 0.3092, 0.1469, 0.2290, 0.2274, 0.2215, 0.1265, 0.2500, 0.0448,\n",
       "        0.2038, 0.0379, 0.0765, 0.1067, 0.2704, 0.2583, 0.3559, 0.2803, 0.0242,\n",
       "        0.0423, 0.1012, 0.1113, 0.2679, 0.0257, 0.0371, 0.0246, 0.1123, 0.4798,\n",
       "        0.3141, 0.2224, 0.2895], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].mean(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1913, 0.2338, 0.2076, 0.2199, 0.2306, 0.1890, 0.1805, 0.1945, 0.1051,\n",
       "        0.2321, 0.0976, 0.1389, 0.1652, 0.1922, 0.2341, 0.2330, 0.2048, 0.0390,\n",
       "        0.0929, 0.1692, 0.1755, 0.2236, 0.0499, 0.0923, 0.0152, 0.1863, 0.1593,\n",
       "        0.2244, 0.2096, 0.2350])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0747, 0.0463, 0.1190, 0.1172, 0.0515, 0.0674, 0.1300, 0.1336, 0.0530,\n",
       "        0.1287, 0.0448, 0.0771, 0.0838, 0.1421, 0.1936, 0.1832, 0.0411, 0.0346,\n",
       "        0.0441, 0.0719, 0.1023, 0.1192, 0.0390, 0.0437, 0.0392, 0.1324, 0.0586,\n",
       "        0.1567, 0.0814, 0.1739], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae(test_data)[0].std(dim = 0)[:D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are a few epochs are good for selecting of features as the Gumbel trick. Maybe even better.\n",
    "In fact, a Vanilla VAE is just as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
