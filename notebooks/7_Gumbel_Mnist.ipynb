{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 40\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        BASE_PATH_DATA + '/mnist/train',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        BASE_PATH_DATA + '/mnist/test', \n",
    "        train=False, \n",
    "        download = True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor()]\n",
    "        )\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Gumbel_MNIST(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.01):\n",
    "        super(VAE_Gumbel_MNIST, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        # should probably add weight clipping to these gradients because you \n",
    "        # do not want the final output (initial logits) of this to be too big or too small\n",
    "        # (values between -1 and 10 for first output seem fine)\n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, img_size * img_size))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function_per_autoencoder(data.view(-1, img_size * img_size), recon_batch, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "def test(model, epoch, k):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function_per_autoencoder(data.view(-1, img_size * img_size), \n",
    "                                                       recon_batch, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         '../data/gumbel_on_mnist/k_{}_reconstruction_'.format(k) + \n",
    "                           str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "\n",
    "# any lower tends to produce sporadic results\n",
    "t = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_Gumbel_MNIST(784, 400, 20, k = k, t = t).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 548.272644\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 193.880295\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 199.551392\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 176.438690\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 168.237976\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 156.291382\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 146.051620\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 151.575043\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 139.460114\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 139.357834\n",
      "====> Epoch: 1 Average loss: 169.4069\n",
      "====> Test set loss: 137.7955\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 144.043121\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 142.749023\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 144.876480\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 135.891174\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 135.233948\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 134.314194\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 145.619339\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 141.249359\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 129.021439\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 141.049225\n",
      "====> Epoch: 2 Average loss: 138.4579\n",
      "====> Test set loss: 133.6308\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 133.604309\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 122.202919\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 127.331558\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 127.387573\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 139.187973\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 136.782196\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 134.538330\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 133.523300\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 125.358879\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 126.986374\n",
      "====> Epoch: 3 Average loss: 132.4119\n",
      "====> Test set loss: 127.2642\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 127.023605\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 118.038811\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 138.595688\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 126.096222\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 120.495117\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 136.592407\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 141.867477\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 135.945480\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 140.568008\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 133.632629\n",
      "====> Epoch: 4 Average loss: 130.1589\n",
      "====> Test set loss: 133.3609\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 128.898712\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 134.597092\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 128.267929\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 137.465332\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 138.260345\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 136.364609\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 129.062302\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 128.447418\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 135.980881\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 130.907349\n",
      "====> Epoch: 5 Average loss: 132.8290\n",
      "====> Test set loss: 130.0007\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 127.668900\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 130.243500\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 135.505844\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 118.283417\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 122.663422\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 122.108917\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 134.613770\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 128.497314\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 131.103592\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 127.839020\n",
      "====> Epoch: 6 Average loss: 128.6537\n",
      "====> Test set loss: 133.4287\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 134.126419\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 128.587982\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 128.605316\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 134.140244\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 132.560455\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 133.552368\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 129.847549\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 130.711960\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 143.969940\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 130.335983\n",
      "====> Epoch: 7 Average loss: 130.1510\n",
      "====> Test set loss: 129.2220\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 134.945663\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 125.348106\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 128.806335\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 129.572403\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 126.614700\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 133.220184\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 123.558876\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 131.144806\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 134.270844\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 125.997414\n",
      "====> Epoch: 8 Average loss: 130.1905\n",
      "====> Test set loss: 127.2329\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 134.033035\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 134.849670\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 128.169083\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 123.251968\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 128.146454\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 134.749496\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 143.162369\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 124.915428\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 124.150398\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 129.464905\n",
      "====> Epoch: 9 Average loss: 128.4089\n",
      "====> Test set loss: 127.2498\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 125.851456\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 129.530075\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 120.557343\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 138.989594\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 121.237968\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 129.573532\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 131.235870\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 126.826485\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 126.997986\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 129.649826\n",
      "====> Epoch: 10 Average loss: 127.6025\n",
      "====> Test set loss: 126.8489\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(model = model, optimizer = optimizer, epoch= epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    diag_model.diag.data[torch.abs(diag_model.diag) < 0.05] = 0\n",
    "        test(model, epoch, k)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/gumbel_on_mnist/k_{}_sample_'.format(k) + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, _) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_index = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = model.weight_creator(data[samp_index, :].view(-1, img_size * img_size))\n",
    "    subset_indices = sample_subset(w, k=k, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[4.4313e+00, 3.9491e+00, 2.0006e+00, 2.0000e+00, 1.8252e+00, 1.7829e+00,\n",
       "         1.0109e+00, 1.0002e+00, 1.0001e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "         9.9993e-01, 9.9977e-01, 6.1054e-01, 3.8945e-01, 1.0300e-05, 2.1860e-07,\n",
       "         4.9880e-08, 9.4936e-09, 6.9199e-09, 5.4256e-09, 3.2302e-09, 3.0438e-09,\n",
       "         2.8028e-09, 2.7042e-09, 2.5622e-09, 2.2953e-09, 2.2676e-09, 1.7808e-09,\n",
       "         1.5922e-09, 1.4989e-09, 1.0562e-09, 1.0123e-09, 7.8829e-10, 7.8070e-10,\n",
       "         5.6448e-10, 5.2509e-10, 2.4681e-10, 1.4397e-10, 9.9517e-11, 8.0048e-11,\n",
       "         7.4768e-11, 5.3834e-11, 5.2832e-11, 4.6600e-11, 4.4521e-11, 4.4296e-11,\n",
       "         3.2621e-11, 2.6647e-11, 2.4202e-11, 2.3765e-11, 2.3572e-11, 2.3056e-11,\n",
       "         2.1425e-11, 1.5054e-11, 1.4171e-11, 1.3824e-11, 1.3174e-11, 1.0539e-11,\n",
       "         1.0063e-11, 9.0390e-12, 7.8765e-12, 5.9020e-12, 5.3655e-12, 4.9208e-12,\n",
       "         4.0843e-12, 3.6180e-12, 3.5852e-12, 3.4444e-12, 3.3856e-12, 2.9518e-12,\n",
       "         2.8105e-12, 2.7995e-12, 2.6551e-12, 2.3884e-12, 2.3272e-12, 1.9875e-12,\n",
       "         1.7662e-12, 1.3864e-12, 1.3693e-12, 1.1686e-12, 1.0907e-12, 1.0837e-12,\n",
       "         1.0265e-12, 8.5780e-13, 8.4541e-13, 6.8510e-13, 6.5669e-13, 6.3668e-13,\n",
       "         6.3155e-13, 6.1249e-13, 5.7436e-13, 4.3831e-13, 3.8823e-13, 3.6358e-13,\n",
       "         3.2771e-13, 2.6937e-13, 2.6785e-13, 2.3863e-13, 2.1399e-13, 2.0138e-13,\n",
       "         1.9954e-13, 1.3640e-13, 1.2607e-13, 1.2253e-13, 9.8746e-14, 9.5904e-14,\n",
       "         8.4741e-14, 7.4653e-14, 7.3651e-14, 6.1330e-14, 5.7135e-14, 5.4673e-14,\n",
       "         5.1540e-14, 4.2945e-14, 4.1976e-14, 3.9282e-14, 3.9177e-14, 3.6537e-14,\n",
       "         3.5220e-14, 2.8779e-14, 2.2408e-14, 1.9819e-14, 1.9743e-14, 1.8954e-14,\n",
       "         1.6933e-14, 1.6828e-14, 1.6687e-14, 1.4914e-14, 1.4845e-14, 1.3762e-14,\n",
       "         1.1266e-14, 9.4729e-15, 9.4656e-15, 7.8235e-15, 7.7641e-15, 7.2773e-15,\n",
       "         6.5922e-15, 5.9220e-15, 5.5665e-15, 5.2324e-15, 4.8575e-15, 4.8399e-15,\n",
       "         3.0202e-15, 2.9510e-15, 2.6288e-15, 2.1881e-15, 1.8578e-15, 1.7447e-15,\n",
       "         1.7279e-15, 1.6326e-15, 1.5937e-15, 1.4938e-15, 1.4197e-15, 1.2186e-15,\n",
       "         1.1619e-15, 1.1598e-15, 7.8131e-16, 7.1549e-16, 6.2849e-16, 5.9105e-16,\n",
       "         5.4171e-16, 4.8292e-16, 4.7034e-16, 4.5251e-16, 3.9988e-16, 3.7393e-16,\n",
       "         2.3329e-16, 2.2604e-16, 1.4821e-16, 1.4162e-16, 1.3771e-16, 1.2390e-16,\n",
       "         1.0168e-16, 9.9928e-17, 8.3475e-17, 8.2718e-17, 7.9457e-17, 7.2010e-17,\n",
       "         6.0537e-17, 6.0111e-17, 5.0238e-17, 4.9388e-17, 4.8054e-17, 4.7614e-17,\n",
       "         4.5266e-17, 4.3590e-17, 4.3213e-17, 3.9872e-17, 3.7626e-17, 3.5417e-17,\n",
       "         3.3445e-17, 3.2875e-17, 3.1513e-17, 2.6571e-17, 2.3870e-17, 2.3182e-17,\n",
       "         2.2134e-17, 2.1075e-17, 2.0229e-17, 1.9934e-17, 1.9782e-17, 1.5540e-17,\n",
       "         1.4921e-17, 1.4827e-17, 1.4586e-17, 1.1197e-17, 1.0420e-17, 1.0014e-17,\n",
       "         8.3144e-18, 8.2051e-18, 7.2698e-18, 6.6728e-18, 6.1690e-18, 4.7499e-18,\n",
       "         4.7017e-18, 4.4377e-18, 4.2581e-18, 4.1274e-18, 2.9087e-18, 2.8857e-18,\n",
       "         2.7922e-18, 2.7302e-18, 2.5018e-18, 2.4136e-18, 2.2480e-18, 2.2403e-18,\n",
       "         2.2130e-18, 2.1516e-18, 2.1191e-18, 1.9665e-18, 1.7893e-18, 1.6389e-18,\n",
       "         1.5384e-18, 1.3756e-18, 1.3640e-18, 9.6447e-19, 9.3287e-19, 9.0064e-19,\n",
       "         8.8114e-19, 8.6778e-19, 8.1717e-19, 8.0365e-19, 7.0624e-19, 6.6998e-19,\n",
       "         6.2087e-19, 5.2204e-19, 5.0083e-19, 3.9298e-19, 3.5337e-19, 3.5280e-19,\n",
       "         3.1135e-19, 2.8292e-19, 2.7034e-19, 2.4457e-19, 2.3607e-19, 2.0858e-19,\n",
       "         1.9643e-19, 1.7940e-19, 1.7890e-19, 1.7271e-19, 1.1867e-19, 1.1846e-19,\n",
       "         1.1770e-19, 1.1404e-19, 1.0428e-19, 9.3618e-20, 8.9894e-20, 8.7454e-20,\n",
       "         8.0715e-20, 6.5105e-20, 6.1922e-20, 5.3224e-20, 5.0794e-20, 4.2206e-20,\n",
       "         4.0439e-20, 2.8756e-20, 2.8438e-20, 2.4137e-20, 2.4011e-20, 2.3123e-20,\n",
       "         2.2955e-20, 2.2903e-20, 2.0858e-20, 2.0147e-20, 1.9259e-20, 1.8042e-20,\n",
       "         1.6644e-20, 1.4366e-20, 1.4003e-20, 1.3997e-20, 1.3695e-20, 1.2501e-20,\n",
       "         1.1888e-20, 1.0670e-20, 9.8282e-21, 9.7730e-21, 9.6593e-21, 9.2756e-21,\n",
       "         8.8396e-21, 8.3685e-21, 6.5495e-21, 6.3983e-21, 6.0981e-21, 5.5961e-21,\n",
       "         5.5087e-21, 4.8639e-21, 4.8061e-21, 4.3516e-21, 4.1398e-21, 3.3768e-21,\n",
       "         2.8639e-21, 2.6318e-21, 2.5725e-21, 2.4800e-21, 2.2487e-21, 2.2345e-21,\n",
       "         1.9950e-21, 1.8831e-21, 1.8576e-21, 1.7933e-21, 1.5543e-21, 1.4359e-21,\n",
       "         1.1472e-21, 1.0201e-21, 9.2621e-22, 7.6566e-22, 7.2425e-22, 7.0960e-22,\n",
       "         7.0641e-22, 6.8694e-22, 6.0985e-22, 6.0834e-22, 5.6819e-22, 5.6689e-22,\n",
       "         5.3138e-22, 4.9567e-22, 4.8363e-22, 4.7759e-22, 4.7134e-22, 4.5301e-22,\n",
       "         4.1140e-22, 3.8560e-22, 3.4775e-22, 3.4663e-22, 3.4252e-22, 3.2867e-22,\n",
       "         3.0235e-22, 2.8837e-22, 2.4067e-22, 1.9756e-22, 1.5523e-22, 1.5237e-22,\n",
       "         1.5003e-22, 1.4873e-22, 1.4677e-22, 1.4101e-22, 1.3850e-22, 1.3245e-22,\n",
       "         1.3142e-22, 1.1933e-22, 1.1387e-22, 1.1307e-22, 1.1182e-22, 1.0641e-22,\n",
       "         9.6502e-23, 9.5277e-23, 9.3571e-23, 9.1097e-23, 8.8616e-23, 8.8479e-23,\n",
       "         8.5357e-23, 8.3411e-23, 8.1506e-23, 7.6053e-23, 7.5965e-23, 7.5497e-23,\n",
       "         6.9829e-23, 6.9059e-23, 6.7587e-23, 6.7520e-23, 5.9789e-23, 5.9061e-23,\n",
       "         5.4729e-23, 5.1892e-23, 4.6482e-23, 4.5036e-23, 4.3278e-23, 3.6268e-23,\n",
       "         3.2541e-23, 3.1597e-23, 3.0885e-23, 3.0065e-23, 2.8667e-23, 2.7942e-23,\n",
       "         2.7766e-23, 2.6821e-23, 2.6188e-23, 2.4935e-23, 2.2345e-23, 2.1525e-23,\n",
       "         1.9246e-23, 1.9210e-23, 1.7014e-23, 1.6868e-23, 1.6424e-23, 1.6226e-23,\n",
       "         1.5548e-23, 1.4542e-23, 1.4212e-23, 1.4135e-23, 1.3447e-23, 1.2551e-23,\n",
       "         1.2025e-23, 1.1064e-23, 1.0853e-23, 1.0514e-23, 9.4069e-24, 8.9930e-24,\n",
       "         8.7483e-24, 8.4155e-24, 8.2299e-24, 8.1644e-24, 7.8417e-24, 7.6181e-24,\n",
       "         7.4291e-24, 7.3749e-24, 7.1520e-24, 6.5319e-24, 6.4577e-24, 6.3391e-24,\n",
       "         5.7399e-24, 4.6480e-24, 4.2807e-24, 4.1584e-24, 4.1347e-24, 3.8001e-24,\n",
       "         3.7662e-24, 3.5095e-24, 3.4037e-24, 3.2613e-24, 2.8526e-24, 2.8002e-24,\n",
       "         2.6816e-24, 2.6332e-24, 2.6323e-24, 2.3814e-24, 2.3399e-24, 2.2583e-24,\n",
       "         2.2263e-24, 2.0190e-24, 1.9235e-24, 1.9182e-24, 1.8326e-24, 1.8049e-24,\n",
       "         1.7533e-24, 1.7057e-24, 1.6565e-24, 1.6246e-24, 1.6086e-24, 1.3179e-24,\n",
       "         1.3127e-24, 1.3124e-24, 1.2829e-24, 1.1186e-24, 1.0719e-24, 1.0622e-24,\n",
       "         9.4883e-25, 9.0457e-25, 8.7071e-25, 8.5488e-25, 8.1104e-25, 8.0820e-25,\n",
       "         7.9430e-25, 7.8474e-25, 7.3849e-25, 7.2756e-25, 7.1900e-25, 6.8302e-25,\n",
       "         6.2038e-25, 6.1436e-25, 5.8631e-25, 5.8243e-25, 5.6620e-25, 5.1704e-25,\n",
       "         5.0896e-25, 4.9390e-25, 4.3183e-25, 4.2192e-25, 4.0841e-25, 3.9831e-25,\n",
       "         3.6634e-25, 3.5633e-25, 3.4617e-25, 3.1212e-25, 2.9439e-25, 2.9439e-25,\n",
       "         2.8975e-25, 2.4147e-25, 2.2296e-25, 2.1734e-25, 2.1645e-25, 2.1166e-25,\n",
       "         2.0901e-25, 2.0390e-25, 1.9633e-25, 1.8914e-25, 1.7435e-25, 1.6830e-25,\n",
       "         1.5823e-25, 1.4917e-25, 1.4603e-25, 1.2838e-25, 1.2301e-25, 1.1937e-25,\n",
       "         1.1457e-25, 1.1374e-25, 1.0764e-25, 1.0730e-25, 1.0621e-25, 1.0377e-25,\n",
       "         1.0199e-25, 1.0182e-25, 9.6716e-26, 9.5376e-26, 9.4792e-26, 9.2771e-26,\n",
       "         8.0931e-26, 6.6615e-26, 6.4014e-26, 6.2949e-26, 5.9873e-26, 5.7680e-26,\n",
       "         5.7610e-26, 5.2658e-26, 4.9868e-26, 4.9233e-26, 4.5181e-26, 4.3024e-26,\n",
       "         4.1367e-26, 3.2063e-26, 3.1362e-26, 2.6778e-26, 2.6344e-26, 2.2352e-26,\n",
       "         2.1005e-26, 2.0680e-26, 2.0329e-26, 1.9563e-26, 1.9511e-26, 1.9501e-26,\n",
       "         1.8675e-26, 1.7763e-26, 1.7402e-26, 1.6932e-26, 1.5572e-26, 1.5433e-26,\n",
       "         1.5089e-26, 1.3396e-26, 1.3336e-26, 1.2921e-26, 1.2824e-26, 1.2556e-26,\n",
       "         1.2056e-26, 1.1499e-26, 1.1340e-26, 1.0363e-26, 1.0052e-26, 9.0932e-27,\n",
       "         8.7829e-27, 8.3576e-27, 8.3142e-27, 7.8374e-27, 7.8204e-27, 5.9851e-27,\n",
       "         5.9543e-27, 5.5239e-27, 5.4871e-27, 4.4972e-27, 4.3925e-27, 4.3498e-27,\n",
       "         4.2870e-27, 3.9780e-27, 3.9652e-27, 3.8325e-27, 3.8320e-27, 3.3461e-27,\n",
       "         2.9720e-27, 2.7478e-27, 2.6265e-27, 2.4837e-27, 2.4324e-27, 2.2369e-27,\n",
       "         2.1028e-27, 1.9508e-27, 1.6589e-27, 1.5441e-27, 1.5034e-27, 1.4638e-27,\n",
       "         1.4266e-27, 1.3830e-27, 1.2941e-27, 1.2267e-27, 1.2160e-27, 8.5422e-28,\n",
       "         6.2991e-28, 5.1428e-28, 4.9443e-28, 4.8014e-28, 4.5949e-28, 4.0907e-28,\n",
       "         3.7904e-28, 3.7773e-28, 3.2488e-28, 2.6415e-28, 2.6296e-28, 2.4060e-28,\n",
       "         2.0847e-28, 1.9820e-28, 1.9338e-28, 1.7840e-28, 1.7710e-28, 1.7605e-28,\n",
       "         1.7002e-28, 1.6910e-28, 1.6448e-28, 9.4492e-29, 8.8565e-29, 8.1031e-29,\n",
       "         6.9192e-29, 5.2885e-29, 4.3367e-29, 4.1501e-29, 4.1142e-29, 3.9882e-29,\n",
       "         3.5310e-29, 3.1894e-29, 2.8199e-29, 2.6032e-29, 2.2795e-29, 2.2273e-29,\n",
       "         2.1252e-29, 2.0870e-29, 2.0387e-29, 2.0009e-29, 1.8210e-29, 1.7007e-29,\n",
       "         1.5138e-29, 1.3371e-29, 1.1143e-29, 1.0373e-29, 9.9912e-30, 9.7836e-30,\n",
       "         9.0296e-30, 8.4965e-30, 7.7644e-30, 6.6444e-30, 5.8850e-30, 5.6185e-30,\n",
       "         5.1948e-30, 5.1335e-30, 4.3910e-30, 4.2441e-30, 3.8125e-30, 3.6182e-30,\n",
       "         3.5725e-30, 2.5453e-30, 2.4893e-30, 2.4525e-30, 2.4277e-30, 2.4015e-30,\n",
       "         1.7721e-30, 1.6998e-30, 1.5459e-30, 1.5118e-30, 1.1762e-30, 1.1174e-30,\n",
       "         1.0538e-30, 1.0246e-30, 9.9618e-31, 9.9148e-31, 9.6262e-31, 9.3244e-31,\n",
       "         8.6838e-31, 8.0638e-31, 7.5009e-31, 7.4964e-31, 7.3645e-31, 5.2407e-31,\n",
       "         4.9578e-31, 3.6051e-31, 3.4654e-31, 3.4454e-31, 3.3745e-31, 3.1653e-31,\n",
       "         3.0821e-31, 3.0611e-31, 2.9914e-31, 2.7317e-31, 2.5315e-31, 1.7502e-31,\n",
       "         1.6732e-31, 1.6453e-31, 7.8701e-32, 6.5202e-32, 4.2825e-32, 2.5370e-32,\n",
       "         2.0679e-32, 1.7770e-32, 1.6709e-32, 1.5192e-32, 1.4700e-32, 9.6743e-33,\n",
       "         5.8200e-33, 4.4788e-33, 4.4749e-33, 3.6343e-33, 2.9848e-33, 2.6145e-33,\n",
       "         2.0189e-33, 2.0143e-33, 1.5831e-33, 1.4266e-33, 1.0773e-33, 7.7283e-34,\n",
       "         6.9104e-34, 6.9077e-34, 5.6146e-34, 5.3619e-34, 5.0435e-34, 4.5510e-34,\n",
       "         3.5850e-34, 3.0529e-34, 3.0205e-34, 2.0611e-34, 1.0110e-34, 8.1073e-35,\n",
       "         5.8053e-35, 3.8437e-35, 1.3817e-35, 1.3469e-35, 9.2806e-36, 7.7238e-36,\n",
       "         4.5524e-36, 4.5055e-36, 3.0558e-36, 2.2236e-36, 6.7376e-37, 5.8226e-37,\n",
       "         5.5907e-37, 5.4520e-37, 4.1933e-37, 4.1362e-37, 3.8614e-37, 1.3999e-37,\n",
       "         1.0427e-37, 1.0199e-37, 6.9222e-38, 4.5883e-38, 3.7604e-38, 2.1251e-38,\n",
       "         1.8897e-38, 6.5861e-39, 2.1180e-39, 1.1183e-39, 7.9810e-40, 3.4869e-40,\n",
       "         2.2185e-40, 7.0218e-41, 2.5804e-41, 8.0743e-42, 7.2181e-42, 4.1717e-42,\n",
       "         2.7816e-42, 7.3008e-43, 5.8294e-43, 5.1288e-43, 2.6485e-43, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0'),\n",
       "indices=tensor([[151, 525, 688, 461, 351, 128, 607, 626, 458, 623, 189, 267, 347, 373,\n",
       "         631, 369, 492, 566, 379, 518, 406, 655, 657, 660, 276, 292, 301, 605,\n",
       "         595, 550, 686, 156, 302, 551, 683, 371, 401, 402, 185, 264, 323, 160,\n",
       "         182, 437, 407, 627, 459, 575, 462, 598, 370, 125, 467, 434, 653, 548,\n",
       "         540, 210, 230, 316, 438, 569, 355, 404, 662, 293, 206, 684, 269, 294,\n",
       "         130, 352, 491, 124, 266, 382, 296, 549, 353, 439, 154, 318, 346, 356,\n",
       "         433, 413, 567, 516, 241, 411, 148, 176, 350, 259, 523, 242, 409, 408,\n",
       "         317, 398, 207, 177, 538, 344, 410, 465, 202, 580, 238, 578, 215, 427,\n",
       "         574, 233, 372, 512, 386, 295, 123, 357, 385, 297, 380, 275, 211, 217,\n",
       "         326, 208, 349, 543, 289, 652, 126, 263, 603, 570, 573, 602, 262, 522,\n",
       "         400, 456, 246, 405, 330, 315, 383, 325, 412, 659, 546, 218, 299, 204,\n",
       "         328, 636, 716, 494, 509, 129, 483, 234,  72, 377, 545, 270, 327, 300,\n",
       "         714, 426, 517, 544, 633, 443, 187, 431, 127, 342, 236, 384, 320, 685,\n",
       "         375, 345, 214, 243, 159, 739, 157, 493, 541, 656, 576, 601, 268, 622,\n",
       "         158, 239, 100, 397, 331, 244, 554, 205, 436, 381, 460, 606, 388, 374,\n",
       "         153, 682, 179, 776, 552, 484, 237, 163, 717, 498, 496, 428, 291, 690,\n",
       "         643,  98, 332, 171,  95, 247, 184, 571, 600, 694, 183, 466, 155, 629,\n",
       "         348, 620, 430, 469, 203, 280, 288, 594, 395, 468, 744, 572, 490, 396,\n",
       "         209, 485, 144, 596, 403, 260, 539, 455, 360, 290, 497,  70, 285, 558,\n",
       "         174, 432,  30,  93, 671, 378,  31, 224, 416, 303, 440, 447, 611, 537,\n",
       "         453, 324, 513, 105, 473, 524,   5, 547, 142, 141,  44, 640, 442, 696,\n",
       "         131, 500, 333, 319, 255, 339, 759, 334, 624, 337, 707, 661, 248, 645,\n",
       "         480, 634, 271, 689,   3, 557, 565, 599, 735, 441,  57, 258, 322, 732,\n",
       "         642, 726, 755, 488, 499,  17, 508, 692, 515, 121, 613, 421,  89, 152,\n",
       "         577, 740,  56, 286, 192, 778, 638,  99,   6, 474, 354, 704, 681, 590,\n",
       "         510, 482, 535,  26,   1, 530, 190, 773,  94, 287,  38, 745, 777, 587,\n",
       "         273, 584, 641, 630, 676, 457, 362, 240,   7, 556,  37, 191, 168,  58,\n",
       "         367, 343,  63,  90, 658, 765, 779, 446, 746, 711, 225,  15, 387, 542,\n",
       "          18, 514, 118, 188, 495, 181, 718, 250,  13, 760,  25,  80, 115, 651,\n",
       "         102, 703,  49, 731, 654,  62, 104, 504,  43, 502, 766, 393, 272, 161,\n",
       "         591,  29, 476,  50, 738, 615, 775, 454, 663, 507, 109, 120, 761, 164,\n",
       "          39,   2, 733, 687,  66, 139, 193, 771, 639,  23, 477, 767, 358,  71,\n",
       "          22, 691, 341, 390, 619, 162, 475, 780, 693, 757, 768,   8, 399, 644,\n",
       "         363, 586, 479,  96, 197, 743, 425,  54, 283, 116, 563, 724,   9, 298,\n",
       "          85, 186,  42,  88,  33,  81,  47, 561, 612, 232,  52, 172, 712, 763,\n",
       "         728, 417,  41, 668, 147, 628, 201, 616, 489, 178, 729, 555, 487, 307,\n",
       "         478, 146,  79, 632, 422, 279, 448, 673, 758, 143, 646, 365, 506, 471,\n",
       "         769, 609, 617, 741, 392, 221, 245, 783, 664, 665, 774, 226, 742, 756,\n",
       "         222, 106, 213, 424, 435, 723, 261, 364, 228, 709, 321, 418, 274,  97,\n",
       "         180, 229, 772, 196, 449, 560, 528, 103, 486,  92, 170, 278,  69, 265,\n",
       "          28, 111, 314,  84, 579, 195, 282, 463,   4,   0, 715,  10, 429, 534,\n",
       "         531,  77, 361, 527, 618, 119, 107, 340, 722,  51, 649, 200, 588, 749,\n",
       "         677, 532, 306, 647, 503, 700, 553, 284, 145, 754, 650, 281, 165, 132,\n",
       "         235,  78, 101, 110, 674, 150, 308, 753, 675,  16,  36, 747, 750, 670,\n",
       "         725, 368,  11,  21, 450, 697, 451, 737, 445, 113, 167, 112, 472, 133,\n",
       "         520, 751, 734, 444, 149, 464, 414, 249, 470, 526, 338, 311, 173, 770,\n",
       "         521,  14, 720, 533, 415,  46, 140, 122,  67, 719, 389,  76, 585, 621,\n",
       "         562,  32, 175, 198, 420, 610, 257, 695, 666,  19, 593, 667, 394, 329,\n",
       "         764, 583, 680, 304,  75,  12, 419, 637, 452, 376, 710, 312, 706, 511,\n",
       "         305, 313, 108, 597, 254, 252,  65, 423, 748,  91, 220,  83, 114, 227,\n",
       "          48, 736, 701, 256, 592,  20, 568, 366, 481, 169, 137, 335,  59, 519,\n",
       "          40, 752, 581,  82,  73, 536, 309,  68, 699,  74, 679, 117, 589,  27,\n",
       "         138, 216, 625,  45, 678,  87,  24, 702, 604,  64, 727, 359,  53, 730,\n",
       "         310,  61, 559,  35, 648, 194, 614, 672, 134, 698, 564, 391, 212, 608,\n",
       "         529, 199, 708, 166, 219, 762, 223,  86,  55, 277, 136, 781, 231, 669,\n",
       "         253, 635,  60, 782, 251, 505,  34, 501, 336, 135, 713, 705, 721, 582]],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_indices.sort(descending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = subset_indices[:,subset_indices.argsort(descending=True)[:,:k][:, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8028e-09]], device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151, 525, 688, 461, 351, 128, 607, 626, 458, 623, 189, 267, 347, 373,\n",
       "         631, 369, 492, 566, 379, 518, 406, 655, 657, 660, 276, 292, 301, 605,\n",
       "         595, 550, 686, 156, 302, 551, 683, 371, 401, 402, 185, 264, 323, 160,\n",
       "         182, 437, 407, 627, 459, 575, 462, 598, 370, 125, 467, 434, 653, 548,\n",
       "         540, 210, 230, 316, 438, 569, 355, 404, 662, 293, 206, 684, 269, 294,\n",
       "         130, 352, 491, 124, 266, 382, 296, 549, 353, 439, 154, 318, 346, 356,\n",
       "         433, 413, 567, 516, 241, 411, 148, 176, 350, 259, 523, 242, 409, 408,\n",
       "         317, 398, 207, 177, 538, 344, 410, 465, 202, 580, 238, 578, 215, 427,\n",
       "         574, 233, 372, 512, 386, 295, 123, 357, 385, 297, 380, 275, 211, 217,\n",
       "         326, 208, 349, 543, 289, 652, 126, 263, 603, 570, 573, 602, 262, 522,\n",
       "         400, 456, 246, 405, 330, 315, 383, 325, 412, 659, 546, 218, 299, 204,\n",
       "         328, 636, 716, 494, 509, 129, 483, 234,  72, 377, 545, 270, 327, 300,\n",
       "         714, 426, 517, 544, 633, 443, 187, 431, 127, 342, 236, 384, 320, 685,\n",
       "         375, 345, 214, 243, 159, 739, 157, 493, 541, 656, 576, 601, 268, 622,\n",
       "         158, 239, 100, 397, 331, 244, 554, 205, 436, 381, 460, 606, 388, 374,\n",
       "         153, 682, 179, 776, 552, 484, 237, 163, 717, 498, 496, 428, 291, 690,\n",
       "         643,  98, 332, 171,  95, 247, 184, 571, 600, 694, 183, 466, 155, 629,\n",
       "         348, 620, 430, 469, 203, 280, 288, 594, 395, 468, 744, 572, 490, 396,\n",
       "         209, 485, 144, 596, 403, 260, 539, 455, 360, 290, 497,  70, 285, 558,\n",
       "         174, 432,  30,  93, 671, 378,  31, 224, 416, 303, 440, 447, 611, 537,\n",
       "         453, 324, 513, 105, 473, 524,   5, 547, 142, 141,  44, 640, 442, 696,\n",
       "         131, 500, 333, 319, 255, 339, 759, 334, 624, 337, 707, 661, 248, 645,\n",
       "         480, 634, 271, 689,   3, 557, 565, 599, 735, 441,  57, 258, 322, 732,\n",
       "         642, 726, 755, 488, 499,  17, 508, 692, 515, 121, 613, 421,  89, 152,\n",
       "         577, 740,  56, 286, 192, 778, 638,  99,   6, 474, 354, 704, 681, 590,\n",
       "         510, 482, 535,  26,   1, 530, 190, 773,  94, 287,  38, 745, 777, 587,\n",
       "         273, 584, 641, 630, 676, 457, 362, 240,   7, 556,  37, 191, 168,  58,\n",
       "         367, 343,  63,  90, 658, 765, 779, 446, 746, 711, 225,  15, 387, 542,\n",
       "          18, 514, 118, 188, 495, 181, 718, 250,  13, 760,  25,  80, 115, 651,\n",
       "         102, 703,  49, 731, 654,  62, 104, 504,  43, 502, 766, 393, 272, 161,\n",
       "         591,  29, 476,  50, 738, 615, 775, 454, 663, 507, 109, 120, 761, 164,\n",
       "          39,   2, 733, 687,  66, 139, 193, 771, 639,  23, 477, 767, 358,  71,\n",
       "          22, 691, 341, 390, 619, 162, 475, 780, 693, 757, 768,   8, 399, 644,\n",
       "         363, 586, 479,  96, 197, 743, 425,  54, 283, 116, 563, 724,   9, 298,\n",
       "          85, 186,  42,  88,  33,  81,  47, 561, 612, 232,  52, 172, 712, 763,\n",
       "         728, 417,  41, 668, 147, 628, 201, 616, 489, 178, 729, 555, 487, 307,\n",
       "         478, 146,  79, 632, 422, 279, 448, 673, 758, 143, 646, 365, 506, 471,\n",
       "         769, 609, 617, 741, 392, 221, 245, 783, 664, 665, 774, 226, 742, 756,\n",
       "         222, 106, 213, 424, 435, 723, 261, 364, 228, 709, 321, 418, 274,  97,\n",
       "         180, 229, 772, 196, 449, 560, 528, 103, 486,  92, 170, 278,  69, 265,\n",
       "          28, 111, 314,  84, 579, 195, 282, 463,   4,   0, 715,  10, 429, 534,\n",
       "         531,  77, 361, 527, 618, 119, 107, 340, 722,  51, 649, 200, 588, 749,\n",
       "         677, 532, 306, 647, 503, 700, 553, 284, 145, 754, 650, 281, 165, 132,\n",
       "         235,  78, 101, 110, 674, 150, 308, 753, 675,  16,  36, 747, 750, 670,\n",
       "         725, 368,  11,  21, 450, 697, 451, 737, 445, 113, 167, 112, 472, 133,\n",
       "         520, 751, 734, 444, 149, 464, 414, 249, 470, 526, 338, 311, 173, 770,\n",
       "         521,  14, 720, 533, 415,  46, 140, 122,  67, 719, 389,  76, 585, 621,\n",
       "         562,  32, 175, 198, 420, 610, 257, 695, 666,  19, 593, 667, 394, 329,\n",
       "         764, 583, 680, 304,  75,  12, 419, 637, 452, 376, 710, 312, 706, 511,\n",
       "         305, 313, 108, 597, 254, 252,  65, 423, 748,  91, 220,  83, 114, 227,\n",
       "          48, 736, 701, 256, 592,  20, 568, 366, 481, 169, 137, 335,  59, 519,\n",
       "          40, 752, 581,  82,  73, 536, 309,  68, 699,  74, 679, 117, 589,  27,\n",
       "         138, 216, 625,  45, 678,  87,  24, 702, 604,  64, 727, 359,  53, 730,\n",
       "         310,  61, 559,  35, 648, 194, 614, 672, 134, 698, 564, 391, 212, 608,\n",
       "         529, 199, 708, 166, 219, 762, 223,  86,  55, 277, 136, 781, 231, 669,\n",
       "         253, 635,  60, 782, 251, 505,  34, 501, 336, 135, 713, 705, 721, 582]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_indices.argsort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are some things are printing above 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Are some things are printing above 1\")\n",
    "subset_indices[subset_indices > 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7cb9405510>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((subset_indices.reshape((28, 28)) > thres).clone().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7cb934f090>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[samp_index, :].view(28, 28).clone().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this needs to be rewritten to use the threshold stuff above\n",
    "\n",
    "k_all = [10, 25, 50, 250]\n",
    "\n",
    "for k in k_all:\n",
    "    model = VAE_Gumbel_MNIST(784, 400, 20, k = k).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (b1,b2))\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(model, optimizer, epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    diag_model.diag.data[torch.abs(diag_model.diag) < 0.05] = 0\n",
    "        test(model, epoch, k)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/gumbel_on_mnist/k_{}_sample_'.format(k) + str(epoch) + '.png')\n",
    "    with torch.no_grad():\n",
    "        w = model.weight_creator(data[0, :].view(-1, 784))\n",
    "        subset_indices = sample_subset(w, k=k, t=0.1).cpu()\n",
    "    plt.imshow(subset_indices.reshape((28, 28)) > 0.01)\n",
    "    plt.imsave('../data/gumbel_on_mnist/k_{}_example_featureselected.png'.format(k), \n",
    "               subset_indices.reshape((28, 28)) > 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
