{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 50\n",
    "#n_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 5000\n",
    "z_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=False)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=10, out_features=30, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7641,  0.4002,  0.9787,  2.2409,  1.8676],\n",
       "        [-0.9773,  0.9501, -0.1514, -0.1032,  0.4106],\n",
       "        [ 0.1440,  1.4543,  0.7610,  0.1217,  0.4439],\n",
       "        ...,\n",
       "        [ 0.2501, -1.0168,  0.0459,  0.5006,  1.2243],\n",
       "        [-0.5595,  1.5234, -0.5857,  0.8466, -0.1063],\n",
       "        [ 0.7700,  0.7508, -0.5606, -1.7603,  0.4371]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = data_mapper(latent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(18, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the gradient vs gumbel loss comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_truncated_with_gradients(df, model, optimizer, epoch, batch_size, Dim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    gradients = torch.zeros(df.shape[1]).to(device)\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone().to(device)\n",
    "        \n",
    "        \n",
    "        # need to do this twice because deriative with respect to input not implemented in BCE\n",
    "        # so need to switch them up\n",
    "        optimizer.zero_grad()\n",
    "        batch_data.requires_grad_(True)\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        # why clone detach here?\n",
    "        # still want gradient with respect to input, but BCE gradient with respect to target is not defined\n",
    "        # plus we only want to see how input affects mu_x, not the target\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim].clone().detach(), mu_x[:, :Dim], \n",
    "                                             mu_latent, logvar_latent) \n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gradients += torch.sqrt(batch_data.grad ** 2).sum(dim = 0)\n",
    "        # no step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # do not calculate with respect to \n",
    "        batch_data.requires_grad_(False)\n",
    "        mu_x.requires_grad_(True)\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim], mu_x[:, :Dim], mu_latent, logvar_latent) \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    return gradients\n",
    "    \n",
    "# match pre trained model\n",
    "def train_pre_trained_truncated(df, model, optimizer, epoch, pretrained_model, batch_size, D):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone()\n",
    "        \n",
    "        batch_data.requires_grad_(True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        with torch.no_grad():\n",
    "            _, mu_latent_2, logvar_latent_2 = pretrained_model(batch_data)\n",
    "        \n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :D], mu_x[:, :D], mu_latent, logvar_latent)\n",
    "        loss += 10*F.mse_loss(mu_latent, mu_latent_2, reduction = 'sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 10.452332\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 10.219682\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 9.858638\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 9.746358\n",
      "====> Epoch: 1 Average loss: 10.0859\n",
      "====> Test set loss: 20.3266\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 9.634346\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 9.531943\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 9.187952\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 8.987955\n",
      "====> Epoch: 2 Average loss: 9.3414\n",
      "====> Test set loss: 19.6330\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 8.865791\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 8.798163\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 8.512140\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 8.519280\n",
      "====> Epoch: 3 Average loss: 8.6299\n",
      "====> Test set loss: 19.0248\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 8.445617\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 8.143982\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 7.809476\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 7.666717\n",
      "====> Epoch: 4 Average loss: 7.9760\n",
      "====> Test set loss: 18.4661\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 7.721672\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 7.224120\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 7.298490\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 7.446839\n",
      "====> Epoch: 5 Average loss: 7.4269\n",
      "====> Test set loss: 18.1401\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 7.107218\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 7.217509\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 6.918293\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 7.001853\n",
      "====> Epoch: 6 Average loss: 7.0951\n",
      "====> Test set loss: 18.0062\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 7.043320\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 6.963954\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 6.991431\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 6.899938\n",
      "====> Epoch: 7 Average loss: 6.8991\n",
      "====> Test set loss: 17.8271\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 6.702937\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 6.697876\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 6.790318\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 6.677932\n",
      "====> Epoch: 8 Average loss: 6.7763\n",
      "====> Test set loss: 17.7034\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 6.718036\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 6.789576\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 6.677063\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 6.635189\n",
      "====> Epoch: 9 Average loss: 6.6838\n",
      "====> Test set loss: 17.6787\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 6.610594\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 6.581790\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 6.753506\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 6.491021\n",
      "====> Epoch: 10 Average loss: 6.5816\n",
      "====> Test set loss: 17.5883\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 6.617684\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 6.214832\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 6.422266\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 6.781167\n",
      "====> Epoch: 11 Average loss: 6.5327\n",
      "====> Test set loss: 17.5733\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 6.446230\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 6.395337\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 6.639224\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 6.333326\n",
      "====> Epoch: 12 Average loss: 6.4772\n",
      "====> Test set loss: 17.4765\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 6.715693\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 6.324173\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 6.635483\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 6.473283\n",
      "====> Epoch: 13 Average loss: 6.4211\n",
      "====> Test set loss: 17.4366\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 6.658976\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 6.227696\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 6.262451\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 6.197556\n",
      "====> Epoch: 14 Average loss: 6.3498\n",
      "====> Test set loss: 17.3720\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 6.262563\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 6.490472\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 5.854154\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 6.165423\n",
      "====> Epoch: 15 Average loss: 6.3018\n",
      "====> Test set loss: 17.3130\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 6.604807\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 6.426657\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 6.147476\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 6.243124\n",
      "====> Epoch: 16 Average loss: 6.2613\n",
      "====> Test set loss: 17.2503\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 6.324232\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 6.358412\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 6.128711\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 6.242067\n",
      "====> Epoch: 17 Average loss: 6.2017\n",
      "====> Test set loss: 17.1952\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 6.411606\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 6.084552\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 6.214274\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 6.015209\n",
      "====> Epoch: 18 Average loss: 6.1321\n",
      "====> Test set loss: 17.1638\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 6.016260\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 6.101333\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 6.223970\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 6.119271\n",
      "====> Epoch: 19 Average loss: 6.0935\n",
      "====> Test set loss: 17.1092\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 5.978640\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 6.045765\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 5.957504\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 6.264866\n",
      "====> Epoch: 20 Average loss: 6.0433\n",
      "====> Test set loss: 17.0434\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 6.123269\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 6.043769\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 5.957098\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 6.025342\n",
      "====> Epoch: 21 Average loss: 6.0132\n",
      "====> Test set loss: 17.0115\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 6.213302\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 5.919282\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 6.097178\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 6.048580\n",
      "====> Epoch: 22 Average loss: 5.9795\n",
      "====> Test set loss: 16.9530\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 5.983620\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 5.820119\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 5.924252\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 5.949360\n",
      "====> Epoch: 23 Average loss: 5.9498\n",
      "====> Test set loss: 16.9249\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 5.925084\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 5.919730\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 5.779309\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 5.905385\n",
      "====> Epoch: 24 Average loss: 5.9117\n",
      "====> Test set loss: 16.9480\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 6.127874\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 5.996351\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 5.670306\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 5.869435\n",
      "====> Epoch: 25 Average loss: 5.8897\n",
      "====> Test set loss: 16.9025\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 6.073798\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 5.881413\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 5.843618\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 5.668038\n",
      "====> Epoch: 26 Average loss: 5.8561\n",
      "====> Test set loss: 16.8934\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 5.959050\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 5.687901\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 5.559843\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 6.000223\n",
      "====> Epoch: 27 Average loss: 5.8344\n",
      "====> Test set loss: 16.8221\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 5.746851\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 5.711776\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 6.008284\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 5.831166\n",
      "====> Epoch: 28 Average loss: 5.8010\n",
      "====> Test set loss: 16.7717\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 5.650460\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 5.538215\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 5.840048\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 5.839487\n",
      "====> Epoch: 29 Average loss: 5.7773\n",
      "====> Test set loss: 16.7872\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 5.629004\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 5.674789\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 5.655321\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 5.738624\n",
      "====> Epoch: 30 Average loss: 5.7395\n",
      "====> Test set loss: 16.7262\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 5.732287\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 5.538768\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 5.823987\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 5.773391\n",
      "====> Epoch: 31 Average loss: 5.7101\n",
      "====> Test set loss: 16.7306\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 5.568404\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 5.578984\n",
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 5.763010\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 5.786530\n",
      "====> Epoch: 32 Average loss: 5.6866\n",
      "====> Test set loss: 16.7026\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 5.802851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 5.704850\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 5.844684\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 5.659781\n",
      "====> Epoch: 33 Average loss: 5.6510\n",
      "====> Test set loss: 16.6884\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 5.718569\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 5.800383\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 5.656431\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 5.551423\n",
      "====> Epoch: 34 Average loss: 5.6512\n",
      "====> Test set loss: 16.6534\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 5.709702\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 5.468955\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 5.465806\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 5.688542\n",
      "====> Epoch: 35 Average loss: 5.6113\n",
      "====> Test set loss: 16.6351\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 5.474185\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 5.786173\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 5.247676\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 5.694684\n",
      "====> Epoch: 36 Average loss: 5.5941\n",
      "====> Test set loss: 16.6039\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 5.472166\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 5.617922\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 5.263113\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 5.791637\n",
      "====> Epoch: 37 Average loss: 5.5742\n",
      "====> Test set loss: 16.6505\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 5.543507\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 5.578171\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 5.623125\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 5.457778\n",
      "====> Epoch: 38 Average loss: 5.5588\n",
      "====> Test set loss: 16.5683\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 5.470272\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 5.459655\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 5.568872\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 5.578827\n",
      "====> Epoch: 39 Average loss: 5.5684\n",
      "====> Test set loss: 16.6404\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 5.447514\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 5.252637\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 5.633356\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 5.584909\n",
      "====> Epoch: 40 Average loss: 5.5516\n",
      "====> Test set loss: 16.5811\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 5.327169\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 5.450005\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 5.617385\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 5.364481\n",
      "====> Epoch: 41 Average loss: 5.5317\n",
      "====> Test set loss: 16.6092\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 5.628969\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 5.485274\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 5.491691\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 5.431292\n",
      "====> Epoch: 42 Average loss: 5.5337\n",
      "====> Test set loss: 16.5217\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 5.609991\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 5.403312\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 5.508939\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 5.410938\n",
      "====> Epoch: 43 Average loss: 5.5263\n",
      "====> Test set loss: 16.5674\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 5.811180\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 5.569396\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 5.722503\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 5.273852\n",
      "====> Epoch: 44 Average loss: 5.5089\n",
      "====> Test set loss: 16.5289\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 5.370144\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 5.485389\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 5.282215\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 5.511310\n",
      "====> Epoch: 45 Average loss: 5.4980\n",
      "====> Test set loss: 16.5372\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 5.593468\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 5.513841\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 5.519940\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 5.566488\n",
      "====> Epoch: 46 Average loss: 5.4894\n",
      "====> Test set loss: 16.5479\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 5.446789\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 5.479490\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 5.715328\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 5.539253\n",
      "====> Epoch: 47 Average loss: 5.4777\n",
      "====> Test set loss: 16.5408\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 5.638768\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 5.672569\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 5.747556\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 5.449678\n",
      "====> Epoch: 48 Average loss: 5.4797\n",
      "====> Test set loss: 16.5366\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 5.305096\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 5.517716\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 5.651197\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 5.475989\n",
      "====> Epoch: 49 Average loss: 5.4789\n",
      "====> Test set loss: 16.5120\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 5.379386\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 5.311292\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 5.515159\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 5.618675\n",
      "====> Epoch: 50 Average loss: 5.4624\n",
      "====> Test set loss: 16.4970\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = 15)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7101fdf650>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7101e9f4d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try higher dimension on whole feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 20.975895\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 20.328279\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.899927\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 19.257933\n",
      "====> Epoch: 1 Average loss: 20.1315\n",
      "====> Test set loss: 19.3123\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 19.321316\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 18.871542\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 18.230387\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 17.939095\n",
      "====> Epoch: 2 Average loss: 18.5596\n",
      "====> Test set loss: 17.6997\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 17.755648\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 17.236288\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 16.690310\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 16.063086\n",
      "====> Epoch: 3 Average loss: 16.9235\n",
      "====> Test set loss: 15.9999\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 15.963059\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 15.830637\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 14.986224\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 14.661016\n",
      "====> Epoch: 4 Average loss: 15.2481\n",
      "====> Test set loss: 14.3831\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 14.401565\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 14.484173\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 13.626694\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 13.714748\n",
      "====> Epoch: 5 Average loss: 13.9323\n",
      "====> Test set loss: 13.5342\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 13.560006\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 13.249876\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.752666\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 13.176271\n",
      "====> Epoch: 6 Average loss: 13.2952\n",
      "====> Test set loss: 13.0309\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 12.997343\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.796581\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.583660\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 12.946981\n",
      "====> Epoch: 7 Average loss: 12.9775\n",
      "====> Test set loss: 12.8914\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.431662\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.734293\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.566127\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.949251\n",
      "====> Epoch: 8 Average loss: 12.7757\n",
      "====> Test set loss: 12.6943\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.927362\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.679183\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 12.314428\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 13.131214\n",
      "====> Epoch: 9 Average loss: 12.6506\n",
      "====> Test set loss: 12.5223\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.587350\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 12.765865\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 12.573797\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 12.039472\n",
      "====> Epoch: 10 Average loss: 12.5251\n",
      "====> Test set loss: 12.5028\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 12.284603\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 12.732681\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 12.313961\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 12.639155\n",
      "====> Epoch: 11 Average loss: 12.4068\n",
      "====> Test set loss: 12.3659\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 12.345366\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 12.212035\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 12.615745\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 12.521564\n",
      "====> Epoch: 12 Average loss: 12.3666\n",
      "====> Test set loss: 12.2897\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 12.291675\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 11.888488\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 11.990358\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 12.490339\n",
      "====> Epoch: 13 Average loss: 12.2718\n",
      "====> Test set loss: 12.1442\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 12.439504\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 11.845933\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 12.224072\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 11.862402\n",
      "====> Epoch: 14 Average loss: 12.1580\n",
      "====> Test set loss: 12.0777\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 12.086116\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 11.773405\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 11.766461\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 11.600407\n",
      "====> Epoch: 15 Average loss: 12.0369\n",
      "====> Test set loss: 11.9580\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 11.544173\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 12.597157\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 12.084489\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 11.709608\n",
      "====> Epoch: 16 Average loss: 11.9076\n",
      "====> Test set loss: 11.8302\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 11.778183\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 11.797688\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 11.409900\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 11.773752\n",
      "====> Epoch: 17 Average loss: 11.7707\n",
      "====> Test set loss: 11.6694\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 11.615557\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 12.040966\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 11.183863\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 11.724724\n",
      "====> Epoch: 18 Average loss: 11.6769\n",
      "====> Test set loss: 11.5900\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 11.949910\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 11.573872\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 11.544198\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 11.133289\n",
      "====> Epoch: 19 Average loss: 11.5729\n",
      "====> Test set loss: 11.5093\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 11.329702\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 11.835723\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 11.146894\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 11.251096\n",
      "====> Epoch: 20 Average loss: 11.4711\n",
      "====> Test set loss: 11.4620\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 11.407199\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 11.219052\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 11.526085\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 11.304830\n",
      "====> Epoch: 21 Average loss: 11.4510\n",
      "====> Test set loss: 11.4160\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 11.685805\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 11.307262\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 11.349314\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 11.478060\n",
      "====> Epoch: 22 Average loss: 11.4117\n",
      "====> Test set loss: 11.3474\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 11.363768\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 11.637946\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 11.187957\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 11.279254\n",
      "====> Epoch: 23 Average loss: 11.3393\n",
      "====> Test set loss: 11.3134\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 11.180388\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 11.332911\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 11.196120\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 11.475232\n",
      "====> Epoch: 24 Average loss: 11.3170\n",
      "====> Test set loss: 11.2573\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 11.426456\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 11.180093\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 11.258121\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 11.433764\n",
      "====> Epoch: 25 Average loss: 11.2880\n",
      "====> Test set loss: 11.2220\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 11.094102\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 11.168372\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 10.773443\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 11.081279\n",
      "====> Epoch: 26 Average loss: 11.2350\n",
      "====> Test set loss: 11.1533\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 11.080537\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 11.094805\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 10.887994\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 11.457227\n",
      "====> Epoch: 27 Average loss: 11.1934\n",
      "====> Test set loss: 11.1668\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 10.950188\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 11.016535\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 10.993670\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 11.152144\n",
      "====> Epoch: 28 Average loss: 11.1629\n",
      "====> Test set loss: 11.1175\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 11.171329\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 10.870151\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 11.206352\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 11.176111\n",
      "====> Epoch: 29 Average loss: 11.1068\n",
      "====> Test set loss: 11.0502\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 11.120786\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 10.911803\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 11.078912\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 10.916289\n",
      "====> Epoch: 30 Average loss: 11.0835\n",
      "====> Test set loss: 11.0261\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 11.421672\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 10.916827\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 10.900466\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 10.845920\n",
      "====> Epoch: 31 Average loss: 11.0144\n",
      "====> Test set loss: 10.9864\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 10.787181\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 10.798644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 11.092668\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 10.974205\n",
      "====> Epoch: 32 Average loss: 10.9907\n",
      "====> Test set loss: 10.9340\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 11.075909\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 10.881733\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 10.644748\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 11.154376\n",
      "====> Epoch: 33 Average loss: 10.9572\n",
      "====> Test set loss: 10.8717\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 10.706729\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 11.181383\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 10.587094\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 11.096233\n",
      "====> Epoch: 34 Average loss: 10.8972\n",
      "====> Test set loss: 10.8470\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 10.970847\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 10.880011\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 10.857474\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 10.607504\n",
      "====> Epoch: 35 Average loss: 10.8298\n",
      "====> Test set loss: 10.7669\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 11.107304\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 10.654974\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 10.799874\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 11.085653\n",
      "====> Epoch: 36 Average loss: 10.8134\n",
      "====> Test set loss: 10.7678\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 10.891788\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 10.709938\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 10.567152\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 10.693405\n",
      "====> Epoch: 37 Average loss: 10.7617\n",
      "====> Test set loss: 10.7407\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 10.453549\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 10.730202\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 10.363804\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 10.228728\n",
      "====> Epoch: 38 Average loss: 10.7115\n",
      "====> Test set loss: 10.6615\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 10.399026\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 10.846106\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 10.353933\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 10.595124\n",
      "====> Epoch: 39 Average loss: 10.6803\n",
      "====> Test set loss: 10.6516\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 10.560628\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 10.637239\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 10.451475\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 10.612430\n",
      "====> Epoch: 40 Average loss: 10.6688\n",
      "====> Test set loss: 10.6189\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 10.656998\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 10.520970\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 10.658960\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 10.674720\n",
      "====> Epoch: 41 Average loss: 10.6329\n",
      "====> Test set loss: 10.6304\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 10.786870\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 10.689404\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 10.703388\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 10.966898\n",
      "====> Epoch: 42 Average loss: 10.5972\n",
      "====> Test set loss: 10.5899\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 10.688991\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 10.606351\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 10.300176\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 10.849827\n",
      "====> Epoch: 43 Average loss: 10.5868\n",
      "====> Test set loss: 10.5807\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 10.457064\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 10.672657\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 10.234201\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 10.377682\n",
      "====> Epoch: 44 Average loss: 10.5565\n",
      "====> Test set loss: 10.5492\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 10.591689\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 10.422358\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 10.453838\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 10.595785\n",
      "====> Epoch: 45 Average loss: 10.5218\n",
      "====> Test set loss: 10.5255\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 10.378367\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 10.672503\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
