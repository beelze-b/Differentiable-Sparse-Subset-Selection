{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#import math\n",
    "\n",
    "#import gc\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good results for vanilla VAE on synthetic data with EPOCHS set to 50, \n",
    "# but when running locally set to 10 for reasonable run times\n",
    "n_epochs = 50\n",
    "#n_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 30\n",
    "N = 5000\n",
    "z_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = np.random.normal(loc=0.0, scale=1.0, size=N*z_size).reshape(N, z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=10, bias=False)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=10, out_features=30, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mapper = nn.Sequential(\n",
    "    nn.Linear(z_size, 2 * z_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2 * z_size, D, bias = True),\n",
    "    nn.ReLU()\n",
    ").to(device)\n",
    "\n",
    "data_mapper.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7641,  0.4002,  0.9787,  2.2409,  1.8676],\n",
       "        [-0.9773,  0.9501, -0.1514, -0.1032,  0.4106],\n",
       "        [ 0.1440,  1.4543,  0.7610,  0.1217,  0.4439],\n",
       "        ...,\n",
       "        [ 0.2501, -1.0168,  0.0459,  0.5006,  1.2243],\n",
       "        [-0.5595,  1.5234, -0.5857,  0.8466, -0.1063],\n",
       "        [ 0.7700,  0.7508, -0.5606, -1.7603,  0.4371]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_data = Tensor(latent_data)\n",
    "latent_data.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = data_mapper(latent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19, device='cuda:0')\n",
      "tensor(12, device='cuda:0')\n",
      "tensor(18, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n",
      "tensor(14, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(torch.sum(actual_data[i,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = actual_data.cpu().numpy()\n",
    "scaler = MinMaxScaler()\n",
    "actual_data = scaler.fit_transform(actual_data)\n",
    "\n",
    "actual_data = Tensor(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(actual_data.shape[0]))\n",
    "upto = int(.8 * len(actual_data))\n",
    "\n",
    "train_data = actual_data[slices[:upto]]\n",
    "test_data = actual_data[slices[upto:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the gradient vs gumbel loss comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_truncated_with_gradients(df, model, optimizer, epoch, batch_size, Dim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    gradients = torch.zeros(df.shape[1]).to(device)\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone().to(device)\n",
    "        \n",
    "        \n",
    "        # need to do this twice because deriative with respect to input not implemented in BCE\n",
    "        # so need to switch them up\n",
    "        optimizer.zero_grad()\n",
    "        batch_data.requires_grad_(True)\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        # why clone detach here?\n",
    "        # still want gradient with respect to input, but BCE gradient with respect to target is not defined\n",
    "        # plus we only want to see how input affects mu_x, not the target\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim].clone().detach(), mu_x[:, :Dim], \n",
    "                                             mu_latent, logvar_latent) \n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gradients += torch.sqrt(batch_data.grad ** 2).sum(dim = 0)\n",
    "        # no step\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # do not calculate with respect to \n",
    "        batch_data.requires_grad_(False)\n",
    "        mu_x.requires_grad_(True)\n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :Dim], mu_x[:, :Dim], mu_latent, logvar_latent) \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    return gradients\n",
    "    \n",
    "# match pre trained model\n",
    "def train_pre_trained_truncated(df, model, optimizer, epoch, pretrained_model, batch_size, D):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :].clone()\n",
    "        \n",
    "        batch_data.requires_grad_(True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        with torch.no_grad():\n",
    "            _, mu_latent_2, logvar_latent_2 = pretrained_model(batch_data)\n",
    "        \n",
    "        loss = loss_function_per_autoencoder(batch_data[:, :D], mu_x[:, :D], mu_latent, logvar_latent)\n",
    "        loss += 10*F.mse_loss(mu_latent, mu_latent_2, reduction = 'sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 10.297588\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 9.839776\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 9.418576\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 8.911005\n",
      "====> Epoch: 1 Average loss: 9.6227\n",
      "====> Test set loss: 19.3842\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 8.976604\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 8.578784\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 7.995737\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 7.651465\n",
      "====> Epoch: 2 Average loss: 8.3064\n",
      "====> Test set loss: 18.1793\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 7.687459\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 7.328920\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 7.010906\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 6.936525\n",
      "====> Epoch: 3 Average loss: 7.2734\n",
      "====> Test set loss: 17.6540\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 7.086658\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 7.011001\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 6.869354\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 7.040095\n",
      "====> Epoch: 4 Average loss: 6.8739\n",
      "====> Test set loss: 17.4268\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 6.890562\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 6.713628\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 6.814690\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 6.614572\n",
      "====> Epoch: 5 Average loss: 6.6700\n",
      "====> Test set loss: 17.3018\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 6.832915\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 6.609388\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 6.512256\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 6.655745\n",
      "====> Epoch: 6 Average loss: 6.5593\n",
      "====> Test set loss: 17.1708\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 6.694185\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 6.222439\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 6.248207\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 6.614993\n",
      "====> Epoch: 7 Average loss: 6.4587\n",
      "====> Test set loss: 17.0823\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 6.433969\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 6.482105\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 6.718382\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 6.503404\n",
      "====> Epoch: 8 Average loss: 6.4055\n",
      "====> Test set loss: 17.0322\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 6.483478\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 6.123491\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 6.491570\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 6.264974\n",
      "====> Epoch: 9 Average loss: 6.3632\n",
      "====> Test set loss: 17.0076\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 6.220320\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 6.379404\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 6.642758\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 6.434837\n",
      "====> Epoch: 10 Average loss: 6.3196\n",
      "====> Test set loss: 16.9833\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 6.364958\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 6.418815\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 6.059800\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 6.369599\n",
      "====> Epoch: 11 Average loss: 6.2821\n",
      "====> Test set loss: 16.8891\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 6.376063\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 6.161296\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 6.190397\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 6.280001\n",
      "====> Epoch: 12 Average loss: 6.2631\n",
      "====> Test set loss: 16.8560\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 6.104093\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 6.170716\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 6.275661\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 6.214219\n",
      "====> Epoch: 13 Average loss: 6.2120\n",
      "====> Test set loss: 16.8192\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 5.976346\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 6.080480\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 6.248082\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 6.122639\n",
      "====> Epoch: 14 Average loss: 6.1720\n",
      "====> Test set loss: 16.7760\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 6.245624\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 6.326456\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 6.026007\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 5.578483\n",
      "====> Epoch: 15 Average loss: 6.1166\n",
      "====> Test set loss: 16.7192\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 5.722740\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 5.809209\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 5.931566\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 5.739356\n",
      "====> Epoch: 16 Average loss: 6.0561\n",
      "====> Test set loss: 16.6112\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 6.353004\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 6.393784\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 6.093320\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 5.794981\n",
      "====> Epoch: 17 Average loss: 6.0134\n",
      "====> Test set loss: 16.6154\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 5.942898\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 6.196563\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 5.930510\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 5.962474\n",
      "====> Epoch: 18 Average loss: 5.9624\n",
      "====> Test set loss: 16.5361\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 5.596308\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 5.762169\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 5.738447\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 6.023212\n",
      "====> Epoch: 19 Average loss: 5.9142\n",
      "====> Test set loss: 16.4935\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 5.739569\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 5.569360\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 6.326886\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 5.763598\n",
      "====> Epoch: 20 Average loss: 5.8884\n",
      "====> Test set loss: 16.4269\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 6.071287\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 5.950865\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 6.027740\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 5.819314\n",
      "====> Epoch: 21 Average loss: 5.8560\n",
      "====> Test set loss: 16.4185\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 5.920295\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 5.756066\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 5.898883\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 5.778002\n",
      "====> Epoch: 22 Average loss: 5.8222\n",
      "====> Test set loss: 16.3888\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 5.643943\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 5.608378\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 5.749924\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 5.792890\n",
      "====> Epoch: 23 Average loss: 5.8089\n",
      "====> Test set loss: 16.3244\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 5.732737\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 5.849672\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 5.577737\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 5.642016\n",
      "====> Epoch: 24 Average loss: 5.7717\n",
      "====> Test set loss: 16.3273\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 5.530777\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 5.704826\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 5.722262\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 5.831369\n",
      "====> Epoch: 25 Average loss: 5.7403\n",
      "====> Test set loss: 16.2570\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 5.831060\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 5.523870\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 5.597277\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 5.929286\n",
      "====> Epoch: 26 Average loss: 5.7172\n",
      "====> Test set loss: 16.2331\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 5.642800\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 5.785043\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 5.338047\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 6.000745\n",
      "====> Epoch: 27 Average loss: 5.6769\n",
      "====> Test set loss: 16.1957\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 5.599895\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 5.623165\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 5.636326\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 5.692954\n",
      "====> Epoch: 28 Average loss: 5.6507\n",
      "====> Test set loss: 16.1878\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 5.391421\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 5.513729\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 5.404637\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 5.751236\n",
      "====> Epoch: 29 Average loss: 5.6174\n",
      "====> Test set loss: 16.1388\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 5.566554\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 5.587844\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 5.966912\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 5.621150\n",
      "====> Epoch: 30 Average loss: 5.5947\n",
      "====> Test set loss: 16.1191\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 5.431939\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 5.636769\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 5.715729\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 5.690195\n",
      "====> Epoch: 31 Average loss: 5.5781\n",
      "====> Test set loss: 16.1442\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 5.685106\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 5.664037\n",
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 5.542943\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 5.762074\n",
      "====> Epoch: 32 Average loss: 5.5746\n",
      "====> Test set loss: 16.0642\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 5.287699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 5.616476\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 5.675218\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 5.391408\n",
      "====> Epoch: 33 Average loss: 5.5424\n",
      "====> Test set loss: 16.0909\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 5.616315\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 5.634793\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 5.321568\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 5.666636\n",
      "====> Epoch: 34 Average loss: 5.5364\n",
      "====> Test set loss: 16.1056\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 5.557191\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 5.385663\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 5.629287\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 5.397124\n",
      "====> Epoch: 35 Average loss: 5.5210\n",
      "====> Test set loss: 16.0742\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 5.579366\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 5.447272\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 5.528569\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 5.757743\n",
      "====> Epoch: 36 Average loss: 5.5102\n",
      "====> Test set loss: 16.0055\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 5.634253\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 5.763332\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 5.200809\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 5.486412\n",
      "====> Epoch: 37 Average loss: 5.5043\n",
      "====> Test set loss: 16.0444\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 5.605244\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 5.537001\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 5.738504\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 5.515255\n",
      "====> Epoch: 38 Average loss: 5.4858\n",
      "====> Test set loss: 16.0375\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 5.375929\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 5.517965\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 5.577127\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 5.469043\n",
      "====> Epoch: 39 Average loss: 5.4874\n",
      "====> Test set loss: 16.0270\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 5.447002\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 5.377802\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 5.563618\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 5.532209\n",
      "====> Epoch: 40 Average loss: 5.4802\n",
      "====> Test set loss: 16.0009\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 5.699099\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 5.407067\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 5.466198\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 5.487772\n",
      "====> Epoch: 41 Average loss: 5.4642\n",
      "====> Test set loss: 15.9819\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 5.370375\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 5.519779\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 5.354524\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 5.540813\n",
      "====> Epoch: 42 Average loss: 5.4481\n",
      "====> Test set loss: 15.9982\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 5.446544\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 5.347227\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 5.433358\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 5.558477\n",
      "====> Epoch: 43 Average loss: 5.4417\n",
      "====> Test set loss: 15.9766\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 5.420812\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 5.629869\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 5.447831\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 5.407512\n",
      "====> Epoch: 44 Average loss: 5.4463\n",
      "====> Test set loss: 15.9752\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 5.396952\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 5.601576\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 5.427397\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 5.598503\n",
      "====> Epoch: 45 Average loss: 5.4337\n",
      "====> Test set loss: 15.9873\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 5.463508\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 5.594244\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 5.602030\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 5.335162\n",
      "====> Epoch: 46 Average loss: 5.4209\n",
      "====> Test set loss: 15.9762\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 5.274783\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 5.069346\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 5.358430\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 5.336737\n",
      "====> Epoch: 47 Average loss: 5.4152\n",
      "====> Test set loss: 15.9721\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 5.313478\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 5.106435\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 5.356067\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 5.251763\n",
      "====> Epoch: 48 Average loss: 5.4106\n",
      "====> Test set loss: 15.9720\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 5.440163\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 5.522230\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 5.409651\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 5.160346\n",
      "====> Epoch: 49 Average loss: 5.4070\n",
      "====> Test set loss: 15.9414\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 5.164806\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 5.508270\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 5.481636\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 5.106859\n",
      "====> Epoch: 50 Average loss: 5.3936\n",
      "====> Test set loss: 15.9734\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = 15)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9301de2450>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9301d57490>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAD5CAYAAAB4Z80xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAV0UlEQVR4nO3dfbQlVX3m8e/TTdvQEmiFgQgSeRPJxBfQG8LoWgmIIgoBVyYENBl8SeyENQrMcgIYM4uFiQYUReLoxBsFJRhQGkR864gKukxi00BEeTVKJN1gazCCICRw733mj1PdHq59TtW5Z5976h6eD6tWn1tVu+rHP7+1d+1dv5JtIiJieMvGHUBExKRIQo2IKCQJNSKikCTUiIhCklAjIgpJQo2IKGS7Ud/g/t99cdZlRUQjqz/2ZQ17jcfuu6txzlmx675D369beqgREYXU9lAlHQgcB+wJGLgXuNr27SOOLSJicHOzY7t13x6qpDOAywAB1wMbqt+XSjpz9OFFRAxodqb5VlhdD/X3gV+x/Vj3TknvAW4FztlWI0lrgDUA5x/yLF67/x4FQo2IqGfPje3edQl1DtgDuHve/qdVx7bJ9jQwDZmUiohFNtfehHoa8CVJ/wxsrPb9ErA/8MZRBhYRsSBt7aHaXifpAOAQOpNSAjYBG2w3evJ757Wrhw4yIp4Yfq3ERcY4KVU7y+/OA4mvL0IsERHDa2sPNSJiqfEIZu+bSkKNiMnS4kmpiIilJUP+iIhC2jwpFRGxpExyD/X53zxv1LeIiPiZTEpFRBQyxkmp2vJ9kg6UdISkHeftP2p0YUVELIw923grra7a1CnAp4A3AbdIOq7r8DuKRxMRMSzPNd9qSPpfkm6VdIukSyVt3+/8uh7qG4AX2H4lcBjwfySduuVefYJYI+kGSTd86OJLa4OOiChmbq751oekPYFTgCnbzwaWAyf2a1P3DHW57YcAbH9P0mHAWknPoE9C7a42NcjnCCIihlZ2ln87YAdJjwGr6BTY76muh7pZ0kFb/qiS6zHArsBzhgw0IqK82ccab92j6Wpbs+Uytu8BzgP+Ffg+8IDtL/S7dV0P9STgcWsQbM8AJ0n64IL+ZyMiRmmAWf7u0fR8kp5C5/NP+wD3A5dL+j3bl/S6Xl35vk19jv19o4Af+vcmp0VEwK77Dn+NckP+lwD/YvvfACRdCbwQWFhCjYhYcsqtQ/1X4FBJq4BHgCOAG/o1SEKNiMlSKKHaXi9pLXATnUef/0SPxwNbJKFGxETx7GP1JzW9ln0WcFbT85NQI2KyTHJxlIiIRZUC0xERhUxyD/WxC1O+LyKaedLbLhv+IumhRkQUMsYeam35vvkkXTyKQCIiipiZab4V1reHKunq+buAwyWtBrB9bPGIIiKG0eJnqE8HbgM+BJhOQp0C3t2vUVVgYA3AXx49xeufv9/wkUZENNHiiv1TwI3AW+lUWrkOeMT2V2x/pVcj29O2p2xPJZlGxKIqWGB6UHXFUeaA8yVdXv37g7o2ERFj1fZZ/qrq1PGSjgZ+MsgNzrg49aUjopn/+7YCF2nxM9THsf1Z4LMjiiUiYngjmL1vKsP3iJgsHt+oOAk1IiZL25+hRkQsGUmoERGFLJVJqYiI1pudHdutR55Q33XBC0Z9i4iIn8mQPyKikLa+eirp1yTtVP3eQdLZkj4t6VxJOy9OiBERAxjjq6d17/JfCDxc/b4A2Bk4t9p3UfFoIiKG5Dk33kqrS6jLbG957WDK9mm2v2b7bGDfXo0krZF0g6QbPvx364sFGxFRa26u+daHpGdJ+kbX9hNJp/VrU/cM9RZJr7N9EXCzpCnbN0g6AOj5rVbb01Tfr37kU+/My/wRsXgKzfLbvhM4CEDScuAe4JP92tQl1D8ALpD0p8B9wD9K2ghsrI5FRLTLaCaljgC+a/vufifVle97AHitpF+gM8TfDthk+wfFwoyIKGmAhNpdDL8yXY2w5zsRuLTuek3L9z0I3Nwownm097MX0iwiYmEGKI7S/XiyF0lPAo4F3lJ3vaxDjYjJUn7I/3LgpiYj8yTUiJgs5ZdDvYoGw31IQo2ISVPwXX5Jq4CXAn/Y5Pwk1IiYKC445Lf9MLBL0/OTUCNisozgDaimklAjYrJMcj3UmY/+1ahvERGT4j2vGP4a6aFGRBQy09IC09WC1hOBe21/UdKrgRcCt9N5o6Dn+/wREWPR4iH/RdU5qyS9BtgRuJLOe62HAK8ZbXgREQNq8ZD/ObafK2k7OpVW9rA9K+kS+ryK2v1+7AVHPJfXP/cZxQKOiOin5LKpQdXWQ62G/b8ArKJTYBpgJbCiVyPb07anbE8lmUbEoppz862wuh7qh4E7gOXAW4HLJd0FHApcVjyaiIhhtXXIb/t8SR+vft8r6WLgJcBf276+yQ0e/e4Dw0cZEdFUmz8jbfvert/3A2tHGlFExBBG8a2oprIONSImSxJqREQhY5zlT0KNiMmSHmpERCFJqBERZXh2gof8K5/3tFHfIiLiZ9JDjYgoI8umIiJKSUKNiChkfI9Q+xdHkbSzpHMk3SHpR9V2e7VvdZ92ayTdIOmGC2/6bvmoIyJ68Mxc4620umpTnwB+DBxmexfbuwCHV/su79XocdWmnr9fuWgjIurMDbDVkLRa0tqqU3m7pP/W7/y6If/ets/t3mF7M3CupNfXhxMRsbgKT0pdAKyz/dtVKdNV/U6u66HeLel0Sbtv2SFpd0lnABuHjzUiorBCPVRJOwG/TqeMKbYfrQpE9VTXQz0BOBP4iqTdqn0/AK4Gjq9pC4AferjJaRERRQzSQ+3+ukhl2vZ09Xtf4N+AiyQ9D7gRONX2T3tdr28P1faPbZ9h+0DbT622X7Z9BvDKxlFHRCyWAXqo3fM91TbddaXtgOcD/8/2wcBP6XQwe6ob8vdz9hBtIyJGwjPNtxqbgE2211d/r6WTYHuq+4z0N3sdAnbvcSwiYmxKfUXa9mZJGyU9y/addL72fFu/NnXPUHcHXkZnmVQ3Af+w4EgjIkal7PLSNwEfq2b47wJe1+/kuoT6GWBH29+Yf0DSdQuNMCJiVEr1UAGq3DfV9Py6j/T9fp9jrx4groiIRVEyoQ5q5O/yz/6o5wqDiIjiPKux3TvFUSJiokx0DzUiYjF5Lj3UiIgixtlDrSvft5Okv5D0N5JePe/YB/q021q+7yPfvqdUrBERtWw13kqre1PqIjprTq8ATpR0haSV1bFDezXqfp3rtQfsWSjUiIh6nmu+lVY35N/P9n+vfl8l6a3AlyUdWz6UiIjhzbV4ln+lpGV2J5fbfrukTcBXgR2b3OBz1/zikCFGxBPFqwpcY5yTUnVD/k8DL+7eYfujwJuBR0cVVETEQnlOjbfS6t6UOr3H/nWS3lE8moiIIXl8Hz1N+b6ImCyt7aGmfF9ELDWjWA7VVMr3RcREmW3xLH/K90XEktLaHmqJ8n3LxviAOCKeePIuf0REIeOc5U9CjYiJsqR6qJJ2s/3DUQQTETGs2blhVoMOp67a1FPnbbsA10t6iqSn9mm3tdrUFx/+TvGgIyJ6sZtvpdX1UO8D7p63b0/gJsDAvttqZHsamAb4+NN+N9NSEbFo5to6yw+cDrwE+GPb3wKQ9C+29xl5ZBERC1By2ZSk7wEPArPAjO2+X0CtWzZ1nqTLgPMlbQTOotMzjYhopREM5Q+3fV+TE2snpWxvAo6X9JvANcCqQSJ5+SsyfxURi2ecQ/7G02G2Pw0cTucRAJJeN6qgIiIWanZuWeOtewK92tbMu5yBL0i6cRvHfs5Ay6ZsPwLcUv15Np1PpEREtMYgI/7uCfQeXmT7Xkm7AddIusP2V3udnGpTETFRSg75bd9b/ftDSZ8EDqHzxZJtSrWpiJgopWb5JT0ZWGb7wer3kcDb+rVJtamImCgFP2a6O/BJSdDJlX9re12/BiOvNhURsZhMmR6q7buA5w3SZuTFUd73xd1GfYuImBBvLXCNmRa/KRURsaSU6qEuRBJqREyUgs9QBzZwnauq4lRERCsZNd5Kqyvfd46kXavfU5LuAtZLulvSb/Rpt/Xtgw0PpXxfRCyeuQG20up6qEd3FQV4F3CC7f2BlwLv7tXI9rTtKdtTv7rj/oVCjYioN4sab6XVPUNdIWk72zPADrY3ANj+tqSVxaOJiBjSGL+AUptQ3w98TtI5wDpJ7wWuBI4Afm6x/7ac9f3rhgowIp44SiybmmvrLL/t90n6FnAycEB1/gHAVcCfjT68iIjBjLNgc5N6qNcB183fX5XvS7WpiGiVJbVsqsvZxaKIiChkTmq8lZbyfRExUWbHeO+U74uIidLmWf6U74uIJaXNs/xDl++7bb/nDBpTRMSCtXqWPyJiKWnzkD8iYkkZ57KpJNSImCizY+yh1lWbmpJ0raRLJO0l6RpJD0jaIOngPu22Vpv6+P0by0cdEdFDm6tNfQB4J/BZOsukPmh7Z+DM6tg2dVebOmH1XsWCjYio0+aEusL2521fCtj2Wjo/vgRsP4J4IiKGYjXfSqtLqP8h6UhJxwOW9EqAqrj0OF9IiIjYptI9VEnLJf2TpM/UnVs3KfVHdIb8c3TemDpZ0keAe4A3NAnmNT/5zyanRUTwjwWuMYKe3qnA7cBOdSf27aHavtn2y2y/3PYdtk+1vdr2rwDPKhRsREQxc2q+1ZH0dOBo4ENN7p1qUxExUQYZ8nevSKq2NfMu917gdBo+IUi1qYiYKIPM3tueBqa3dUzSMcAPbd8o6bAm10u1qYiYKAXf5X8RcKykV9BZ1bSTpEts/16vBqk2FRETpdS7/LbfArwFoOqh/u9+yRQWodpURMRianOB6aGdzJ6jvkVExFZzIyjg1+vbevOlOEpETJRUm4qIKGScBabrqk3tLOkcSXdI+lG13V7tW71YQUZENNXm4iifoLNk6jDbu9jeBTi82nd5r0bdi2Wv/ek/l4s2IqLGjNx4K60uoe5t+1zbm7fssL3Z9rnAL/Vq1F2+7/AnP7NUrBERtTzAVlpdQr1b0umStr4VJWl3SWcAqRwdEa0zziF/3aTUCXSKSX+lSqoGfgBcDfxOkxv89mlPGirAiIhBjGLZVFN1C/t/LOki4Brg67Yf2nJM0lHAuhHHFxExkDbP8p8CfAp4I3CLpOO6Dr9jlIFFRCxEm4f8bwBeYPshSXsDayXtbfsCOgVSIiJaZbatQ35g+ZZhvu3vVQUC1kp6BkmoEdFC43xTqm6Wf7Okg7b8USXXY4BdgeeMMrCIiIXwAP+VVpdQTwI2d++wPWP7JODXi0cTETGk1j5Dtb2pz7G/b3KDXz3v1kFjiognqFv/ePhrtHbZVETEUjPOZVNJqBExUWbGmFLr1qHuJOkvJP2NpFfPO/aB0YYWETG4Nk9KXURnedQVwImSrpC0sjp2aK9G3dWmfvzIDwuFGhFRr83l+/azfabtq2wfC9wEfFnSLv0adVebesoOuxULNiKizjh7qHXPUFdKWmZ7DsD22yVtAr4K7Fg8moiIIbV5Yf+ngRd377D9UeDNwKOjCioiYqFm7cZbP5K2l3S9pJsl3Srp7Lp7161DPV3SgZKOANZ3vYa6riqcUmvlshVNTouIKKLgOtT/BF5c1TJZAXxN0udtf71Xg7pZ/jfRqTb1Jn6+2tTbS0QcEVFSqWeo7thSsnRFtfVtVPcMdQ2pNhURS0jJZ6iSlgM3AvsD77e9vt/5dc9QH1dtCjgMeLmk95CEGhEtNIcbb91LPKttTfe1bM/aPgh4OnCIpGf3u3ddD3WzpINsf6O6+EOSjgEuJNWmIqKFBlkOZXsamG5w3v2SrgOOAm7pdV6qTUXERCk4y/9fJK2ufu8AvAS4o1+bkVebiohYTAVn+Z8GfLR6jroM+ITtz/RrMPLiKK9ZsfeobxERsVWpSSnb3wQOHqRNqk1FxEQZxSulTQ2cUCXtZjsVTyKilVpbYFrSU+fvAq6XdDAg2/8+ssgiIhbANZNNo1TXQ70PuHvevj3pVJ0ysO+2GlVrudYA/M5TDuGFOz5zyDAjIpoZ52ek65ZNnQ7cCRxrex/b+wCbqt/bTKbw+PJ9SaYRsZgGWdhfWt2yqfMkXQacL2kjcBbj/WRLRERfbR7yb1mLeryk3wSuAVYNcoM9Hkv+jYjF09pJKQBJB9J5bnot8EVgv2r/UbbXjTa8iIjBjHPZVF35vlPoKt8HHGl7y3us7xhxbBERAyv16ulC1PVQ30DK90XEEtLmIf/jyvdJOoxOUn0GSagR0ULjTKh1y6Y2Szpoyx9Vcj0G2JWU74uIFrLdeCutrod6EjAzL9gZ4CRJHyweTUTEkFo75C9Rvu+1D6TKX0Q0c3yBayyp4igREW0265JflRrMQqpN7WL7R6MIJiJiWON8U6puHeo5knatfk9JugtYL+luSb+xKBFGRAxgnO/y183yH237vur3u4ATbO8PvBR4d69G3V8SnJl5sFCoERH1PMB/pdUN+VdI2q6a2d/B9gYA29+WtLJXo+4vCT551d55mT8iFs1ci4ujvB/4nKRzgHWS3gtcCRwBfGPUwUVEDKq1s/y23yfpW8DJwAHV+QcAVwF/PvrwIiIG0/ZZ/s10hu/rt7yGCp1qU0Bttaldd9hp4dFFRAyo1JBf0l7AxcAv0vmY6nRVx6SngapNSTqu63CqTUVE6xSclJoB3mz7l4FDgf8p6b/2a5BqUxExUUr1UG1/H/h+9ftBSbfTqQ19W682qTYVERNlFJNSVYfyYGB9v/NSbSoiJsqsZxtv3Wvmq23N/OtJ2hG4AjjN9k/63TvVpiJiogzy6mn3mvltkbSCTjL9mO0r66438mpTERGLqdQrpZIEfBi43fZ7mrQZebWp81c8e9S3iIjYqmBxlBcB/wP4lqQtLzL9ie3P9WqQ8n0RMVEKzvJ/jQEn35NQI2KitPkz0lOSrpV0iaS9JF0j6QFJGyQdvFhBRkQ0Neu5xltpdcumPgC8E/gs8A/AB23vDJxZHdum7qUIX3j4O8WCjYioM86P9NUl1BW2P2/70k6cXlsF/CVg+16NbE/bnrI9deSq/QuGGxHR35zdeCut7hnqf0g6EtgZsKRX2r6qqtY/WzyaiIghjfMTKHUJ9Y/oDPnngJcBJ0v6CHAPnff8a315++TdiGjmtwpco82fkb5Z0mnAHsAm26cCp8LW8n0REa3S5o/0nQJ8kpTvi4glYpyz/E3K902lfF9ELBVt/qZUyvdFxJLS2iE/Kd8XEUtMmz8jnfJ9EbGktHbZVMr3RcRSM85nqBpnNo8nLklrquK+EROj7hlqxKj83KcmIpa6JNSIiEKSUCMiCklCjXHJ89OYOJmUiogoJD3UiIhCklBj0Uk6StKdkr4j6cxxxxNRSob8sagkLQe+DbwU2ARsAF5l+7axBhZRQHqosdgOAb5j+y7bjwKXAcfVtIlYEpJQY7HtCWzs+ntTtS9iyUtCjcW2rbKPee4UEyEJNRbbJmCvrr+fDtw7plgiikpCjcW2AXimpH0kPQk4Ebh6zDFFFFFXDzWiKNszkt4I/B2wHLjQ9q1jDiuiiCybiogoJEP+iIhCklAjIgpJQo2IKCQJNSKikCTUiIhCklAjIgpJQo2IKCQJNSKikP8PB3tcDU3kljoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try higher dimension on whole feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it does here\n",
    "vae_gumbel_truncated = VAE_Gumbel(D, 100, 20, k = 3*z_size, t = global_t)\n",
    "vae_gumbel_truncated.to(device)\n",
    "vae_gumbel_trunc_optimizer = torch.optim.Adam(vae_gumbel_truncated.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 20.870840\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 19.952461\n",
      "Train Epoch: 1 [2560/4000 (64%)]\tLoss: 19.070566\n",
      "Train Epoch: 1 [3840/4000 (96%)]\tLoss: 18.329435\n",
      "====> Epoch: 1 Average loss: 19.5233\n",
      "====> Test set loss: 18.1645\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 18.270126\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 17.388006\n",
      "Train Epoch: 2 [2560/4000 (64%)]\tLoss: 16.164896\n",
      "Train Epoch: 2 [3840/4000 (96%)]\tLoss: 15.148496\n",
      "====> Epoch: 2 Average loss: 16.7001\n",
      "====> Test set loss: 14.9510\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 14.829627\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 14.571329\n",
      "Train Epoch: 3 [2560/4000 (64%)]\tLoss: 14.078301\n",
      "Train Epoch: 3 [3840/4000 (96%)]\tLoss: 13.617233\n",
      "====> Epoch: 3 Average loss: 14.1556\n",
      "====> Test set loss: 13.5166\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 13.540990\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 13.025706\n",
      "Train Epoch: 4 [2560/4000 (64%)]\tLoss: 13.174789\n",
      "Train Epoch: 4 [3840/4000 (96%)]\tLoss: 13.437339\n",
      "====> Epoch: 4 Average loss: 13.2070\n",
      "====> Test set loss: 12.9597\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 13.129204\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 12.595613\n",
      "Train Epoch: 5 [2560/4000 (64%)]\tLoss: 12.493498\n",
      "Train Epoch: 5 [3840/4000 (96%)]\tLoss: 12.412797\n",
      "====> Epoch: 5 Average loss: 12.8220\n",
      "====> Test set loss: 12.6305\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 12.619157\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 12.949213\n",
      "Train Epoch: 6 [2560/4000 (64%)]\tLoss: 12.329291\n",
      "Train Epoch: 6 [3840/4000 (96%)]\tLoss: 12.334397\n",
      "====> Epoch: 6 Average loss: 12.5651\n",
      "====> Test set loss: 12.4810\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 12.785959\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 12.209386\n",
      "Train Epoch: 7 [2560/4000 (64%)]\tLoss: 12.323133\n",
      "Train Epoch: 7 [3840/4000 (96%)]\tLoss: 11.779248\n",
      "====> Epoch: 7 Average loss: 12.4057\n",
      "====> Test set loss: 12.3298\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 12.381624\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 12.218850\n",
      "Train Epoch: 8 [2560/4000 (64%)]\tLoss: 12.342216\n",
      "Train Epoch: 8 [3840/4000 (96%)]\tLoss: 12.073955\n",
      "====> Epoch: 8 Average loss: 12.2818\n",
      "====> Test set loss: 12.2129\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 12.256794\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 12.591062\n",
      "Train Epoch: 9 [2560/4000 (64%)]\tLoss: 12.325843\n",
      "Train Epoch: 9 [3840/4000 (96%)]\tLoss: 12.452831\n",
      "====> Epoch: 9 Average loss: 12.2140\n",
      "====> Test set loss: 12.1511\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 12.190773\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 12.493846\n",
      "Train Epoch: 10 [2560/4000 (64%)]\tLoss: 12.494857\n",
      "Train Epoch: 10 [3840/4000 (96%)]\tLoss: 11.807241\n",
      "====> Epoch: 10 Average loss: 12.1283\n",
      "====> Test set loss: 12.1012\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 12.134873\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 11.921102\n",
      "Train Epoch: 11 [2560/4000 (64%)]\tLoss: 12.204382\n",
      "Train Epoch: 11 [3840/4000 (96%)]\tLoss: 12.168402\n",
      "====> Epoch: 11 Average loss: 12.0876\n",
      "====> Test set loss: 12.0625\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 12.132994\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 12.047829\n",
      "Train Epoch: 12 [2560/4000 (64%)]\tLoss: 11.889533\n",
      "Train Epoch: 12 [3840/4000 (96%)]\tLoss: 12.159696\n",
      "====> Epoch: 12 Average loss: 12.0512\n",
      "====> Test set loss: 12.0502\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 11.803803\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 11.886803\n",
      "Train Epoch: 13 [2560/4000 (64%)]\tLoss: 12.283241\n",
      "Train Epoch: 13 [3840/4000 (96%)]\tLoss: 12.451668\n",
      "====> Epoch: 13 Average loss: 12.0012\n",
      "====> Test set loss: 11.9636\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 11.642995\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 12.584284\n",
      "Train Epoch: 14 [2560/4000 (64%)]\tLoss: 11.948069\n",
      "Train Epoch: 14 [3840/4000 (96%)]\tLoss: 11.893464\n",
      "====> Epoch: 14 Average loss: 11.9501\n",
      "====> Test set loss: 11.9224\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 11.968706\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 11.879769\n",
      "Train Epoch: 15 [2560/4000 (64%)]\tLoss: 12.252148\n",
      "Train Epoch: 15 [3840/4000 (96%)]\tLoss: 11.518414\n",
      "====> Epoch: 15 Average loss: 11.8972\n",
      "====> Test set loss: 11.8264\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 11.952985\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 11.747293\n",
      "Train Epoch: 16 [2560/4000 (64%)]\tLoss: 11.778825\n",
      "Train Epoch: 16 [3840/4000 (96%)]\tLoss: 11.490246\n",
      "====> Epoch: 16 Average loss: 11.8046\n",
      "====> Test set loss: 11.6974\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 11.931363\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 11.799001\n",
      "Train Epoch: 17 [2560/4000 (64%)]\tLoss: 11.244071\n",
      "Train Epoch: 17 [3840/4000 (96%)]\tLoss: 11.388744\n",
      "====> Epoch: 17 Average loss: 11.6419\n",
      "====> Test set loss: 11.5341\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 11.578346\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 11.382900\n",
      "Train Epoch: 18 [2560/4000 (64%)]\tLoss: 11.279573\n",
      "Train Epoch: 18 [3840/4000 (96%)]\tLoss: 11.243348\n",
      "====> Epoch: 18 Average loss: 11.4723\n",
      "====> Test set loss: 11.3782\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 11.611951\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 11.422504\n",
      "Train Epoch: 19 [2560/4000 (64%)]\tLoss: 11.190935\n",
      "Train Epoch: 19 [3840/4000 (96%)]\tLoss: 11.588806\n",
      "====> Epoch: 19 Average loss: 11.3547\n",
      "====> Test set loss: 11.2669\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 11.691871\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 11.222486\n",
      "Train Epoch: 20 [2560/4000 (64%)]\tLoss: 11.402368\n",
      "Train Epoch: 20 [3840/4000 (96%)]\tLoss: 11.351008\n",
      "====> Epoch: 20 Average loss: 11.2590\n",
      "====> Test set loss: 11.1761\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 11.428996\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 11.310767\n",
      "Train Epoch: 21 [2560/4000 (64%)]\tLoss: 11.404423\n",
      "Train Epoch: 21 [3840/4000 (96%)]\tLoss: 11.114553\n",
      "====> Epoch: 21 Average loss: 11.2150\n",
      "====> Test set loss: 11.1442\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 11.065437\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 11.102719\n",
      "Train Epoch: 22 [2560/4000 (64%)]\tLoss: 10.610100\n",
      "Train Epoch: 22 [3840/4000 (96%)]\tLoss: 11.081409\n",
      "====> Epoch: 22 Average loss: 11.1394\n",
      "====> Test set loss: 11.0850\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 11.240884\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 10.759630\n",
      "Train Epoch: 23 [2560/4000 (64%)]\tLoss: 10.801010\n",
      "Train Epoch: 23 [3840/4000 (96%)]\tLoss: 11.100410\n",
      "====> Epoch: 23 Average loss: 11.0885\n",
      "====> Test set loss: 11.0494\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 10.930786\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 11.083556\n",
      "Train Epoch: 24 [2560/4000 (64%)]\tLoss: 11.409400\n",
      "Train Epoch: 24 [3840/4000 (96%)]\tLoss: 10.909419\n",
      "====> Epoch: 24 Average loss: 11.0458\n",
      "====> Test set loss: 10.9548\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 11.254046\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 10.679246\n",
      "Train Epoch: 25 [2560/4000 (64%)]\tLoss: 11.055143\n",
      "Train Epoch: 25 [3840/4000 (96%)]\tLoss: 10.965467\n",
      "====> Epoch: 25 Average loss: 10.9870\n",
      "====> Test set loss: 10.9330\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 11.256165\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 10.909465\n",
      "Train Epoch: 26 [2560/4000 (64%)]\tLoss: 10.813652\n",
      "Train Epoch: 26 [3840/4000 (96%)]\tLoss: 10.799227\n",
      "====> Epoch: 26 Average loss: 10.9390\n",
      "====> Test set loss: 10.8639\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 11.117853\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 10.984675\n",
      "Train Epoch: 27 [2560/4000 (64%)]\tLoss: 11.457674\n",
      "Train Epoch: 27 [3840/4000 (96%)]\tLoss: 10.870454\n",
      "====> Epoch: 27 Average loss: 10.8606\n",
      "====> Test set loss: 10.7950\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 10.685052\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 10.825153\n",
      "Train Epoch: 28 [2560/4000 (64%)]\tLoss: 11.029694\n",
      "Train Epoch: 28 [3840/4000 (96%)]\tLoss: 10.862294\n",
      "====> Epoch: 28 Average loss: 10.8110\n",
      "====> Test set loss: 10.7456\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 11.073690\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 10.830215\n",
      "Train Epoch: 29 [2560/4000 (64%)]\tLoss: 10.795018\n",
      "Train Epoch: 29 [3840/4000 (96%)]\tLoss: 10.747856\n",
      "====> Epoch: 29 Average loss: 10.7342\n",
      "====> Test set loss: 10.7078\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 10.615479\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 10.979120\n",
      "Train Epoch: 30 [2560/4000 (64%)]\tLoss: 10.283306\n",
      "Train Epoch: 30 [3840/4000 (96%)]\tLoss: 10.770411\n",
      "====> Epoch: 30 Average loss: 10.6892\n",
      "====> Test set loss: 10.6538\n",
      "Train Epoch: 31 [0/4000 (0%)]\tLoss: 11.089605\n",
      "Train Epoch: 31 [1280/4000 (32%)]\tLoss: 10.816459\n",
      "Train Epoch: 31 [2560/4000 (64%)]\tLoss: 10.476610\n",
      "Train Epoch: 31 [3840/4000 (96%)]\tLoss: 10.626451\n",
      "====> Epoch: 31 Average loss: 10.6350\n",
      "====> Test set loss: 10.5792\n",
      "Train Epoch: 32 [0/4000 (0%)]\tLoss: 10.705904\n",
      "Train Epoch: 32 [1280/4000 (32%)]\tLoss: 10.472621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2560/4000 (64%)]\tLoss: 10.551461\n",
      "Train Epoch: 32 [3840/4000 (96%)]\tLoss: 10.807224\n",
      "====> Epoch: 32 Average loss: 10.5973\n",
      "====> Test set loss: 10.5572\n",
      "Train Epoch: 33 [0/4000 (0%)]\tLoss: 10.514088\n",
      "Train Epoch: 33 [1280/4000 (32%)]\tLoss: 10.622087\n",
      "Train Epoch: 33 [2560/4000 (64%)]\tLoss: 10.615241\n",
      "Train Epoch: 33 [3840/4000 (96%)]\tLoss: 10.287411\n",
      "====> Epoch: 33 Average loss: 10.5228\n",
      "====> Test set loss: 10.5030\n",
      "Train Epoch: 34 [0/4000 (0%)]\tLoss: 10.358050\n",
      "Train Epoch: 34 [1280/4000 (32%)]\tLoss: 10.817330\n",
      "Train Epoch: 34 [2560/4000 (64%)]\tLoss: 10.386443\n",
      "Train Epoch: 34 [3840/4000 (96%)]\tLoss: 10.402726\n",
      "====> Epoch: 34 Average loss: 10.5003\n",
      "====> Test set loss: 10.4747\n",
      "Train Epoch: 35 [0/4000 (0%)]\tLoss: 10.315069\n",
      "Train Epoch: 35 [1280/4000 (32%)]\tLoss: 10.634645\n",
      "Train Epoch: 35 [2560/4000 (64%)]\tLoss: 10.517384\n",
      "Train Epoch: 35 [3840/4000 (96%)]\tLoss: 10.579914\n",
      "====> Epoch: 35 Average loss: 10.4573\n",
      "====> Test set loss: 10.4457\n",
      "Train Epoch: 36 [0/4000 (0%)]\tLoss: 10.321739\n",
      "Train Epoch: 36 [1280/4000 (32%)]\tLoss: 10.548800\n",
      "Train Epoch: 36 [2560/4000 (64%)]\tLoss: 10.548282\n",
      "Train Epoch: 36 [3840/4000 (96%)]\tLoss: 10.159883\n",
      "====> Epoch: 36 Average loss: 10.4232\n",
      "====> Test set loss: 10.4425\n",
      "Train Epoch: 37 [0/4000 (0%)]\tLoss: 10.268500\n",
      "Train Epoch: 37 [1280/4000 (32%)]\tLoss: 10.171836\n",
      "Train Epoch: 37 [2560/4000 (64%)]\tLoss: 10.447670\n",
      "Train Epoch: 37 [3840/4000 (96%)]\tLoss: 10.507710\n",
      "====> Epoch: 37 Average loss: 10.4135\n",
      "====> Test set loss: 10.3988\n",
      "Train Epoch: 38 [0/4000 (0%)]\tLoss: 10.655976\n",
      "Train Epoch: 38 [1280/4000 (32%)]\tLoss: 10.288979\n",
      "Train Epoch: 38 [2560/4000 (64%)]\tLoss: 10.387819\n",
      "Train Epoch: 38 [3840/4000 (96%)]\tLoss: 10.344398\n",
      "====> Epoch: 38 Average loss: 10.3773\n",
      "====> Test set loss: 10.3910\n",
      "Train Epoch: 39 [0/4000 (0%)]\tLoss: 10.308690\n",
      "Train Epoch: 39 [1280/4000 (32%)]\tLoss: 10.421466\n",
      "Train Epoch: 39 [2560/4000 (64%)]\tLoss: 10.267026\n",
      "Train Epoch: 39 [3840/4000 (96%)]\tLoss: 9.932081\n",
      "====> Epoch: 39 Average loss: 10.3545\n",
      "====> Test set loss: 10.3665\n",
      "Train Epoch: 40 [0/4000 (0%)]\tLoss: 10.373111\n",
      "Train Epoch: 40 [1280/4000 (32%)]\tLoss: 10.264694\n",
      "Train Epoch: 40 [2560/4000 (64%)]\tLoss: 10.707935\n",
      "Train Epoch: 40 [3840/4000 (96%)]\tLoss: 10.350736\n",
      "====> Epoch: 40 Average loss: 10.3381\n",
      "====> Test set loss: 10.3455\n",
      "Train Epoch: 41 [0/4000 (0%)]\tLoss: 10.681108\n",
      "Train Epoch: 41 [1280/4000 (32%)]\tLoss: 10.277610\n",
      "Train Epoch: 41 [2560/4000 (64%)]\tLoss: 10.050622\n",
      "Train Epoch: 41 [3840/4000 (96%)]\tLoss: 10.102536\n",
      "====> Epoch: 41 Average loss: 10.3244\n",
      "====> Test set loss: 10.3268\n",
      "Train Epoch: 42 [0/4000 (0%)]\tLoss: 10.372196\n",
      "Train Epoch: 42 [1280/4000 (32%)]\tLoss: 10.649116\n",
      "Train Epoch: 42 [2560/4000 (64%)]\tLoss: 10.431314\n",
      "Train Epoch: 42 [3840/4000 (96%)]\tLoss: 10.177486\n",
      "====> Epoch: 42 Average loss: 10.2940\n",
      "====> Test set loss: 10.2808\n",
      "Train Epoch: 43 [0/4000 (0%)]\tLoss: 10.230044\n",
      "Train Epoch: 43 [1280/4000 (32%)]\tLoss: 10.382212\n",
      "Train Epoch: 43 [2560/4000 (64%)]\tLoss: 10.238670\n",
      "Train Epoch: 43 [3840/4000 (96%)]\tLoss: 10.283669\n",
      "====> Epoch: 43 Average loss: 10.2783\n",
      "====> Test set loss: 10.2745\n",
      "Train Epoch: 44 [0/4000 (0%)]\tLoss: 10.303740\n",
      "Train Epoch: 44 [1280/4000 (32%)]\tLoss: 10.185147\n",
      "Train Epoch: 44 [2560/4000 (64%)]\tLoss: 10.259853\n",
      "Train Epoch: 44 [3840/4000 (96%)]\tLoss: 9.921582\n",
      "====> Epoch: 44 Average loss: 10.2738\n",
      "====> Test set loss: 10.2435\n",
      "Train Epoch: 45 [0/4000 (0%)]\tLoss: 10.284977\n",
      "Train Epoch: 45 [1280/4000 (32%)]\tLoss: 10.288052\n",
      "Train Epoch: 45 [2560/4000 (64%)]\tLoss: 9.802613\n",
      "Train Epoch: 45 [3840/4000 (96%)]\tLoss: 10.108558\n",
      "====> Epoch: 45 Average loss: 10.2436\n",
      "====> Test set loss: 10.2240\n",
      "Train Epoch: 46 [0/4000 (0%)]\tLoss: 10.114181\n",
      "Train Epoch: 46 [1280/4000 (32%)]\tLoss: 10.604025\n",
      "Train Epoch: 46 [2560/4000 (64%)]\tLoss: 10.100937\n",
      "Train Epoch: 46 [3840/4000 (96%)]\tLoss: 10.323787\n",
      "====> Epoch: 46 Average loss: 10.2269\n",
      "====> Test set loss: 10.2149\n",
      "Train Epoch: 47 [0/4000 (0%)]\tLoss: 10.087420\n",
      "Train Epoch: 47 [1280/4000 (32%)]\tLoss: 10.219042\n",
      "Train Epoch: 47 [2560/4000 (64%)]\tLoss: 9.921982\n",
      "Train Epoch: 47 [3840/4000 (96%)]\tLoss: 10.137710\n",
      "====> Epoch: 47 Average loss: 10.2053\n",
      "====> Test set loss: 10.2071\n",
      "Train Epoch: 48 [0/4000 (0%)]\tLoss: 9.965625\n",
      "Train Epoch: 48 [1280/4000 (32%)]\tLoss: 10.325160\n",
      "Train Epoch: 48 [2560/4000 (64%)]\tLoss: 10.183228\n",
      "Train Epoch: 48 [3840/4000 (96%)]\tLoss: 10.353086\n",
      "====> Epoch: 48 Average loss: 10.1833\n",
      "====> Test set loss: 10.1652\n",
      "Train Epoch: 49 [0/4000 (0%)]\tLoss: 10.573447\n",
      "Train Epoch: 49 [1280/4000 (32%)]\tLoss: 10.381042\n",
      "Train Epoch: 49 [2560/4000 (64%)]\tLoss: 10.514054\n",
      "Train Epoch: 49 [3840/4000 (96%)]\tLoss: 10.019875\n",
      "====> Epoch: 49 Average loss: 10.1727\n",
      "====> Test set loss: 10.1707\n",
      "Train Epoch: 50 [0/4000 (0%)]\tLoss: 10.123894\n",
      "Train Epoch: 50 [1280/4000 (32%)]\tLoss: 10.160532\n",
      "Train Epoch: 50 [2560/4000 (64%)]\tLoss: 9.972918\n",
      "Train Epoch: 50 [3840/4000 (96%)]\tLoss: 10.031758\n",
      "====> Epoch: 50 Average loss: 10.1553\n",
      "====> Test set loss: 10.1133\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.zeros(train_data.shape[1]).to(device)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    grads=train_truncated_with_gradients(train_data, vae_gumbel_truncated, \n",
    "                                         vae_gumbel_trunc_optimizer, epoch, batch_size, Dim = D)\n",
    "    if epoch > 5:\n",
    "        gradients += grads\n",
    "    if epoch > 10:\n",
    "        vae_gumbel_truncated.t = 0.1\n",
    "    test(test_data, vae_gumbel_truncated, epoch, batch_size)\n",
    "    \n",
    "gradients = gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9301c30110>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(gradients.clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w = vae_gumbel_truncated.weight_creator(test_data[0:10, :])\n",
    "    subset_indices = sample_subset(w, k=3*z_size, t=0.01).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9301b6ed50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(subset_indices.sum(dim = 0).clone().detach().cpu().numpy()[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9986e-01, 7.1917e-01, 9.3894e-01, 1.0003e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.2081e+00, 9.9998e-01, 0.0000e+00, 9.7339e-15, 2.0631e-20, 1.4278e+00,\n",
       "         9.9968e-01, 3.9651e-23, 0.0000e+00, 5.9864e-01, 9.3648e-01, 0.0000e+00,\n",
       "         7.4607e-15, 9.2526e-01, 9.6716e-01, 1.0012e+00, 2.6316e-42, 5.3607e-10,\n",
       "         0.0000e+00, 2.6638e-01, 9.1652e-01, 0.0000e+00, 0.0000e+00, 1.0945e+00],\n",
       "        [1.0006e+00, 7.2446e-11, 1.4841e-26, 1.0000e+00, 9.8817e-01, 1.9276e-18,\n",
       "         1.0000e+00, 1.0000e+00, 9.9746e-01, 1.0000e+00, 3.9368e-35, 1.0000e+00,\n",
       "         1.0000e+00, 1.0121e+00, 1.0000e+00, 0.0000e+00, 1.0017e+00, 3.7794e-18,\n",
       "         1.8526e-19, 1.0000e+00, 3.0530e-41, 0.0000e+00, 0.0000e+00, 1.2058e-18,\n",
       "         0.0000e+00, 1.0000e+00, 0.0000e+00, 2.6625e-44, 1.1175e-26, 1.0000e+00],\n",
       "        [1.0003e+00, 0.0000e+00, 9.9906e-01, 3.6641e-01, 1.3702e-31, 0.0000e+00,\n",
       "         6.1408e-01, 1.0000e+00, 9.9992e-01, 9.9975e-01, 1.0000e+00, 9.1052e-01,\n",
       "         0.0000e+00, 1.0000e+00, 1.0203e+00, 0.0000e+00, 8.8712e-02, 3.5276e-23,\n",
       "         0.0000e+00, 1.0000e+00, 1.8313e-06, 0.0000e+00, 9.9999e-01, 1.0000e+00,\n",
       "         0.0000e+00, 7.8413e-17, 0.0000e+00, 9.9994e-01, 0.0000e+00, 1.0009e+00],\n",
       "        [1.1698e+00, 0.0000e+00, 2.2214e-08, 9.9999e-01, 8.3284e-01, 9.9734e-01,\n",
       "         9.2414e-11, 9.3346e-01, 9.9878e-01, 2.0299e-13, 1.0051e+00, 0.0000e+00,\n",
       "         1.0027e+00, 1.8988e-36, 9.9933e-01, 0.0000e+00, 1.0000e+00, 2.1643e-14,\n",
       "         0.0000e+00, 1.4135e+00, 9.9209e-01, 5.2754e-10, 1.1274e-03, 0.0000e+00,\n",
       "         1.3553e-22, 1.0012e+00, 2.0976e-24, 0.0000e+00, 6.6087e-02, 1.5867e+00],\n",
       "        [8.9830e-01, 2.4432e-08, 0.0000e+00, 1.0260e+00, 5.5574e-28, 1.0000e+00,\n",
       "         1.0000e+00, 1.0000e+00, 0.0000e+00, 1.1394e-10, 9.9996e-01, 8.2195e-07,\n",
       "         4.8496e-01, 1.8645e-11, 1.0195e+00, 0.0000e+00, 5.1206e-27, 1.2586e+00,\n",
       "         2.7579e-23, 1.0066e+00, 9.9341e-01, 5.8865e-01, 4.1318e-05, 6.5644e-21,\n",
       "         1.7521e-22, 1.0005e+00, 1.0038e+00, 9.6972e-01, 0.0000e+00, 7.4998e-01],\n",
       "        [1.0017e+00, 5.0331e-01, 9.9818e-01, 2.5980e-10, 9.9079e-01, 1.0305e+00,\n",
       "         0.0000e+00, 1.0000e+00, 9.7615e-01, 1.4378e-37, 0.0000e+00, 5.1274e-09,\n",
       "         8.3439e-01, 1.0000e+00, 0.0000e+00, 2.0175e-14, 9.8317e-01, 0.0000e+00,\n",
       "         1.0306e-31, 9.7872e-01, 9.9991e-01, 0.0000e+00, 1.0178e+00, 3.2291e-32,\n",
       "         1.0001e+00, 2.9716e-12, 0.0000e+00, 9.9996e-01, 6.8525e-01, 0.0000e+00],\n",
       "        [1.6910e+00, 4.7595e-01, 1.7645e-19, 2.7828e-18, 2.9665e-32, 1.0031e+00,\n",
       "         1.0000e+00, 1.0000e+00, 1.0221e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 9.9998e-01, 0.0000e+00, 1.0007e+00, 1.2810e+00, 3.3163e-01,\n",
       "         5.2421e-01, 1.0000e+00, 6.4330e-25, 1.0000e+00, 4.0057e-19, 2.8652e-36,\n",
       "         9.9931e-01, 1.2421e-04, 9.9989e-01, 0.0000e+00, 1.0222e+00, 6.7074e-01],\n",
       "        [9.3107e-01, 1.0690e+00, 0.0000e+00, 1.0000e+00, 8.5919e-01, 1.0047e+00,\n",
       "         1.0019e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 9.7898e-26,\n",
       "         0.0000e+00, 1.0000e+00, 1.2415e-06, 2.5549e-14, 1.0000e+00, 2.8476e-41,\n",
       "         9.7530e-02, 9.9523e-01, 2.5202e-32, 0.0000e+00, 1.6811e-08, 9.9790e-01,\n",
       "         9.9990e-01, 6.3850e-21, 1.0312e+00, 9.6904e-01, 5.1760e-33, 4.3284e-02],\n",
       "        [0.0000e+00, 7.1466e-44, 1.4078e+00, 3.1258e-11, 1.1145e+00, 0.0000e+00,\n",
       "         9.8175e-01, 8.8848e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4678e-01,\n",
       "         0.0000e+00, 1.9674e-42, 7.8547e-01, 7.7073e-01, 0.0000e+00, 9.9646e-01,\n",
       "         2.5834e-34, 1.0000e+00, 1.2139e+00, 1.4451e+00, 1.0539e+00, 1.0001e+00,\n",
       "         2.1894e-37, 3.0688e-43, 7.3551e-01, 0.0000e+00, 6.5957e-01, 0.0000e+00],\n",
       "        [1.1637e-08, 8.8606e-32, 7.3580e-01, 7.3492e-01, 4.6182e-01, 8.9355e-06,\n",
       "         1.0000e+00, 1.0000e+00, 3.6446e-01, 4.7354e-01, 1.6118e-01, 0.0000e+00,\n",
       "         1.0000e+00, 4.4083e-01, 0.0000e+00, 1.0000e+00, 1.1534e+00, 3.1692e-01,\n",
       "         1.0000e+00, 1.0000e+00, 9.9999e-01, 0.0000e+00, 2.3359e-16, 5.0196e-07,\n",
       "         4.8994e-01, 8.1698e-01, 0.0000e+00, 8.1674e-01, 1.0335e+00, 6.7376e-33]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
